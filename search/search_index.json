{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to our Core Software Documentation pages! The DevOps documentation provides the details about deploying the CyVerse cyberinfrastructure. It is intended for professional DevOps and IT specialists with experience in Infrastructure as Code (IaC), Kubernetes (K8s), OpenStack Cloud, and Networking. The Admin Guides are intended for CyVerse staff and administrators who will be managing user accounts and deploying containerized software applications on the infrastructure. The CyVerse infrastructure layer cake is stacked upon bare-metal Hardware, then Services, and finally Products What is CyVerse? Public Website CyVerse is a powerful computational infrastructure and the people who support its operations. It is fully open source, and dedicated to furthering open science. CyVerse has been funded by the United States National Science Foundation (NSF) from 2008 until the present SOFTWARE LICENSE Copyright \u00a9 2010-2023, The Arizona Board of Regents on behalf of The University of Arizona All rights reserved. Developed by: CyVerse as a collaboration between participants at BIO5 at The University of Arizona (the primary hosting institution), Cold Spring Harbor Laboratory, The University of Texas at Austin, and individual contributors. Find out more at http://www.cyverse.org/ . Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of CyVerse, BIO5, The University of Arizona, Cold Spring Harbor Laboratory, The University of Texas at Austin, nor the names of other contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"About"},{"location":"database/de-db/","text":"DE Database \u00b6 This Database is used for CyVerse discovery environment. Initialize database \u00b6 Access Database \u00b6 ssh root@DB_HOST.com psql -U postgres Create required Database and User \u00b6 # create de user create user de with password '********' ; # create de database create database de with owner de ; Add required extensions \u00b6 # psql -U postgres \\c de create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ; Populate Database \u00b6 Clone de-database repository \u00b6 git clone https://github.com/cyverse-de/de-database.git cd de-database install golang-migrate \u00b6 These steps are used on Ubuntu debian based OS. curl -s https://packagecloud.io/install/repositories/golang-migrate/migrate/script.deb.sh | sudo bash apt-get update apt-get install -y migrate # check migrate -help Run to populate de database \u00b6 Note : we are running the /migration directory from de-database migrate -database postgres://USER:PASSWORD@DB_HOST.com/de?sslmode = disable -path migrations up Additional database queries \u00b6 # checkout to de user psql -U de SET search_path = public, pg_catalog ; # create table version CREATE TABLE version ( version character varying ( 20 ) NOT NULL, applied timestamp DEFAULT now () ) ; # populate the Version table from a file # location of 999_version.sql: # https://github.com/cyverse-de/de-database/edit/master/old-databases/de-db/src/main/data/999_version.sql psql -U postgres -h localhost -d de -f 999_version.sql Migrate Database \u00b6 Once a while upon updating the k8s services, we would require to migrate the de-database , to add the latest database changes. Clone latest de-database \u00b6 git clone https://github.com/cyverse-de/de-database.git cd de-database install golang-migrate \u00b6 These steps are used on Ubuntu debian based OS. curl -s https://packagecloud.io/install/repositories/golang-migrate/migrate/script.deb.sh | sudo bash apt-get update apt-get install -y migrate # check migrate -help Run to migrate de database \u00b6 Note : we are running the /migration directory from de-database migrate -database postgres://USER:PASSWORD@DB_HOST.com/de?sslmode = disable -path migrations up","title":"DE"},{"location":"database/de-db/#de-database","text":"This Database is used for CyVerse discovery environment.","title":"DE Database"},{"location":"database/de-db/#initialize-database","text":"","title":"Initialize database"},{"location":"database/de-db/#access-database","text":"ssh root@DB_HOST.com psql -U postgres","title":"Access Database"},{"location":"database/de-db/#create-required-database-and-user","text":"# create de user create user de with password '********' ; # create de database create database de with owner de ;","title":"Create required Database and User"},{"location":"database/de-db/#add-required-extensions","text":"# psql -U postgres \\c de create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ;","title":"Add required extensions"},{"location":"database/de-db/#populate-database","text":"","title":"Populate Database"},{"location":"database/de-db/#clone-de-database-repository","text":"git clone https://github.com/cyverse-de/de-database.git cd de-database","title":"Clone de-database repository"},{"location":"database/de-db/#install-golang-migrate","text":"These steps are used on Ubuntu debian based OS. curl -s https://packagecloud.io/install/repositories/golang-migrate/migrate/script.deb.sh | sudo bash apt-get update apt-get install -y migrate # check migrate -help","title":"install golang-migrate"},{"location":"database/de-db/#run-to-populate-de-database","text":"Note : we are running the /migration directory from de-database migrate -database postgres://USER:PASSWORD@DB_HOST.com/de?sslmode = disable -path migrations up","title":"Run to populate de database"},{"location":"database/de-db/#additional-database-queries","text":"# checkout to de user psql -U de SET search_path = public, pg_catalog ; # create table version CREATE TABLE version ( version character varying ( 20 ) NOT NULL, applied timestamp DEFAULT now () ) ; # populate the Version table from a file # location of 999_version.sql: # https://github.com/cyverse-de/de-database/edit/master/old-databases/de-db/src/main/data/999_version.sql psql -U postgres -h localhost -d de -f 999_version.sql","title":"Additional database queries"},{"location":"database/de-db/#migrate-database","text":"Once a while upon updating the k8s services, we would require to migrate the de-database , to add the latest database changes.","title":"Migrate Database"},{"location":"database/de-db/#clone-latest-de-database","text":"git clone https://github.com/cyverse-de/de-database.git cd de-database","title":"Clone latest de-database"},{"location":"database/de-db/#install-golang-migrate_1","text":"These steps are used on Ubuntu debian based OS. curl -s https://packagecloud.io/install/repositories/golang-migrate/migrate/script.deb.sh | sudo bash apt-get update apt-get install -y migrate # check migrate -help","title":"install golang-migrate"},{"location":"database/de-db/#run-to-migrate-de-database","text":"Note : we are running the /migration directory from de-database migrate -database postgres://USER:PASSWORD@DB_HOST.com/de?sslmode = disable -path migrations up","title":"Run to migrate de database"},{"location":"database/grouper-db/","text":"Grouper Database \u00b6 This Database is used for CyVerse Grouper service. Initialize database \u00b6 Access Database \u00b6 ssh root@DB_HOST.com psql -U postgres Create required Database and User \u00b6 ## create grouper user create user grouper with password '********' ; ## create grouper database create database grouper with owner grouper ; Add required extensions \u00b6 # psql -U postgres \\c grouper create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ; Populate Database \u00b6 TODO Migrate Database \u00b6 TODO","title":"Grouper"},{"location":"database/grouper-db/#grouper-database","text":"This Database is used for CyVerse Grouper service.","title":"Grouper Database"},{"location":"database/grouper-db/#initialize-database","text":"","title":"Initialize database"},{"location":"database/grouper-db/#access-database","text":"ssh root@DB_HOST.com psql -U postgres","title":"Access Database"},{"location":"database/grouper-db/#create-required-database-and-user","text":"## create grouper user create user grouper with password '********' ; ## create grouper database create database grouper with owner grouper ;","title":"Create required Database and User"},{"location":"database/grouper-db/#add-required-extensions","text":"# psql -U postgres \\c grouper create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ;","title":"Add required extensions"},{"location":"database/grouper-db/#populate-database","text":"TODO","title":"Populate Database"},{"location":"database/grouper-db/#migrate-database","text":"TODO","title":"Migrate Database"},{"location":"database/keycloak-db/","text":"Keycloak Database \u00b6 This Database is used for Keycloak. Initialize database \u00b6 Access Database \u00b6 ssh root@DB_HOST.com psql -U postgres Create required Database and User \u00b6 # create keycloak user create user keycloak with password '********' ; # create keycloak database create database keycloak with owner keycloak ; Migrate Database \u00b6 TODO","title":"KeyCloak"},{"location":"database/keycloak-db/#keycloak-database","text":"This Database is used for Keycloak.","title":"Keycloak Database"},{"location":"database/keycloak-db/#initialize-database","text":"","title":"Initialize database"},{"location":"database/keycloak-db/#access-database","text":"ssh root@DB_HOST.com psql -U postgres","title":"Access Database"},{"location":"database/keycloak-db/#create-required-database-and-user","text":"# create keycloak user create user keycloak with password '********' ; # create keycloak database create database keycloak with owner keycloak ;","title":"Create required Database and User"},{"location":"database/keycloak-db/#migrate-database","text":"TODO","title":"Migrate Database"},{"location":"database/main/","text":"Databases \u00b6 CyVerse uses PostgreSQL as its primary database platform. Each database is maintained in its own GitHub Repository in the core CyVerse Organization or CyVerse Discovery Environment Organization Provisioning \u00b6 iCAT - the iRODS iCAT metadata database DE - the Discovery Environment database Metadata - the CyVerse Metadata Service Keycloak - the database used by Keycloak Notifications - the database used for notifications Unleash - the database used for Unleash service Grouper - the database for Grouper service QMS - the database for QMS service Portal - the User Portal database User Guides \u00b6 DevOps - instructions for setting up a remote DevOps environment to manage a CyVerse deployment Discovery Environment - designed for data scientists, students, and researchers who want to use CyVerse for research. Data Store - instructions for using the iRODS Data Store. This database dedicated to the discovery environment of Cyverse, which is used for multiple services such as: de notifications metadata unleash grouper qms portal keycloak NOTE: permissions database has been merged with DE database. Install \u00b6 The installation of this database is manully done on the host DB_HOST.com' . See documentation on how to install postgresql? Installing postgresql 12 on Centos7 sudo yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm sudo yum install -y postgresql12-server sudo /usr/pgsql-12/bin/postgresql-12-setup initdb sudo systemctl enable postgresql-12 sudo systemctl start postgresql-12 sudo yum install postgresql12-contrib Setup \u00b6 On this paragraph we will cover first and necessary steps to configure the database. ~postgres/12/data/pg_hba.conf IPv4 local connections: Add IP or IP range of kubernetes worker node, that requires connection to this database. TYPE DATABASE USER ADDRESS METHOD host all all *******/32 md5 host all all *******/32 md5 host all all *******/32 md5 host all all *******/32 md5 host all all *******/32 md5 host all all *******/32 md5 ~postgres/12/data/postgresql.conf # vi ~postgres/12/data/postgresql.conf listen_addresses = '*' # what IP address(es) to listen on; ** .pgpass** TODO: Databases and its Users \u00b6 DATABASE USER de de notifications de metadata de unleash unleash_user grouper grouper portal portal keycloak keycloak qms de","title":"Overview"},{"location":"database/main/#databases","text":"CyVerse uses PostgreSQL as its primary database platform. Each database is maintained in its own GitHub Repository in the core CyVerse Organization or CyVerse Discovery Environment Organization","title":"Databases"},{"location":"database/main/#provisioning","text":"iCAT - the iRODS iCAT metadata database DE - the Discovery Environment database Metadata - the CyVerse Metadata Service Keycloak - the database used by Keycloak Notifications - the database used for notifications Unleash - the database used for Unleash service Grouper - the database for Grouper service QMS - the database for QMS service Portal - the User Portal database","title":" Provisioning"},{"location":"database/main/#user-guides","text":"DevOps - instructions for setting up a remote DevOps environment to manage a CyVerse deployment Discovery Environment - designed for data scientists, students, and researchers who want to use CyVerse for research. Data Store - instructions for using the iRODS Data Store. This database dedicated to the discovery environment of Cyverse, which is used for multiple services such as: de notifications metadata unleash grouper qms portal keycloak NOTE: permissions database has been merged with DE database.","title":" User Guides"},{"location":"database/main/#install","text":"The installation of this database is manully done on the host DB_HOST.com' . See documentation on how to install postgresql? Installing postgresql 12 on Centos7 sudo yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm sudo yum install -y postgresql12-server sudo /usr/pgsql-12/bin/postgresql-12-setup initdb sudo systemctl enable postgresql-12 sudo systemctl start postgresql-12 sudo yum install postgresql12-contrib","title":"Install"},{"location":"database/main/#setup","text":"On this paragraph we will cover first and necessary steps to configure the database. ~postgres/12/data/pg_hba.conf IPv4 local connections: Add IP or IP range of kubernetes worker node, that requires connection to this database. TYPE DATABASE USER ADDRESS METHOD host all all *******/32 md5 host all all *******/32 md5 host all all *******/32 md5 host all all *******/32 md5 host all all *******/32 md5 host all all *******/32 md5 ~postgres/12/data/postgresql.conf # vi ~postgres/12/data/postgresql.conf listen_addresses = '*' # what IP address(es) to listen on; ** .pgpass** TODO:","title":"Setup"},{"location":"database/main/#databases-and-its-users","text":"DATABASE USER de de notifications de metadata de unleash unleash_user grouper grouper portal portal keycloak keycloak qms de","title":"Databases and its Users"},{"location":"database/metadata-db/","text":"Metadata Database \u00b6 This Database is used for CyVerse Metadata service. Initialize database \u00b6 Access Database \u00b6 ssh root@DB_HOST.com psql -U postgres Create required Database \u00b6 # create metadata database with de user create database metadata with owner de ; Add required extensions \u00b6 # psql -U postgres \\c metadata create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ; Populate Database \u00b6 TODO Migrate Database \u00b6 TODO","title":"Metadata"},{"location":"database/metadata-db/#metadata-database","text":"This Database is used for CyVerse Metadata service.","title":"Metadata Database"},{"location":"database/metadata-db/#initialize-database","text":"","title":"Initialize database"},{"location":"database/metadata-db/#access-database","text":"ssh root@DB_HOST.com psql -U postgres","title":"Access Database"},{"location":"database/metadata-db/#create-required-database","text":"# create metadata database with de user create database metadata with owner de ;","title":"Create required Database"},{"location":"database/metadata-db/#add-required-extensions","text":"# psql -U postgres \\c metadata create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ;","title":"Add required extensions"},{"location":"database/metadata-db/#populate-database","text":"TODO","title":"Populate Database"},{"location":"database/metadata-db/#migrate-database","text":"TODO","title":"Migrate Database"},{"location":"database/notifications-db/","text":"Notifications Database \u00b6 This Database is used for CyVerse notifications service. Initialize database \u00b6 Access Database \u00b6 ssh root@DB_HOST.com psql -U postgres Create required Database \u00b6 # create notifications database with de owner. create database notifications with owner de ; Add required extensions \u00b6 # psql -U postgres \\c notifications create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ; Populate Database \u00b6 TODO Migrate Database \u00b6 TODO","title":"Notifications"},{"location":"database/notifications-db/#notifications-database","text":"This Database is used for CyVerse notifications service.","title":"Notifications Database"},{"location":"database/notifications-db/#initialize-database","text":"","title":"Initialize database"},{"location":"database/notifications-db/#access-database","text":"ssh root@DB_HOST.com psql -U postgres","title":"Access Database"},{"location":"database/notifications-db/#create-required-database","text":"# create notifications database with de owner. create database notifications with owner de ;","title":"Create required Database"},{"location":"database/notifications-db/#add-required-extensions","text":"# psql -U postgres \\c notifications create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ;","title":"Add required extensions"},{"location":"database/notifications-db/#populate-database","text":"TODO","title":"Populate Database"},{"location":"database/notifications-db/#migrate-database","text":"TODO","title":"Migrate Database"},{"location":"database/portal-db/","text":"Portal Database \u00b6 This Database is used for CyVerse User portal Service. Initialize database \u00b6 Access Database \u00b6 ssh root@DB_HOST.com psql -U postgres Create required Database and User \u00b6 See also portal2/setup-database # create user create user portal_db_reader with password '********' ; # create portal database with the owner portal_db_reader create database portal with owner portal_db_reader ; ## Grant user to member of postgres # psql -U postgres GRANT postgres TO portal_db_reader ; Restore from dump \u00b6 For this we have to download portal.sql file. # restore to user portal_db_reader and database portal psql -U portal_db_reader -d portal -f portal.sql Populate Database \u00b6 make sure session Table exist \u00b6 CREATE TABLE public.session ( sid character varying NOT NULL, sess json NOT NULL, expire timestamp ( 6 ) without time zone NOT NULL ) ; ALTER TABLE public.session OWNER TO portal ; CREATE INDEX \"IDX_session_expire\" ON public.session USING btree ( expire ) ; Import GRID institutions \u00b6 download required grid file \u00b6 Official website # download grid wget https://digitalscience.figshare.com/ndownloader/files/30895309 # unzip unzip 30895309 import to the database \u00b6 For imorting this grid file we will use the script from portal2/scripts/import_grid_institutions.py . ./import_grid_institutions.py --host root@DB_HOST.com --user portal_db_reader --database portal grid.csv Populate these Tables \u00b6 These sql files can be found here . psql -U portal_db_reader -d portal -f ./account_country.sql psql -U portal_db_reader -d portal -f ./account_region.sql psql -U portal_db_reader -d portal -f ./account_gender.sql psql -U portal_db_reader -d portal -f ./account_occupation.sql psql -U portal_db_reader -d portal -f ./account_ethnicity.sql psql -U portal_db_reader -d portal -f ./account_fundingagency.sql psql -U portal_db_reader -d portal -f ./account_awarechannel.sql psql -U portal_db_reader -d portal -f ./account_researcharea.sql Extra \u00b6 Give Admin privilege to a user \u00b6 --update is_superuser UPDATE account_user SET is_superuser = true WHERE username = 'USERNAME' ; ---update is_staff UPDATE account_user SET is_staff = true WHERE username = 'USERNAME' ; Verify User email \u00b6 -- check if its verified select has_verified_email from account_user where username = 'USERNAME' ; --Verify email UPDATE account_user SET has_verified_email = true WHERE username = 'USERNAME' ; Migrate Database \u00b6 TODO","title":"Portal"},{"location":"database/portal-db/#portal-database","text":"This Database is used for CyVerse User portal Service.","title":"Portal Database"},{"location":"database/portal-db/#initialize-database","text":"","title":"Initialize database"},{"location":"database/portal-db/#access-database","text":"ssh root@DB_HOST.com psql -U postgres","title":"Access Database"},{"location":"database/portal-db/#create-required-database-and-user","text":"See also portal2/setup-database # create user create user portal_db_reader with password '********' ; # create portal database with the owner portal_db_reader create database portal with owner portal_db_reader ; ## Grant user to member of postgres # psql -U postgres GRANT postgres TO portal_db_reader ;","title":"Create required Database and User"},{"location":"database/portal-db/#restore-from-dump","text":"For this we have to download portal.sql file. # restore to user portal_db_reader and database portal psql -U portal_db_reader -d portal -f portal.sql","title":"Restore from dump"},{"location":"database/portal-db/#populate-database","text":"","title":"Populate Database"},{"location":"database/portal-db/#make-sure-session-table-exist","text":"CREATE TABLE public.session ( sid character varying NOT NULL, sess json NOT NULL, expire timestamp ( 6 ) without time zone NOT NULL ) ; ALTER TABLE public.session OWNER TO portal ; CREATE INDEX \"IDX_session_expire\" ON public.session USING btree ( expire ) ;","title":"make sure session Table exist"},{"location":"database/portal-db/#import-grid-institutions","text":"","title":"Import GRID institutions"},{"location":"database/portal-db/#download-required-grid-file","text":"Official website # download grid wget https://digitalscience.figshare.com/ndownloader/files/30895309 # unzip unzip 30895309","title":"download required grid file"},{"location":"database/portal-db/#import-to-the-database","text":"For imorting this grid file we will use the script from portal2/scripts/import_grid_institutions.py . ./import_grid_institutions.py --host root@DB_HOST.com --user portal_db_reader --database portal grid.csv","title":"import to the database"},{"location":"database/portal-db/#populate-these-tables","text":"These sql files can be found here . psql -U portal_db_reader -d portal -f ./account_country.sql psql -U portal_db_reader -d portal -f ./account_region.sql psql -U portal_db_reader -d portal -f ./account_gender.sql psql -U portal_db_reader -d portal -f ./account_occupation.sql psql -U portal_db_reader -d portal -f ./account_ethnicity.sql psql -U portal_db_reader -d portal -f ./account_fundingagency.sql psql -U portal_db_reader -d portal -f ./account_awarechannel.sql psql -U portal_db_reader -d portal -f ./account_researcharea.sql","title":"Populate these Tables"},{"location":"database/portal-db/#extra","text":"","title":"Extra"},{"location":"database/portal-db/#give-admin-privilege-to-a-user","text":"--update is_superuser UPDATE account_user SET is_superuser = true WHERE username = 'USERNAME' ; ---update is_staff UPDATE account_user SET is_staff = true WHERE username = 'USERNAME' ;","title":"Give Admin privilege to a user"},{"location":"database/portal-db/#verify-user-email","text":"-- check if its verified select has_verified_email from account_user where username = 'USERNAME' ; --Verify email UPDATE account_user SET has_verified_email = true WHERE username = 'USERNAME' ;","title":"Verify User email"},{"location":"database/portal-db/#migrate-database","text":"TODO","title":"Migrate Database"},{"location":"database/qms-db/","text":"QMS Database \u00b6 This Database is used for CyVerse QMS service. Initialize database \u00b6 Access vm \u00b6 ssh root@DB_HOST.com Create required Database \u00b6 # create qms database with de owner create database qms with owner de ; Add required extensions \u00b6 # psql -U postgres \\c de create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ; create extension \"insert_username\" ; Populate Database \u00b6 TODO Migrate Database \u00b6 Once a while upon updating the k8s services, we would require to migrate the qms-database , to add the latest database changes. Clone QMS repo \u00b6 git clone https://github.com/cyverse/QMS.git git fetch && git checkout prod cd QMS install golang-migrate \u00b6 These steps are used on Ubuntu debian based OS. curl -s https://packagecloud.io/install/repositories/golang-migrate/migrate/script.deb.sh | sudo bash apt-get update apt-get install -y migrate # check migrate -help Run to migrate QMS database \u00b6 Note : we are running the /migration directory from QMS migrate -database postgres://USER:PASSWORD@DB_HOST.com/qms?sslmode = disable -path migrations up","title":"QMS"},{"location":"database/qms-db/#qms-database","text":"This Database is used for CyVerse QMS service.","title":"QMS Database"},{"location":"database/qms-db/#initialize-database","text":"","title":"Initialize database"},{"location":"database/qms-db/#access-vm","text":"ssh root@DB_HOST.com","title":"Access vm"},{"location":"database/qms-db/#create-required-database","text":"# create qms database with de owner create database qms with owner de ;","title":"Create required Database"},{"location":"database/qms-db/#add-required-extensions","text":"# psql -U postgres \\c de create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ; create extension \"insert_username\" ;","title":"Add required extensions"},{"location":"database/qms-db/#populate-database","text":"TODO","title":"Populate Database"},{"location":"database/qms-db/#migrate-database","text":"Once a while upon updating the k8s services, we would require to migrate the qms-database , to add the latest database changes.","title":"Migrate Database"},{"location":"database/qms-db/#clone-qms-repo","text":"git clone https://github.com/cyverse/QMS.git git fetch && git checkout prod cd QMS","title":"Clone QMS repo"},{"location":"database/qms-db/#install-golang-migrate","text":"These steps are used on Ubuntu debian based OS. curl -s https://packagecloud.io/install/repositories/golang-migrate/migrate/script.deb.sh | sudo bash apt-get update apt-get install -y migrate # check migrate -help","title":"install golang-migrate"},{"location":"database/qms-db/#run-to-migrate-qms-database","text":"Note : we are running the /migration directory from QMS migrate -database postgres://USER:PASSWORD@DB_HOST.com/qms?sslmode = disable -path migrations up","title":"Run to migrate QMS database"},{"location":"database/unleash-db/","text":"Unleash Database \u00b6 This Database is used for CyVerse Unleash service. Initialize database \u00b6 Access vm \u00b6 ssh root@DB_HOST.com Create required Database and User \u00b6 # create unleash user create user unleash with password '********' ; # create unleash databased create database unleash with owner unleash ; Add required extensions \u00b6 # psql -U postgres \\c unleash create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ; Populate Database \u00b6 TODO Migrate Database \u00b6 TODO","title":"Unleash"},{"location":"database/unleash-db/#unleash-database","text":"This Database is used for CyVerse Unleash service.","title":"Unleash Database"},{"location":"database/unleash-db/#initialize-database","text":"","title":"Initialize database"},{"location":"database/unleash-db/#access-vm","text":"ssh root@DB_HOST.com","title":"Access vm"},{"location":"database/unleash-db/#create-required-database-and-user","text":"# create unleash user create user unleash with password '********' ; # create unleash databased create database unleash with owner unleash ;","title":"Create required Database and User"},{"location":"database/unleash-db/#add-required-extensions","text":"# psql -U postgres \\c unleash create extension \"uuid-ossp\" ; create extension \"moddatetime\" ; create extension \"btree_gist\" ;","title":"Add required extensions"},{"location":"database/unleash-db/#populate-database","text":"TODO","title":"Populate Database"},{"location":"database/unleash-db/#migrate-database","text":"TODO","title":"Migrate Database"},{"location":"deployments/DiscoveryEnvironment/","text":"Discovery Environment \u00b6 Prerequisites Make sure you have the harbor-registry-credentials secrets in your NAMESPACE, see also k8s-resources Make sure you have de-nginx-tls secret created, also see k8s-resources Make sure your Haproxy has the CA cert, Add /docker-tugraz-data/ca/ca.pem to HAPROXY_DOMAIN :/etc/ssl/certs/ca-bundle.crt` remember nginx is proxying all the services to the DE in CyVerse. Use the manifest files from the k8s-resources Change hardcoded \u00b6 If you are using a diffrent domain instead of cyverse.tugraz.at , e.g. cyverse.at - change these two files. /k8s-resources/resources/kustomize/de-nginx/base/nginx.conf - server_name ~^[^.]+[.]cyverse[.]tugraz[.]at$; + server_name ~^[^.]+[.]cyverse[.]at$; k8s-resources/resources/kustomize/de-nginx/base/kustomization.yaml - namespace: prod + namespace: discover Deploy \u00b6 ## For prod env kubectl apply -k resources/kustomize/de-nginx/overlays/prod/ -n prod kubectl apply -f resources/services/tugraz.yml -n prod ## For discover env # kubectl apply -k resources/kustomize/de-nginx/overlays/prod/ -n discover # kubectl apply -f resources/services/tugraz.yml -n discover","title":"Discovery Environment"},{"location":"deployments/DiscoveryEnvironment/#discovery-environment","text":"Prerequisites Make sure you have the harbor-registry-credentials secrets in your NAMESPACE, see also k8s-resources Make sure you have de-nginx-tls secret created, also see k8s-resources Make sure your Haproxy has the CA cert, Add /docker-tugraz-data/ca/ca.pem to HAPROXY_DOMAIN :/etc/ssl/certs/ca-bundle.crt` remember nginx is proxying all the services to the DE in CyVerse. Use the manifest files from the k8s-resources","title":"Discovery Environment"},{"location":"deployments/DiscoveryEnvironment/#change-hardcoded","text":"If you are using a diffrent domain instead of cyverse.tugraz.at , e.g. cyverse.at - change these two files. /k8s-resources/resources/kustomize/de-nginx/base/nginx.conf - server_name ~^[^.]+[.]cyverse[.]tugraz[.]at$; + server_name ~^[^.]+[.]cyverse[.]at$; k8s-resources/resources/kustomize/de-nginx/base/kustomization.yaml - namespace: prod + namespace: discover","title":"Change hardcoded"},{"location":"deployments/DiscoveryEnvironment/#deploy","text":"## For prod env kubectl apply -k resources/kustomize/de-nginx/overlays/prod/ -n prod kubectl apply -f resources/services/tugraz.yml -n prod ## For discover env # kubectl apply -k resources/kustomize/de-nginx/overlays/prod/ -n discover # kubectl apply -f resources/services/tugraz.yml -n discover","title":"Deploy"},{"location":"deployments/RabbitMQ/","text":"TODO \u00b6 Prerequisites Reindex RabbitMQ jobs \u00b6 access the vm where the RabbitMQ is installed. \u00b6 ssh root@RABBITMQ_HOST Configure rabbitmqadmin \u00b6 This step you have to do only once, if the rabbitmqadmin is not present. mkdir adm cd adm wget http://localhost:15672/cli/rabbitmqadmin chmod +x rabbitmqadmin Commands \u00b6 discover \u00b6 # check status systemctl status rabbitmq-server.service -l ## add your password to a temp var read -s PASSWORD && export PASSWORD # list exchange ./rabbitmqadmin -V /cyverse/de list exchanges -u cyverse -p $PASSWORD # publish the message to reindex all ./rabbitmqadmin publish -V /cyverse/de -u cyverse -p $PASSWORD exchange = de routing_key = index.all payload = \"\" # restart infosquito2 kubectl rollout restart deployment infosquito2 -n discover # IF NOT deployed # ./deploy.py -Bn discover -p infosquito2 -C prod \u00b6 ## add your password to a temp var read -s PASSWORD && export PASSWORD # list ./rabbitmqadmin -V /tugraz/de list exchanges -u tugraz -p $PASSWORD # publish the message to reindex all ./rabbitmqadmin publish -V /tugraz/de -u tugraz -p $PASSWORD exchange = de routing_key = index.all payload = \"\"","title":"RabbitMQ"},{"location":"deployments/RabbitMQ/#todo","text":"Prerequisites","title":"TODO"},{"location":"deployments/RabbitMQ/#reindex-rabbitmq-jobs","text":"","title":"Reindex RabbitMQ jobs"},{"location":"deployments/RabbitMQ/#access-the-vm-where-the-rabbitmq-is-installed","text":"ssh root@RABBITMQ_HOST","title":"access the vm where the RabbitMQ is installed."},{"location":"deployments/RabbitMQ/#configure-rabbitmqadmin","text":"This step you have to do only once, if the rabbitmqadmin is not present. mkdir adm cd adm wget http://localhost:15672/cli/rabbitmqadmin chmod +x rabbitmqadmin","title":"Configure rabbitmqadmin"},{"location":"deployments/RabbitMQ/#commands","text":"","title":"Commands"},{"location":"deployments/RabbitMQ/#discover","text":"# check status systemctl status rabbitmq-server.service -l ## add your password to a temp var read -s PASSWORD && export PASSWORD # list exchange ./rabbitmqadmin -V /cyverse/de list exchanges -u cyverse -p $PASSWORD # publish the message to reindex all ./rabbitmqadmin publish -V /cyverse/de -u cyverse -p $PASSWORD exchange = de routing_key = index.all payload = \"\" # restart infosquito2 kubectl rollout restart deployment infosquito2 -n discover # IF NOT deployed # ./deploy.py -Bn discover -p infosquito2 -C","title":"discover"},{"location":"deployments/RabbitMQ/#prod","text":"## add your password to a temp var read -s PASSWORD && export PASSWORD # list ./rabbitmqadmin -V /tugraz/de list exchanges -u tugraz -p $PASSWORD # publish the message to reindex all ./rabbitmqadmin publish -V /tugraz/de -u tugraz -p $PASSWORD exchange = de routing_key = index.all payload = \"\"","title":"prod"},{"location":"deployments/deployment_overview/","text":"Deployment Overview \u00b6 All deployments of CyVerse Products and Services are managed through Kubernetes. Each service is maintained in its own GitHub Repository in the core CyVerse Organization or CyVerse Discovery Environment Organization Deployments \u00b6 All of CyVerse primary services and database deployments are containers, controled via fully managed Kubernetes Discovery Environment - deploy primary data science workbench site Kubernetes (K8s) - deploy the main K8s cluster for running DE applications K8s Resources - deploy the various resources in DE managed by K8s K8s NameSpaces - list of namespaces used in DE User Portal - deploy the User Portal website via K8s OpenEBS - deploy K8s stateful workloads that require container attached storage KeyCloak - deploy K8s KeyCloak configuration Exim4 Mail - deploy exim4 (MTA) running as a smarthost via K8s Redis HA - installing the Redis Server and Redis Haproxy ElasticSearch - deploy stateful set ES cluster RabbitMQ - deploy RabbitMQ services Unleash - deploy the Unleash database Grouper - Internet2 Grouper Service iRODS CSI Driver - K8s Container Storage Interface (CSI Driver) for iRODS Local Exim - verification of email using Exim VICE - Manage K8s interactive jobs in DE Jaeger - open-source end-to-end distributed tracing Data Store - deploy the CyVerse data Store","title":"Overview"},{"location":"deployments/deployment_overview/#deployment-overview","text":"All deployments of CyVerse Products and Services are managed through Kubernetes. Each service is maintained in its own GitHub Repository in the core CyVerse Organization or CyVerse Discovery Environment Organization","title":"Deployment Overview"},{"location":"deployments/deployment_overview/#deployments","text":"All of CyVerse primary services and database deployments are containers, controled via fully managed Kubernetes Discovery Environment - deploy primary data science workbench site Kubernetes (K8s) - deploy the main K8s cluster for running DE applications K8s Resources - deploy the various resources in DE managed by K8s K8s NameSpaces - list of namespaces used in DE User Portal - deploy the User Portal website via K8s OpenEBS - deploy K8s stateful workloads that require container attached storage KeyCloak - deploy K8s KeyCloak configuration Exim4 Mail - deploy exim4 (MTA) running as a smarthost via K8s Redis HA - installing the Redis Server and Redis Haproxy ElasticSearch - deploy stateful set ES cluster RabbitMQ - deploy RabbitMQ services Unleash - deploy the Unleash database Grouper - Internet2 Grouper Service iRODS CSI Driver - K8s Container Storage Interface (CSI Driver) for iRODS Local Exim - verification of email using Exim VICE - Manage K8s interactive jobs in DE Jaeger - open-source end-to-end distributed tracing Data Store - deploy the CyVerse data Store","title":" Deployments"},{"location":"deployments/elasticsearch/","text":"Elasticsearch \u00b6 Deploying the statefulsets Elasticsearch cluster on kubernetes. Prerequisites clone the repo see also k8s-resources cd /k8s-resources In case you have limited resources change these: cd /k8s-resources/resources/addons/elasticsearch/elasticsearch.yml ```yaml replicas: 2 resources: requests: memory: \"4Gi\" limits: memory: \"4Gi\" - name: ES_JAVA_OPTS value: \"-Xms2g -Xmx2g\" ``` Deploy \u00b6 # deploy ES on prod env kubectl apply -n prod -f resources/addons/elasticsearch/elasticsearch.yml ## deploy ES on discover env # kubectl apply -n discover -f resources/addons/elasticsearch/elasticsearch.yml Indexing \u00b6 Preq \u00b6 Save below json to a file settings.json , we will use this file to index our elasticsearch. { \"mappings\" : { \"file\" : { \"properties\" : { \"creator\" : { \"type\" : \"keyword\" }, \"dateCreated\" : { \"type\" : \"date\" }, \"dateModified\" : { \"type\" : \"date\" }, \"fileSize\" : { \"type\" : \"long\" }, \"fileType\" : { \"type\" : \"keyword\" }, \"id\" : { \"type\" : \"keyword\" }, \"label\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" } }, \"analyzer\" : \"irods_entity\" }, \"metadata\" : { \"type\" : \"nested\" , \"properties\" : { \"attribute\" : { \"type\" : \"text\" , \"analyzer\" : \"irods_entity\" }, \"unit\" : { \"type\" : \"keyword\" }, \"value\" : { \"type\" : \"text\" } } }, \"path\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" } }, \"analyzer\" : \"irods_path\" }, \"userPermissions\" : { \"type\" : \"nested\" , \"properties\" : { \"permission\" : { \"type\" : \"keyword\" }, \"user\" : { \"type\" : \"keyword\" } } } } }, \"tag\" : { \"properties\" : { \"creator\" : { \"type\" : \"keyword\" }, \"dateCreated\" : { \"type\" : \"date\" }, \"dateModified\" : { \"type\" : \"date\" }, \"description\" : { \"type\" : \"text\" }, \"id\" : { \"type\" : \"keyword\" }, \"targets\" : { \"type\" : \"nested\" , \"properties\" : { \"id\" : { \"type\" : \"keyword\" }, \"type\" : { \"type\" : \"keyword\" } } }, \"value\" : { \"type\" : \"text\" , \"analyzer\" : \"tag_value\" } } }, \"file_metadata\" : { \"_parent\" : { \"type\" : \"file\" }, \"_routing\" : { \"required\" : true }, \"properties\" : { \"id\" : { \"type\" : \"keyword\" }, \"metadata\" : { \"type\" : \"nested\" , \"properties\" : { \"attribute\" : { \"type\" : \"text\" , \"analyzer\" : \"irods_entity\" }, \"unit\" : { \"type\" : \"keyword\" }, \"value\" : { \"type\" : \"text\" } } } } }, \"folder\" : { \"properties\" : { \"creator\" : { \"type\" : \"keyword\" }, \"dateCreated\" : { \"type\" : \"date\" }, \"dateModified\" : { \"type\" : \"date\" }, \"fileSize\" : { \"type\" : \"long\" }, \"fileType\" : { \"type\" : \"keyword\" }, \"id\" : { \"type\" : \"keyword\" }, \"label\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" } }, \"analyzer\" : \"irods_entity\" }, \"metadata\" : { \"type\" : \"nested\" , \"properties\" : { \"attribute\" : { \"type\" : \"text\" , \"analyzer\" : \"irods_entity\" }, \"unit\" : { \"type\" : \"keyword\" }, \"value\" : { \"type\" : \"text\" } } }, \"path\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" } }, \"analyzer\" : \"irods_path\" }, \"userPermissions\" : { \"type\" : \"nested\" , \"properties\" : { \"permission\" : { \"type\" : \"keyword\" }, \"user\" : { \"type\" : \"keyword\" } } } } }, \"folder_metadata\" : { \"_parent\" : { \"type\" : \"folder\" }, \"_routing\" : { \"required\" : true }, \"properties\" : { \"id\" : { \"type\" : \"keyword\" }, \"metadata\" : { \"type\" : \"nested\" , \"properties\" : { \"attribute\" : { \"type\" : \"text\" , \"analyzer\" : \"irods_entity\" }, \"unit\" : { \"type\" : \"keyword\" }, \"value\" : { \"type\" : \"text\" } } } } } }, \"settings\" : { \"index\" : { \"mapper\" : { \"dynamic\" : \"false\" }, \"analysis\" : { \"analyzer\" : { \"irods_entity\" : { \"filter\" : [ \"asciifolding\" , \"lowercase\" ], \"type\" : \"custom\" , \"tokenizer\" : \"irods_entity\" }, \"irods_path\" : { \"type\" : \"custom\" , \"tokenizer\" : \"irods_path\" }, \"tag_value\" : { \"filter\" : [ \"asciifolding\" , \"lowercase\" ], \"type\" : \"custom\" , \"tokenizer\" : \"keyword\" } }, \"tokenizer\" : { \"irods_entity\" : { \"type\" : \"keyword\" , \"buffer_size\" : \"2700\" }, \"irods_path\" : { \"type\" : \"path_hierarchy\" , \"buffer_size\" : \"2700\" } } }, \"number_of_replicas\" : \"1\" } } } Index \u00b6 For indexing we will run a container inside your namespace where the elasticsearch is running, and copy the settings.json file inside this container and run the following commands: ## run container if es in prod namespace kubectl run --namespace = prod --rm utils -it --image arunvelsriram/utils bash ## run container if es in discover namespace # kubectl run --namespace=discover --rm utils -it --image arunvelsriram/utils bash ######### PS open a new terminal######### ## copy the settings.json if PROD kubectl -n prod cp settings.json utils:/home/utils ## copy the settings.json if Discover # kubectl -n discover cp settings.json utils:/home/utils run indexing \u00b6 Inside the running container shell run this command to add the indexes. # check elasticsearch health curl -XGET \"http://elasticsearch:9200/_cluster/health?pretty\" # run indexing from file curl -sX PUT \"http://elasticsearch:9200/data\" -d @settings.json (optional) delete current indexes \u00b6 # delete data indexes curl -sX DELETE \"http://elasticsearch:9200/data\" # delete everything curl -sX DELETE \"http://elasticsearch:9200/*\" restart related services \u00b6 # kubectl rollout restart statefulset elasticsearch -n prod # not sure kubectl rollout restart deployment infosquito2 search -n <NAMESPACE>","title":"ElasticSearch"},{"location":"deployments/elasticsearch/#elasticsearch","text":"Deploying the statefulsets Elasticsearch cluster on kubernetes. Prerequisites clone the repo see also k8s-resources cd /k8s-resources In case you have limited resources change these: cd /k8s-resources/resources/addons/elasticsearch/elasticsearch.yml ```yaml replicas: 2 resources: requests: memory: \"4Gi\" limits: memory: \"4Gi\" - name: ES_JAVA_OPTS value: \"-Xms2g -Xmx2g\" ```","title":"Elasticsearch"},{"location":"deployments/elasticsearch/#deploy","text":"# deploy ES on prod env kubectl apply -n prod -f resources/addons/elasticsearch/elasticsearch.yml ## deploy ES on discover env # kubectl apply -n discover -f resources/addons/elasticsearch/elasticsearch.yml","title":"Deploy"},{"location":"deployments/elasticsearch/#indexing","text":"","title":"Indexing"},{"location":"deployments/elasticsearch/#preq","text":"Save below json to a file settings.json , we will use this file to index our elasticsearch. { \"mappings\" : { \"file\" : { \"properties\" : { \"creator\" : { \"type\" : \"keyword\" }, \"dateCreated\" : { \"type\" : \"date\" }, \"dateModified\" : { \"type\" : \"date\" }, \"fileSize\" : { \"type\" : \"long\" }, \"fileType\" : { \"type\" : \"keyword\" }, \"id\" : { \"type\" : \"keyword\" }, \"label\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" } }, \"analyzer\" : \"irods_entity\" }, \"metadata\" : { \"type\" : \"nested\" , \"properties\" : { \"attribute\" : { \"type\" : \"text\" , \"analyzer\" : \"irods_entity\" }, \"unit\" : { \"type\" : \"keyword\" }, \"value\" : { \"type\" : \"text\" } } }, \"path\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" } }, \"analyzer\" : \"irods_path\" }, \"userPermissions\" : { \"type\" : \"nested\" , \"properties\" : { \"permission\" : { \"type\" : \"keyword\" }, \"user\" : { \"type\" : \"keyword\" } } } } }, \"tag\" : { \"properties\" : { \"creator\" : { \"type\" : \"keyword\" }, \"dateCreated\" : { \"type\" : \"date\" }, \"dateModified\" : { \"type\" : \"date\" }, \"description\" : { \"type\" : \"text\" }, \"id\" : { \"type\" : \"keyword\" }, \"targets\" : { \"type\" : \"nested\" , \"properties\" : { \"id\" : { \"type\" : \"keyword\" }, \"type\" : { \"type\" : \"keyword\" } } }, \"value\" : { \"type\" : \"text\" , \"analyzer\" : \"tag_value\" } } }, \"file_metadata\" : { \"_parent\" : { \"type\" : \"file\" }, \"_routing\" : { \"required\" : true }, \"properties\" : { \"id\" : { \"type\" : \"keyword\" }, \"metadata\" : { \"type\" : \"nested\" , \"properties\" : { \"attribute\" : { \"type\" : \"text\" , \"analyzer\" : \"irods_entity\" }, \"unit\" : { \"type\" : \"keyword\" }, \"value\" : { \"type\" : \"text\" } } } } }, \"folder\" : { \"properties\" : { \"creator\" : { \"type\" : \"keyword\" }, \"dateCreated\" : { \"type\" : \"date\" }, \"dateModified\" : { \"type\" : \"date\" }, \"fileSize\" : { \"type\" : \"long\" }, \"fileType\" : { \"type\" : \"keyword\" }, \"id\" : { \"type\" : \"keyword\" }, \"label\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" } }, \"analyzer\" : \"irods_entity\" }, \"metadata\" : { \"type\" : \"nested\" , \"properties\" : { \"attribute\" : { \"type\" : \"text\" , \"analyzer\" : \"irods_entity\" }, \"unit\" : { \"type\" : \"keyword\" }, \"value\" : { \"type\" : \"text\" } } }, \"path\" : { \"type\" : \"text\" , \"fields\" : { \"keyword\" : { \"type\" : \"keyword\" } }, \"analyzer\" : \"irods_path\" }, \"userPermissions\" : { \"type\" : \"nested\" , \"properties\" : { \"permission\" : { \"type\" : \"keyword\" }, \"user\" : { \"type\" : \"keyword\" } } } } }, \"folder_metadata\" : { \"_parent\" : { \"type\" : \"folder\" }, \"_routing\" : { \"required\" : true }, \"properties\" : { \"id\" : { \"type\" : \"keyword\" }, \"metadata\" : { \"type\" : \"nested\" , \"properties\" : { \"attribute\" : { \"type\" : \"text\" , \"analyzer\" : \"irods_entity\" }, \"unit\" : { \"type\" : \"keyword\" }, \"value\" : { \"type\" : \"text\" } } } } } }, \"settings\" : { \"index\" : { \"mapper\" : { \"dynamic\" : \"false\" }, \"analysis\" : { \"analyzer\" : { \"irods_entity\" : { \"filter\" : [ \"asciifolding\" , \"lowercase\" ], \"type\" : \"custom\" , \"tokenizer\" : \"irods_entity\" }, \"irods_path\" : { \"type\" : \"custom\" , \"tokenizer\" : \"irods_path\" }, \"tag_value\" : { \"filter\" : [ \"asciifolding\" , \"lowercase\" ], \"type\" : \"custom\" , \"tokenizer\" : \"keyword\" } }, \"tokenizer\" : { \"irods_entity\" : { \"type\" : \"keyword\" , \"buffer_size\" : \"2700\" }, \"irods_path\" : { \"type\" : \"path_hierarchy\" , \"buffer_size\" : \"2700\" } } }, \"number_of_replicas\" : \"1\" } } }","title":"Preq"},{"location":"deployments/elasticsearch/#index","text":"For indexing we will run a container inside your namespace where the elasticsearch is running, and copy the settings.json file inside this container and run the following commands: ## run container if es in prod namespace kubectl run --namespace = prod --rm utils -it --image arunvelsriram/utils bash ## run container if es in discover namespace # kubectl run --namespace=discover --rm utils -it --image arunvelsriram/utils bash ######### PS open a new terminal######### ## copy the settings.json if PROD kubectl -n prod cp settings.json utils:/home/utils ## copy the settings.json if Discover # kubectl -n discover cp settings.json utils:/home/utils","title":"Index"},{"location":"deployments/elasticsearch/#run-indexing","text":"Inside the running container shell run this command to add the indexes. # check elasticsearch health curl -XGET \"http://elasticsearch:9200/_cluster/health?pretty\" # run indexing from file curl -sX PUT \"http://elasticsearch:9200/data\" -d @settings.json","title":"run indexing"},{"location":"deployments/elasticsearch/#optional-delete-current-indexes","text":"# delete data indexes curl -sX DELETE \"http://elasticsearch:9200/data\" # delete everything curl -sX DELETE \"http://elasticsearch:9200/*\"","title":"(optional) delete current indexes"},{"location":"deployments/elasticsearch/#restart-related-services","text":"# kubectl rollout restart statefulset elasticsearch -n prod # not sure kubectl rollout restart deployment infosquito2 search -n <NAMESPACE>","title":"restart related services"},{"location":"deployments/exim4/","text":"Mail \u00b6 Prerequisites exim4-helm A Helm chart to provide a exim4 deployment, exim4 (MTA) running as a smarthost. Deploy \u00b6 Add & update helm chart \u00b6 helm repo add exim4 https://mb-wali.github.io/exim4-helm helm repo update Install \u00b6 # replace the secrets with yours helm install exim4 --set secrets.EXIM_SMARTHOST = 'localhost' ,secrets.EXIM_PASSWORD = 'passw0rd' ,secrets.EXIM_ALLOWED_SENDERS = '*' exim4/exim4 --namespace mail --create-namespace --wait Debugging \u00b6 Once the pod is running # execute shell kubectl exec -it exim4-6ff546fb9f-ff47m -- bash # send a test mail echo \"This is test\" | mail -s \"The subject\" receiver@myhost.com -aFrom:sender@myhost.com Usage \u00b6 Use your deployed exim4 to send mails, e.g. connect from a another service. SMTP_HOST = exim4.mail.svc.cluster.local","title":"Mail"},{"location":"deployments/exim4/#mail","text":"Prerequisites exim4-helm A Helm chart to provide a exim4 deployment, exim4 (MTA) running as a smarthost.","title":"Mail"},{"location":"deployments/exim4/#deploy","text":"","title":"Deploy"},{"location":"deployments/exim4/#add-update-helm-chart","text":"helm repo add exim4 https://mb-wali.github.io/exim4-helm helm repo update","title":"Add &amp; update helm chart"},{"location":"deployments/exim4/#install","text":"# replace the secrets with yours helm install exim4 --set secrets.EXIM_SMARTHOST = 'localhost' ,secrets.EXIM_PASSWORD = 'passw0rd' ,secrets.EXIM_ALLOWED_SENDERS = '*' exim4/exim4 --namespace mail --create-namespace --wait","title":"Install"},{"location":"deployments/exim4/#debugging","text":"Once the pod is running # execute shell kubectl exec -it exim4-6ff546fb9f-ff47m -- bash # send a test mail echo \"This is test\" | mail -s \"The subject\" receiver@myhost.com -aFrom:sender@myhost.com","title":"Debugging"},{"location":"deployments/exim4/#usage","text":"Use your deployed exim4 to send mails, e.g. connect from a another service. SMTP_HOST = exim4.mail.svc.cluster.local","title":"Usage"},{"location":"deployments/grouper/","text":"Grouper \u00b6 Prerequisites Make sure grouper database is setup, see also grouper-db Deploy \u00b6 Grouper consist of two deployments, grouper-loader & grouper-ws . grouper-loader \u00b6 ## Deploy for prod env kubectl apply -f resources/deployments/grouper-loader.yml -n prod ## Deploy for discover env # kubectl apply -f resources/deployments/grouper-loader.yml -n discover grouper-ws \u00b6 ## Deploy for prod env kubectl apply -f resources/deployments/grouper-ws.yml -n prod ## Deploy for discover env # kubectl apply -f resources/deployments/grouper-ws.yml -n discover","title":"Grouper"},{"location":"deployments/grouper/#grouper","text":"Prerequisites Make sure grouper database is setup, see also grouper-db","title":"Grouper"},{"location":"deployments/grouper/#deploy","text":"Grouper consist of two deployments, grouper-loader & grouper-ws .","title":"Deploy"},{"location":"deployments/grouper/#grouper-loader","text":"## Deploy for prod env kubectl apply -f resources/deployments/grouper-loader.yml -n prod ## Deploy for discover env # kubectl apply -f resources/deployments/grouper-loader.yml -n discover","title":"grouper-loader"},{"location":"deployments/grouper/#grouper-ws","text":"## Deploy for prod env kubectl apply -f resources/deployments/grouper-ws.yml -n prod ## Deploy for discover env # kubectl apply -f resources/deployments/grouper-ws.yml -n discover","title":"grouper-ws"},{"location":"deployments/ingress-nginx/","text":"ingress-nginx \u00b6 Prerequisites ingress-nginx is used to give vice-apps ingresses, using nodeport and what not. deploy \u00b6 The deployment manifests are in k8s-resources . (optional) changing the env \u00b6 Modify file resources/kustomize/ingress-nginx/overlays/prod/args.yaml , to change the namespace. - --default-backend-service=prod/vice-default-backend + --default-backend-service=discover/vice-default-backend Deploy kustomize \u00b6 This script will create a namespace ingress-nginx . kubectl apply -k resources/kustomize/ingress-nginx/overlays/prod","title":"ingress-nginx"},{"location":"deployments/ingress-nginx/#ingress-nginx","text":"Prerequisites ingress-nginx is used to give vice-apps ingresses, using nodeport and what not.","title":"ingress-nginx"},{"location":"deployments/ingress-nginx/#deploy","text":"The deployment manifests are in k8s-resources .","title":"deploy"},{"location":"deployments/ingress-nginx/#optional-changing-the-env","text":"Modify file resources/kustomize/ingress-nginx/overlays/prod/args.yaml , to change the namespace. - --default-backend-service=prod/vice-default-backend + --default-backend-service=discover/vice-default-backend","title":"(optional) changing the env"},{"location":"deployments/ingress-nginx/#deploy-kustomize","text":"This script will create a namespace ingress-nginx . kubectl apply -k resources/kustomize/ingress-nginx/overlays/prod","title":"Deploy kustomize"},{"location":"deployments/irods-csi-driver/","text":"iRODS CSI Driver \u00b6 iRODS Container Storage Interface (CSI) Driver implements the CSI Specification to provide container orchestration engines (like Kubernetes) iRODS access. Prerequisites Before we install irods-csi-driver , we need to create a values.yaml file. Change the values accordingly and save the file as values.yaml . globalConfig : secret : stringData : client : \"irodsfuse\" host : <IRODS-SERVER-HOST> port : \"1247\" zone : \"TUG\" user : <IRODS-ADMIN-USER> password : <PASSWORD> retainData : \"false\" enforceProxyAccess : \"true\" mountPathWhitelist : \"/TUG/home\" nodeService : irodsPool : extraArgs : - --cache_size_max=10737418240 - --cache_root=/irodsfs_pool_cache - '--cache_timeout_settings=[{\"path\":\"/\",\"timeout\":\"-1ns\",\"inherit\":false},{\"path\":\"/TUG\",\"timeout\":\"-1ns\",\"inherit\":false},{\"path\":\"/TUG/home\",\"timeout\":\"5m\",\"inherit\":false},{\"path\":\"/TUG/home/shared\",\"timeout\":\"5m\",\"inherit\":true}]' Deploy \u00b6 we use helm to deploy irods-csi-driver. # Add the Helm repository. helm repo add irods-csi-driver-repo https://cyverse.github.io/irods-csi-driver-helm/ # Update the local repository caches. helm repo update # create namespace kubectl create namespace irods-csi-driver # install csi-driver # make sure to edit values-cyverse.at.yaml helm install -n irods-csi-driver irods-csi-driver irods-csi-driver-repo/irods-csi-driver -f ./values.yaml # or upgrade helm upgrade -n irods-csi-driver irods-csi-driver irods-csi-driver-repo/irods-csi-driver -f ./values.yaml Upgrading to a newer version \u00b6 When we want to upgrade the irods-csi-driver to a newer version, we need to stop all running vice-apps and delete all the pvcs. # update helm repo helm repo update # delete the pvc kubectl delete pvc -l app-type = interactive -n vice-apps # uninstall the irods-csi-driver helm uninstall irods-csi-driver -n irods-csi-driver # delete all the vice-apps deployments ## see below for the content of this file ./nuke-vice-analysis.sh $( kubectl get deployments -n vice-apps -l app-type = interactive -o name ) # install again helm install -n irods-csi-driver irods-csi-driver irods-csi-driver-repo/irods-csi-driver -f values.yaml install specific version \u00b6 helm install -n irods-csi-driver irods-csi-driver --version 0 .8.7 irods-csi-driver-repo/irods-csi-driver -f values.yaml ## try the latest helm install -n irods-csi-driver irods-csi-driver --version 0 .9.2 irods-csi-driver-repo/irods-csi-driver -f values.yaml NOTE \u00b6 With irods-csi-driver version > 0.8.7 , there\u2019s a small change on the configuration file user_config.yaml for driver installation. You will need to delete --cache_root and --temp_root flags if you used it. nuke-vice-analysis.sh \u00b6 function delete_resources () { local external_id = \" $1 \" kubectl -n vice-apps delete deployment \" ${ external_id } \" kubectl -n vice-apps delete service \"vice- ${ external_id } \" kubectl -n vice-apps delete ingress \" ${ external_id } \" kubectl -n vice-apps delete configmap \"excludes-file- ${ external_id } \" kubectl -n vice-apps delete configmap \"input-path-list- ${ external_id } \" } function remove_deployment_prefix () { local external_id = \" $1 \" echo -n \" $external_id \" | sed 's;^deployment.apps/;;' } # Iterate over all arguments on the command line. for id in \" $@ \" ; do delete_resources $( remove_deployment_prefix \" $id \" ) done","title":"iRODS CSI Driver"},{"location":"deployments/irods-csi-driver/#irods-csi-driver","text":"iRODS Container Storage Interface (CSI) Driver implements the CSI Specification to provide container orchestration engines (like Kubernetes) iRODS access. Prerequisites Before we install irods-csi-driver , we need to create a values.yaml file. Change the values accordingly and save the file as values.yaml . globalConfig : secret : stringData : client : \"irodsfuse\" host : <IRODS-SERVER-HOST> port : \"1247\" zone : \"TUG\" user : <IRODS-ADMIN-USER> password : <PASSWORD> retainData : \"false\" enforceProxyAccess : \"true\" mountPathWhitelist : \"/TUG/home\" nodeService : irodsPool : extraArgs : - --cache_size_max=10737418240 - --cache_root=/irodsfs_pool_cache - '--cache_timeout_settings=[{\"path\":\"/\",\"timeout\":\"-1ns\",\"inherit\":false},{\"path\":\"/TUG\",\"timeout\":\"-1ns\",\"inherit\":false},{\"path\":\"/TUG/home\",\"timeout\":\"5m\",\"inherit\":false},{\"path\":\"/TUG/home/shared\",\"timeout\":\"5m\",\"inherit\":true}]'","title":"iRODS CSI Driver"},{"location":"deployments/irods-csi-driver/#deploy","text":"we use helm to deploy irods-csi-driver. # Add the Helm repository. helm repo add irods-csi-driver-repo https://cyverse.github.io/irods-csi-driver-helm/ # Update the local repository caches. helm repo update # create namespace kubectl create namespace irods-csi-driver # install csi-driver # make sure to edit values-cyverse.at.yaml helm install -n irods-csi-driver irods-csi-driver irods-csi-driver-repo/irods-csi-driver -f ./values.yaml # or upgrade helm upgrade -n irods-csi-driver irods-csi-driver irods-csi-driver-repo/irods-csi-driver -f ./values.yaml","title":"Deploy"},{"location":"deployments/irods-csi-driver/#upgrading-to-a-newer-version","text":"When we want to upgrade the irods-csi-driver to a newer version, we need to stop all running vice-apps and delete all the pvcs. # update helm repo helm repo update # delete the pvc kubectl delete pvc -l app-type = interactive -n vice-apps # uninstall the irods-csi-driver helm uninstall irods-csi-driver -n irods-csi-driver # delete all the vice-apps deployments ## see below for the content of this file ./nuke-vice-analysis.sh $( kubectl get deployments -n vice-apps -l app-type = interactive -o name ) # install again helm install -n irods-csi-driver irods-csi-driver irods-csi-driver-repo/irods-csi-driver -f values.yaml","title":"Upgrading to a newer version"},{"location":"deployments/irods-csi-driver/#install-specific-version","text":"helm install -n irods-csi-driver irods-csi-driver --version 0 .8.7 irods-csi-driver-repo/irods-csi-driver -f values.yaml ## try the latest helm install -n irods-csi-driver irods-csi-driver --version 0 .9.2 irods-csi-driver-repo/irods-csi-driver -f values.yaml","title":"install specific version"},{"location":"deployments/irods-csi-driver/#note","text":"With irods-csi-driver version > 0.8.7 , there\u2019s a small change on the configuration file user_config.yaml for driver installation. You will need to delete --cache_root and --temp_root flags if you used it.","title":"NOTE"},{"location":"deployments/irods-csi-driver/#nuke-vice-analysissh","text":"function delete_resources () { local external_id = \" $1 \" kubectl -n vice-apps delete deployment \" ${ external_id } \" kubectl -n vice-apps delete service \"vice- ${ external_id } \" kubectl -n vice-apps delete ingress \" ${ external_id } \" kubectl -n vice-apps delete configmap \"excludes-file- ${ external_id } \" kubectl -n vice-apps delete configmap \"input-path-list- ${ external_id } \" } function remove_deployment_prefix () { local external_id = \" $1 \" echo -n \" $external_id \" | sed 's;^deployment.apps/;;' } # Iterate over all arguments on the command line. for id in \" $@ \" ; do delete_resources $( remove_deployment_prefix \" $id \" ) done","title":"nuke-vice-analysis.sh"},{"location":"deployments/jaeger/","text":"jaeger \u00b6 Jaeger: open source, end-to-end distributed tracing Prerequisites Deploy \u00b6 The deployment manifests are in k8s-resources . Create namespace kubectl create ns jaeger (optional) changing the env Modify files to change the namespace: resources/addons/jaeger/collector.yaml resources/addons/jaeger/query.yaml resources/addons/jaeger/rollover-cron.yaml - \"http://elasticsearch.prod:9200\" + \"http://elasticsearch.discover:9200\" Apply manifests kubectl apply -f resources/addons/jaeger/rollover-cron.yaml -n jaeger kubectl apply -f resources/addons/jaeger/query.yaml -n jaeger kubectl apply -f resources/addons/jaeger/collector.yaml -n jaeger","title":"Jaeger"},{"location":"deployments/jaeger/#jaeger","text":"Jaeger: open source, end-to-end distributed tracing Prerequisites","title":"jaeger"},{"location":"deployments/jaeger/#deploy","text":"The deployment manifests are in k8s-resources . Create namespace kubectl create ns jaeger (optional) changing the env Modify files to change the namespace: resources/addons/jaeger/collector.yaml resources/addons/jaeger/query.yaml resources/addons/jaeger/rollover-cron.yaml - \"http://elasticsearch.prod:9200\" + \"http://elasticsearch.discover:9200\" Apply manifests kubectl apply -f resources/addons/jaeger/rollover-cron.yaml -n jaeger kubectl apply -f resources/addons/jaeger/query.yaml -n jaeger kubectl apply -f resources/addons/jaeger/collector.yaml -n jaeger","title":"Deploy"},{"location":"deployments/k8s-namespace/","text":"Namespaces \u00b6 List of namespaces used for CyVerse deployment in kubernetes. Prerequisites Note : this documentation is relavant only for single/current environment. prod \u00b6 This namespace is dedicated to the core services & some of non-core services such as : redis-ha redis-haproxy elasticsearch Except the irods-csi-driver which runs in a specific namespace. ingress-nginx \u00b6 This namespace is dedicated to the ingress-nginx deployment. vice-apps \u00b6 This namespace is dedicated to the VICE related deployment and configurations. keycloak \u00b6 This namespace is dedicated to Keycloak related deployment and configurations. openebs \u00b6 This namespace is dedicated to the openebs deployment. OpenEBS turns any storage available on the Kubernetes worker nodes into local or distributed Kubernetes Persistent Volumes. irods-csi-driver \u00b6 This namespace is dedicated to the irods-csi-driver deployment and configurations.","title":"K8s Name Spaces"},{"location":"deployments/k8s-namespace/#namespaces","text":"List of namespaces used for CyVerse deployment in kubernetes. Prerequisites Note : this documentation is relavant only for single/current environment.","title":"Namespaces"},{"location":"deployments/k8s-namespace/#prod","text":"This namespace is dedicated to the core services & some of non-core services such as : redis-ha redis-haproxy elasticsearch Except the irods-csi-driver which runs in a specific namespace.","title":"prod"},{"location":"deployments/k8s-namespace/#ingress-nginx","text":"This namespace is dedicated to the ingress-nginx deployment.","title":"ingress-nginx"},{"location":"deployments/k8s-namespace/#vice-apps","text":"This namespace is dedicated to the VICE related deployment and configurations.","title":"vice-apps"},{"location":"deployments/k8s-namespace/#keycloak","text":"This namespace is dedicated to Keycloak related deployment and configurations.","title":"keycloak"},{"location":"deployments/k8s-namespace/#openebs","text":"This namespace is dedicated to the openebs deployment. OpenEBS turns any storage available on the Kubernetes worker nodes into local or distributed Kubernetes Persistent Volumes.","title":"openebs"},{"location":"deployments/k8s-namespace/#irods-csi-driver","text":"This namespace is dedicated to the irods-csi-driver deployment and configurations.","title":"irods-csi-driver"},{"location":"deployments/k8s-resources/","text":"k8s-resources \u00b6 This repository includes all the manifests and resources for kubernetes deployment. Prerequisites Clone \u00b6 git clone git@gitlab.cyverse.org:tugraz/k8s-resources.git Generate config/secrets for prod env \u00b6 Make sure gomplate is installed in your OS. Here is an example on ubuntu: sudo curl -o /usr/local/bin/gomplate -sSL https://github.com/hairyhenderson/gomplate/releases/download/v3.10.0/gomplate_linux-amd64 sudo chmod 755 /usr/local/bin/gomplate gomplate --help Generate configs/secrets ./generate_configs.py -e prod ./generate_secrets.py -e prod Load config/secrets in your cluster for prod env \u00b6 ./load_configs.py -e prod -n prod ./load_secrets.py -e prod -n prod Deploy services \u00b6 preq \u00b6 Before we start to deploy the services we need to create these secrets, and install some dependencies. vice-image-pull-secret TODO: kubectl apply. harbor-registry-credentials TODO: kubectl apply. These secrets below can be deployed using: git clone -b discover https://gitlab.cyverse.org/tugraz/docker-tugraz-data see also cyverse.at repo. gpg-keys ui-nginx-tls gpg-keys pgpass-files signing-keys accepted-keys ssl-files Make sure elasticsearch is deployed. Make sure skaffold is installed in your OS. # For Linux x86_64 (amd64) curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 sudo install skaffold /usr/local/bin/ # check skaffold --help Make sure ServiceAccounts are created: # For prod env kubectl apply -f /k8s-resources/resources/serviceaccounts/app-exposer.yml -n prod ## For discover env # kubectl apply -f /k8s-resources/resources/serviceaccounts/app-exposer.yml -n discover TODO above \u00b6 Deploy \u00b6 This command will deploy all the services listed on k8s-resources/repos # deploy services for prod env ./deploy.py -n prod -BCa ## deploy services for discover env # ./deploy.py -n discover -BCa Deploy single service \u00b6 If you want to deploy a single service, e.g. search ## Run for the prod env ./deploy.py -Bn prod -p search -C ## Run for the discover env # ./deploy.py -Bn discover -p search -C create a new env \u00b6 Currently we have prod environment which is our productive instance. Create your environment: cp config_values/prod.yaml config_values/discover.yaml This will create a new config file for your environment. You will have to update all the values as you see fit for your environment. fill in all these values: # discover.yaml --- Environment : Agave : Key : Secret : RedirectURI : StorageSystem : CallbackBaseURI : ReadTimeout : Enabled : JobsEnabled : AMQP : URI : AnonFiles : BaseURI : AppExposer : BaseURI : BaseURLs : Analyses : Apps : AsyncTasks : DashboardAggregator : DataInfo : GrouperWebServices : IplantEmail : IplantGroups : JexAdapter : Metadata : Notifications : Permissions : Requests : Search : Terrain : UserInfo : CAS : BaseURI : ServerName : UIDDomain : DashboardAggregator : PublicGroup : LogLevel : DataOne : BaseURI : DE : Version : VersionName : AMQP : URI : Host : BaseURI : Legacy : BaseURI : Subscriptions : CheckoutURL : KeepAlive : Service : Target : ContextMenu : Enabled : BaseTrash : Path : ProdDeployment : DefaultOutputFolder : WSO2 : JWTHeader : Coge : BaseURI : Tools : Admin : MaxCpuLimit : MaxMemoryLimit : MaxDiskLimit : Docker : TrustedRegistries : Tag : Elasticsearch : BaseURI : Username : Password : Index : Email : AppDeletion : Src : Dest : AppPublicationRequest : Src : Dest : ToolRequest : Src : Dest : PermIDRequest : Src : Dest : Support : Src : Dest : Grouper : Environment : MorphString : WebService : Password : Password : DB : User : Password : Host : Port : Name : FolderNamePrefix : Loader : URI : User : Password : SubjectSource : ID : Name : SearchBase : ICAT : Host : Port : User : Password : Infosquito : DayNum : PrefixLength : InteractiveApps : BaseURI : ServiceSuffix : Intercom : AppID : CompanyID : CompanyName : Intercom : IRODS : AMQP : URI : Host : User : Zone : Password : AdminUsers : PermsFilter : ExternalHost : QuotaRootResources : Jobs : DataTransferImage : JobStatusListener : BaseURI : Keycloak : ServerURI : Realm : ClientID : ClientSecret : VICE : ClientID : ClientSecret : Kifshare : ExternalUri : PGP : KeyPassword : PermanentID : CuratorsGroup : DataCite : BaseURI : User : Password : DOIPrefix : Redis : Host : Port : HA : Name : Password : DB : Number : TimeZone : Vault : Token : URL : IRODS : MountPath : ChildToken : UseLimit : VICE : DB : User : Password : Host : Port : Name : FileTransfers : Image : Tag : JobStatus : BaseURI : K8sEnabled : BackendNamespace : ImagePullSecret : ImageCache : UseCSIDriver : DefaultImage : DefaultName : DefaultCasUrl : DefaultCasValidate : ConcurrentJobs : UseCaseCharsMin : DefaultBackend : LoadingPageTemplateString : Sonora : BaseURI : Terrain : CASClientID : CASClientSecret : JWT : SigningKey : Password : Unleash : BaseUrl : APIPath : APIToken : MaintenanceFlag : UserPortal : BaseURI : DEDB : User : Password : Host : Port : Name : NewNotificationsDB : User : Password : Host : Port : Name : NotificationsDB : User : Password : Host : Port : Name : PermissionsDB : User : Password : Host : Port : Name : QMSDB : User : Password : Host : Port : Name : Reinitialize : MetadataDB : User : Password : Host : Port : Name : UnleashDB : User : Password : Host : Port : Name : Admin : Groups : Attribute : FileIdentifier : HtPathList : MultiInputPathList : Analytics : Enabled : Id : Harbor : URL : ProjectQARobotName : ProjectQARobotSecret : QMS : Enabled : Base : Usage : Jaeger : Endpoint : Generate config/secrets for discover env \u00b6 ./generate_configs.py -e discover ./generate_secrets.py -e discover Load config/secrets in your cluster for discover env \u00b6 ./load_configs.py -e discover -n discover ./load_secrets.py -e discover -n discover","title":"K8s Resources"},{"location":"deployments/k8s-resources/#k8s-resources","text":"This repository includes all the manifests and resources for kubernetes deployment. Prerequisites","title":"k8s-resources"},{"location":"deployments/k8s-resources/#clone","text":"git clone git@gitlab.cyverse.org:tugraz/k8s-resources.git","title":"Clone"},{"location":"deployments/k8s-resources/#generate-configsecrets-for-prod-env","text":"Make sure gomplate is installed in your OS. Here is an example on ubuntu: sudo curl -o /usr/local/bin/gomplate -sSL https://github.com/hairyhenderson/gomplate/releases/download/v3.10.0/gomplate_linux-amd64 sudo chmod 755 /usr/local/bin/gomplate gomplate --help Generate configs/secrets ./generate_configs.py -e prod ./generate_secrets.py -e prod","title":"Generate config/secrets for prod env"},{"location":"deployments/k8s-resources/#load-configsecrets-in-your-cluster-for-prod-env","text":"./load_configs.py -e prod -n prod ./load_secrets.py -e prod -n prod","title":"Load config/secrets in your cluster for prod env"},{"location":"deployments/k8s-resources/#deploy-services","text":"","title":"Deploy services"},{"location":"deployments/k8s-resources/#preq","text":"Before we start to deploy the services we need to create these secrets, and install some dependencies. vice-image-pull-secret TODO: kubectl apply. harbor-registry-credentials TODO: kubectl apply. These secrets below can be deployed using: git clone -b discover https://gitlab.cyverse.org/tugraz/docker-tugraz-data see also cyverse.at repo. gpg-keys ui-nginx-tls gpg-keys pgpass-files signing-keys accepted-keys ssl-files Make sure elasticsearch is deployed. Make sure skaffold is installed in your OS. # For Linux x86_64 (amd64) curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 sudo install skaffold /usr/local/bin/ # check skaffold --help Make sure ServiceAccounts are created: # For prod env kubectl apply -f /k8s-resources/resources/serviceaccounts/app-exposer.yml -n prod ## For discover env # kubectl apply -f /k8s-resources/resources/serviceaccounts/app-exposer.yml -n discover","title":"preq"},{"location":"deployments/k8s-resources/#todo-above","text":"","title":"TODO above"},{"location":"deployments/k8s-resources/#deploy","text":"This command will deploy all the services listed on k8s-resources/repos # deploy services for prod env ./deploy.py -n prod -BCa ## deploy services for discover env # ./deploy.py -n discover -BCa","title":"Deploy"},{"location":"deployments/k8s-resources/#deploy-single-service","text":"If you want to deploy a single service, e.g. search ## Run for the prod env ./deploy.py -Bn prod -p search -C ## Run for the discover env # ./deploy.py -Bn discover -p search -C","title":"Deploy single service"},{"location":"deployments/k8s-resources/#create-a-new-env","text":"Currently we have prod environment which is our productive instance. Create your environment: cp config_values/prod.yaml config_values/discover.yaml This will create a new config file for your environment. You will have to update all the values as you see fit for your environment. fill in all these values: # discover.yaml --- Environment : Agave : Key : Secret : RedirectURI : StorageSystem : CallbackBaseURI : ReadTimeout : Enabled : JobsEnabled : AMQP : URI : AnonFiles : BaseURI : AppExposer : BaseURI : BaseURLs : Analyses : Apps : AsyncTasks : DashboardAggregator : DataInfo : GrouperWebServices : IplantEmail : IplantGroups : JexAdapter : Metadata : Notifications : Permissions : Requests : Search : Terrain : UserInfo : CAS : BaseURI : ServerName : UIDDomain : DashboardAggregator : PublicGroup : LogLevel : DataOne : BaseURI : DE : Version : VersionName : AMQP : URI : Host : BaseURI : Legacy : BaseURI : Subscriptions : CheckoutURL : KeepAlive : Service : Target : ContextMenu : Enabled : BaseTrash : Path : ProdDeployment : DefaultOutputFolder : WSO2 : JWTHeader : Coge : BaseURI : Tools : Admin : MaxCpuLimit : MaxMemoryLimit : MaxDiskLimit : Docker : TrustedRegistries : Tag : Elasticsearch : BaseURI : Username : Password : Index : Email : AppDeletion : Src : Dest : AppPublicationRequest : Src : Dest : ToolRequest : Src : Dest : PermIDRequest : Src : Dest : Support : Src : Dest : Grouper : Environment : MorphString : WebService : Password : Password : DB : User : Password : Host : Port : Name : FolderNamePrefix : Loader : URI : User : Password : SubjectSource : ID : Name : SearchBase : ICAT : Host : Port : User : Password : Infosquito : DayNum : PrefixLength : InteractiveApps : BaseURI : ServiceSuffix : Intercom : AppID : CompanyID : CompanyName : Intercom : IRODS : AMQP : URI : Host : User : Zone : Password : AdminUsers : PermsFilter : ExternalHost : QuotaRootResources : Jobs : DataTransferImage : JobStatusListener : BaseURI : Keycloak : ServerURI : Realm : ClientID : ClientSecret : VICE : ClientID : ClientSecret : Kifshare : ExternalUri : PGP : KeyPassword : PermanentID : CuratorsGroup : DataCite : BaseURI : User : Password : DOIPrefix : Redis : Host : Port : HA : Name : Password : DB : Number : TimeZone : Vault : Token : URL : IRODS : MountPath : ChildToken : UseLimit : VICE : DB : User : Password : Host : Port : Name : FileTransfers : Image : Tag : JobStatus : BaseURI : K8sEnabled : BackendNamespace : ImagePullSecret : ImageCache : UseCSIDriver : DefaultImage : DefaultName : DefaultCasUrl : DefaultCasValidate : ConcurrentJobs : UseCaseCharsMin : DefaultBackend : LoadingPageTemplateString : Sonora : BaseURI : Terrain : CASClientID : CASClientSecret : JWT : SigningKey : Password : Unleash : BaseUrl : APIPath : APIToken : MaintenanceFlag : UserPortal : BaseURI : DEDB : User : Password : Host : Port : Name : NewNotificationsDB : User : Password : Host : Port : Name : NotificationsDB : User : Password : Host : Port : Name : PermissionsDB : User : Password : Host : Port : Name : QMSDB : User : Password : Host : Port : Name : Reinitialize : MetadataDB : User : Password : Host : Port : Name : UnleashDB : User : Password : Host : Port : Name : Admin : Groups : Attribute : FileIdentifier : HtPathList : MultiInputPathList : Analytics : Enabled : Id : Harbor : URL : ProjectQARobotName : ProjectQARobotSecret : QMS : Enabled : Base : Usage : Jaeger : Endpoint :","title":"create a new env"},{"location":"deployments/k8s-resources/#generate-configsecrets-for-discover-env","text":"./generate_configs.py -e discover ./generate_secrets.py -e discover","title":"Generate config/secrets for discover env"},{"location":"deployments/k8s-resources/#load-configsecrets-in-your-cluster-for-discover-env","text":"./load_configs.py -e discover -n discover ./load_secrets.py -e discover -n discover","title":"Load config/secrets in your cluster for discover env"},{"location":"deployments/keycloak/","text":"Keycloak \u00b6 Prerequisites configure database \u00b6 To setup and configure database please have a look at keycloak database . create namsespace \u00b6 # create ns kubectl create ns keycloak Create required secrets & configmap \u00b6 Keycloak deployment requires secrets and configmaps, which can be done via a kustomization.yaml file, please see below for an example of this template: secretGenerator : - name : dbuser # Database literals : - username=<USERNAME> - password=<PASSWORD> - name : kcadmin # keyclaok literals : - username=<USERNAME> - password=<PASSWORD> configMapGenerator : - name : keycloak-config literals : - KEYCLOAK_HOSTNAME=keycloak.example.com - KEYCLOAK_LOGLEVEL=INFO - DB_VENDOR=postgres - DB_ADDR=<DATABASE_HOST> - DB_PORT=5432 - PROXY_ADDRESS_FORWARDING=true - JDBC_PARAMS=connectTimeout=21600 - JAVA_OPTS=-server -Xms4096m -Xmx8192m -XX:MetaspaceSize=96m -XX:MaxMetaspaceSize=256m -Djboss.modules.system.pkgs=org.jboss.byteman -Djava.awt.headless=true -Dkeycloak.profile.feature.token_exchange=enabled -Djava.security.egd=file:/dev/urandom namespace : keycloak resources : - deployment.yaml - service.yaml generatorOptions : disableNameSuffixHash : true Update the values of the kustomization.yaml file. place the kustomization.yaml , deployment.yaml and service.yaml inside a directory. e.g. base deployment & service YAML files: \u00b6 TODO: find a way to add those files. Deploy \u00b6 # apply kustomize kubectl apply -k ./base/ -n keycloak","title":"KeyCloak"},{"location":"deployments/keycloak/#keycloak","text":"Prerequisites","title":"Keycloak"},{"location":"deployments/keycloak/#configure-database","text":"To setup and configure database please have a look at keycloak database .","title":"configure database"},{"location":"deployments/keycloak/#create-namsespace","text":"# create ns kubectl create ns keycloak","title":"create namsespace"},{"location":"deployments/keycloak/#create-required-secrets-configmap","text":"Keycloak deployment requires secrets and configmaps, which can be done via a kustomization.yaml file, please see below for an example of this template: secretGenerator : - name : dbuser # Database literals : - username=<USERNAME> - password=<PASSWORD> - name : kcadmin # keyclaok literals : - username=<USERNAME> - password=<PASSWORD> configMapGenerator : - name : keycloak-config literals : - KEYCLOAK_HOSTNAME=keycloak.example.com - KEYCLOAK_LOGLEVEL=INFO - DB_VENDOR=postgres - DB_ADDR=<DATABASE_HOST> - DB_PORT=5432 - PROXY_ADDRESS_FORWARDING=true - JDBC_PARAMS=connectTimeout=21600 - JAVA_OPTS=-server -Xms4096m -Xmx8192m -XX:MetaspaceSize=96m -XX:MaxMetaspaceSize=256m -Djboss.modules.system.pkgs=org.jboss.byteman -Djava.awt.headless=true -Dkeycloak.profile.feature.token_exchange=enabled -Djava.security.egd=file:/dev/urandom namespace : keycloak resources : - deployment.yaml - service.yaml generatorOptions : disableNameSuffixHash : true Update the values of the kustomization.yaml file. place the kustomization.yaml , deployment.yaml and service.yaml inside a directory. e.g. base","title":"Create required secrets &amp; configmap"},{"location":"deployments/keycloak/#deployment-service-yaml-files","text":"TODO: find a way to add those files.","title":"deployment &amp; service YAML files:"},{"location":"deployments/keycloak/#deploy","text":"# apply kustomize kubectl apply -k ./base/ -n keycloak","title":"Deploy"},{"location":"deployments/kubernetes-deploy/","text":"Kubernetes Cluster \u00b6 Prerequisites Make sure you have at least 6 VMs configured with Centos7 1 Master node 4 worker nodes 1 worker node dedicated only for vice-apps Make sure you have Ansible installed, and you can reach your VMs via ssh TODO rewrite the docs once ansible-k8s-centos7 is ready. \u00b6 We are using Ansible to set up a kubernetes cluster. Current kubernetes cluster is configured using ansible playbooks from deployments/ansible/kubernetes . Looking to setup your own k8s cluster? \u00b6 If your are intrested to setup your own kubernetes cluster on Centos 7, please follow the steps bellow. Steps \u00b6 Clone the repo \u00b6 # clone repo git clone https://github.com/cyverse-de/deployments.git # navigate to ansible playbooks for k8s cd /ansible/kubernetes create your inventory \u00b6 Create your inventory file under /inventory/cyverse and replace the host names as yours. [ k8s:children ] k8s-control-plane k8s-worker [ kube-apiserver-haproxy ] k8s-reverse-proxy.example.com [ k8s-control-plane ] k8s-c1.example.com [ k8s-storage:children ] k8s-worker [ k8s-worker ] k8s-w1.example.com k8s-w2.example.com k8s-w3.example.com k8s-w4.example.com vice-w1.example.com [ outward-facing-proxy ] vice-haproxy.example.com [ haproxy ] vice-haproxy.example.com [ vice-workers ] vice-w1.example.com Run playbooks \u00b6 Note: we are using --user root , if your virtual machines have a diffrent user you could change this. Check if your hosts are reachable \u00b6 ansible -i inventory/ -m ping all --user root Setup firewall configs \u00b6 ansible-playbook -i inventory/ firewalld-config.yml --user root Provision your nodes \u00b6 ansible-playbook -i inventory/ provision-nodes.yml --user root Tainting and Labeling VICE Worker Nodes \u00b6 kubectl label nodes vice-w1.example.com vice = true kubectl taint nodes vice-w1.example.com vice = only:NoSchedule","title":"Kubernetes (K8s)"},{"location":"deployments/kubernetes-deploy/#kubernetes-cluster","text":"Prerequisites Make sure you have at least 6 VMs configured with Centos7 1 Master node 4 worker nodes 1 worker node dedicated only for vice-apps Make sure you have Ansible installed, and you can reach your VMs via ssh","title":"Kubernetes Cluster"},{"location":"deployments/kubernetes-deploy/#todo-rewrite-the-docs-once-ansible-k8s-centos7-is-ready","text":"We are using Ansible to set up a kubernetes cluster. Current kubernetes cluster is configured using ansible playbooks from deployments/ansible/kubernetes .","title":"TODO rewrite the docs once ansible-k8s-centos7 is ready."},{"location":"deployments/kubernetes-deploy/#looking-to-setup-your-own-k8s-cluster","text":"If your are intrested to setup your own kubernetes cluster on Centos 7, please follow the steps bellow.","title":"Looking to setup your own k8s cluster?"},{"location":"deployments/kubernetes-deploy/#steps","text":"","title":"Steps"},{"location":"deployments/kubernetes-deploy/#clone-the-repo","text":"# clone repo git clone https://github.com/cyverse-de/deployments.git # navigate to ansible playbooks for k8s cd /ansible/kubernetes","title":"Clone the repo"},{"location":"deployments/kubernetes-deploy/#create-your-inventory","text":"Create your inventory file under /inventory/cyverse and replace the host names as yours. [ k8s:children ] k8s-control-plane k8s-worker [ kube-apiserver-haproxy ] k8s-reverse-proxy.example.com [ k8s-control-plane ] k8s-c1.example.com [ k8s-storage:children ] k8s-worker [ k8s-worker ] k8s-w1.example.com k8s-w2.example.com k8s-w3.example.com k8s-w4.example.com vice-w1.example.com [ outward-facing-proxy ] vice-haproxy.example.com [ haproxy ] vice-haproxy.example.com [ vice-workers ] vice-w1.example.com","title":"create your inventory"},{"location":"deployments/kubernetes-deploy/#run-playbooks","text":"Note: we are using --user root , if your virtual machines have a diffrent user you could change this.","title":"Run playbooks"},{"location":"deployments/kubernetes-deploy/#check-if-your-hosts-are-reachable","text":"ansible -i inventory/ -m ping all --user root","title":"Check if your hosts are reachable"},{"location":"deployments/kubernetes-deploy/#setup-firewall-configs","text":"ansible-playbook -i inventory/ firewalld-config.yml --user root","title":"Setup firewall configs"},{"location":"deployments/kubernetes-deploy/#provision-your-nodes","text":"ansible-playbook -i inventory/ provision-nodes.yml --user root","title":"Provision your nodes"},{"location":"deployments/kubernetes-deploy/#tainting-and-labeling-vice-worker-nodes","text":"kubectl label nodes vice-w1.example.com vice = true kubectl taint nodes vice-w1.example.com vice = only:NoSchedule","title":"Tainting and Labeling VICE Worker Nodes"},{"location":"deployments/local-exim/","text":"Local Exim (exim-sender) \u00b6 local-exim or also known as exim-sender Prerequisites Deploy \u00b6 ## deploy for prod env kubectl apply -f resources/deployments/exim-sender.yml -n prod ## deploy for prod discover # kubectl apply -f resources/deployments/exim-sender.yml -n discover","title":"Local Exim"},{"location":"deployments/local-exim/#local-exim-exim-sender","text":"local-exim or also known as exim-sender Prerequisites","title":"Local Exim (exim-sender)"},{"location":"deployments/local-exim/#deploy","text":"## deploy for prod env kubectl apply -f resources/deployments/exim-sender.yml -n prod ## deploy for prod discover # kubectl apply -f resources/deployments/exim-sender.yml -n discover","title":"Deploy"},{"location":"deployments/openebs/","text":"OpenEBS \u00b6 Prerequisites Deploy \u00b6 # create namespace kubectl create ns openebs # deploy kubectl -n openebs apply -f https://openebs.github.io/charts/openebs-operator.yaml","title":"OpenEBS"},{"location":"deployments/openebs/#openebs","text":"Prerequisites","title":"OpenEBS"},{"location":"deployments/openebs/#deploy","text":"# create namespace kubectl create ns openebs # deploy kubectl -n openebs apply -f https://openebs.github.io/charts/openebs-operator.yaml","title":"Deploy"},{"location":"deployments/redis-ha/","text":"Redis HA \u00b6 In this Document we will cover installing the Redis Server and Redis Haproxy , due to the k8s-resources configurations both would be installed and configured inside the prod namespace. Prerequisites Make sure to create a values.yaml and replace add your credentials values.yaml Overrides the PersistentVolume, adds secrets and uses openebs as storage class. ## replicas number for each component replicas : 3 persistentVolume : enabled : true ## redis-ha data Persistent Volume Storage Class ## If defined, storageClassName: <storageClass> ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS & OpenStack) ## storageClass : openebs-hostpath ## Sentinel specific configuration options sentinel : auth : true authkey : <AUTH-KEY> password : <PASSWORD> ## Redis specific configuration options auth : true authkey : <AUTH-KEY> redisPassword : <PASSWORD> Redis Server & Sentinel \u00b6 # add helm repo helm repo add dandydev https://dandydeveloper.github.io/charts helm repo update ## deploy redis servers for prod env helm upgrade --install --namespace prod redis-ha dandydev/redis-ha --values values.yaml ## deploy redis server for discover env # helm upgrade --install --namespace discover redis-ha dandydev/redis-ha --values values.yaml Redis Haproxy \u00b6 Preq \u00b6 Make sure to load the configs/secrets see also k8s-resources Deploy redis-haproxy \u00b6 ## deploy for prod env kubectl apply -n prod -f resources/deployments/redis-haproxy.yml ## deploy for discover env # kubectl apply -n discover -f resources/deployments/redis-haproxy.yml","title":"Redis HA"},{"location":"deployments/redis-ha/#redis-ha","text":"In this Document we will cover installing the Redis Server and Redis Haproxy , due to the k8s-resources configurations both would be installed and configured inside the prod namespace. Prerequisites Make sure to create a values.yaml and replace add your credentials values.yaml Overrides the PersistentVolume, adds secrets and uses openebs as storage class. ## replicas number for each component replicas : 3 persistentVolume : enabled : true ## redis-ha data Persistent Volume Storage Class ## If defined, storageClassName: <storageClass> ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS & OpenStack) ## storageClass : openebs-hostpath ## Sentinel specific configuration options sentinel : auth : true authkey : <AUTH-KEY> password : <PASSWORD> ## Redis specific configuration options auth : true authkey : <AUTH-KEY> redisPassword : <PASSWORD>","title":"Redis HA"},{"location":"deployments/redis-ha/#redis-server-sentinel","text":"# add helm repo helm repo add dandydev https://dandydeveloper.github.io/charts helm repo update ## deploy redis servers for prod env helm upgrade --install --namespace prod redis-ha dandydev/redis-ha --values values.yaml ## deploy redis server for discover env # helm upgrade --install --namespace discover redis-ha dandydev/redis-ha --values values.yaml","title":"Redis Server &amp; Sentinel"},{"location":"deployments/redis-ha/#redis-haproxy","text":"","title":"Redis Haproxy"},{"location":"deployments/redis-ha/#preq","text":"Make sure to load the configs/secrets see also k8s-resources","title":"Preq"},{"location":"deployments/redis-ha/#deploy-redis-haproxy","text":"## deploy for prod env kubectl apply -n prod -f resources/deployments/redis-haproxy.yml ## deploy for discover env # kubectl apply -n discover -f resources/deployments/redis-haproxy.yml","title":"Deploy redis-haproxy"},{"location":"deployments/unleash/","text":"Unleash \u00b6 Prerequisites Make sure unleash database is setup, see also unleash-db Deploy \u00b6 ## Deploy for prod env kubectl apply -f resources/deployments/unleash.yml -n prod ## Deploy for discover env # kubectl apply -f resources/deployments/unleash.yml -n discover","title":"Unleash"},{"location":"deployments/unleash/#unleash","text":"Prerequisites Make sure unleash database is setup, see also unleash-db","title":"Unleash"},{"location":"deployments/unleash/#deploy","text":"## Deploy for prod env kubectl apply -f resources/deployments/unleash.yml -n prod ## Deploy for discover env # kubectl apply -f resources/deployments/unleash.yml -n discover","title":"Deploy"},{"location":"deployments/userportal/","text":"User Portal \u00b6 Prerequisites TODO: add source location. \u00b6 LDAP \u00b6 Setup required ldap user. create portal user \u00b6 Create portal-user.ldif \u00b6 dn: uid=portal,ou=People,dc=tugraz,dc=at objectClass: inetOrgPerson objectClass: posixAccount objectClass: shadowAccount uid: portal mail: portal@example.com sn: SeviceAccount givenName: PORTAL cn: portal title: Other o: N/A departmentNumber: N/A uidNumber: 40003 gidNumber: 10003 homeDirectory: /home/portal Apply ldap file \u00b6 # replace LDAP_PASSWORD ldapadd -x -D \"cn=Manager,dc=tugraz,dc=at\" -w \"LDAP_PASSWORD\" -f portal-user.ldif # create a password for the user ## replace PORTAL_PASSWORD & LDAP_PASSWORD ldappasswd -x -D cn = Manager,dc = tugraz,dc = at -w \"LDAP_PASSWORD\" -s \"PORTAL_PASSWORD\" \"uid=portal,ou=People,dc=tugraz,dc=at\" Add portal user to admim group \u00b6 Create portal-de_admin.ldif \u00b6 dn: cn=de_admins,ou=Groups,dc=tugraz,dc=at changetype: modify add: memberUid memberUid: portal Apply ldap file \u00b6 # replace LDAP_PASSWORD ## add user to the de_admin group ldapmodify -x -D \"cn=Manager,dc=tugraz,dc=at\" -w \"LDAP_PASSWORD\" -f portal-de_admin.ldif iRODs \u00b6 For user portal we would need to create an irods rodsadmin user. # replace YOURPASSWORDHERE ## su - irods iadmin mkuser portal rodsadmin iadmin moduser portal password YOURPASSWORDHERE Database \u00b6 For Database configuration and setup vist portal-db docs . Images \u00b6 harbor.cyverse.org/hub/library/nginx:1.20-alpine Replaced with :nginx:1.20-alpine mbwali/portal:tug-stable Todo: tekton should build this via the URL. Deploy \u00b6 # create ns kubectl create ns user-portal # deploy kubectl apply -k portal/user-portal/base -n user-portal","title":"User Portal"},{"location":"deployments/userportal/#user-portal","text":"Prerequisites","title":"User Portal"},{"location":"deployments/userportal/#todo-add-source-location","text":"","title":"TODO: add source location."},{"location":"deployments/userportal/#ldap","text":"Setup required ldap user.","title":"LDAP"},{"location":"deployments/userportal/#create-portal-user","text":"","title":"create portal user"},{"location":"deployments/userportal/#create-portal-userldif","text":"dn: uid=portal,ou=People,dc=tugraz,dc=at objectClass: inetOrgPerson objectClass: posixAccount objectClass: shadowAccount uid: portal mail: portal@example.com sn: SeviceAccount givenName: PORTAL cn: portal title: Other o: N/A departmentNumber: N/A uidNumber: 40003 gidNumber: 10003 homeDirectory: /home/portal","title":"Create portal-user.ldif"},{"location":"deployments/userportal/#apply-ldap-file","text":"# replace LDAP_PASSWORD ldapadd -x -D \"cn=Manager,dc=tugraz,dc=at\" -w \"LDAP_PASSWORD\" -f portal-user.ldif # create a password for the user ## replace PORTAL_PASSWORD & LDAP_PASSWORD ldappasswd -x -D cn = Manager,dc = tugraz,dc = at -w \"LDAP_PASSWORD\" -s \"PORTAL_PASSWORD\" \"uid=portal,ou=People,dc=tugraz,dc=at\"","title":"Apply ldap file"},{"location":"deployments/userportal/#add-portal-user-to-admim-group","text":"","title":"Add portal user to admim group"},{"location":"deployments/userportal/#create-portal-de_adminldif","text":"dn: cn=de_admins,ou=Groups,dc=tugraz,dc=at changetype: modify add: memberUid memberUid: portal","title":"Create portal-de_admin.ldif"},{"location":"deployments/userportal/#apply-ldap-file_1","text":"# replace LDAP_PASSWORD ## add user to the de_admin group ldapmodify -x -D \"cn=Manager,dc=tugraz,dc=at\" -w \"LDAP_PASSWORD\" -f portal-de_admin.ldif","title":"Apply ldap file"},{"location":"deployments/userportal/#irods","text":"For user portal we would need to create an irods rodsadmin user. # replace YOURPASSWORDHERE ## su - irods iadmin mkuser portal rodsadmin iadmin moduser portal password YOURPASSWORDHERE","title":"iRODs"},{"location":"deployments/userportal/#database","text":"For Database configuration and setup vist portal-db docs .","title":"Database"},{"location":"deployments/userportal/#images","text":"harbor.cyverse.org/hub/library/nginx:1.20-alpine Replaced with :nginx:1.20-alpine mbwali/portal:tug-stable Todo: tekton should build this via the URL.","title":"Images"},{"location":"deployments/userportal/#deploy","text":"# create ns kubectl create ns user-portal # deploy kubectl apply -k portal/user-portal/base -n user-portal","title":"Deploy"},{"location":"deployments/vice/","text":"vice apps \u00b6 Prerequisites Create namespace \u00b6 kubectl create ns vice-apps Create a secret on namespace vice-apps vice-image-pull-secret \u00b6 This secret has the harbor.org which allows the pod to pull docker images. kubectl create secret generic vice-image-pull-secret \\ --from-file = .dockerconfigjson = /root/.docker/config.json \\ --type = kubernetes.io/dockerconfigjson -n vice-apps Create serviceAccounts \u00b6 ## edit the namespace if you are running in a diffrent env ## vi /k8s-resources/resources/serviceaccounts/app-exposer.yml kubectl apply -f /k8s-resources/resources/serviceaccounts/app-exposer.yml kubectl apply -f /k8s-resources/resources/serviceaccounts/vice-app-runner.yml Apply clusterrolebindings \u00b6 ## edit the namespace if you are running in a diffrent env ## vi /k8s-resources/resources/clusterrolebindings/app-exposer.yml kubectl apply -f /k8s-resources/resources/clusterrolebindings/app-exposer.yml Apply networkpolicies \u00b6 To apply networkpolicies we need to edit the file /k8s-resources/resources/networkpolicies/vice-apps.yml , and add all the worker nodes, and master nodes, to it if we are using a diffrent env rather than prod . e.g. - except: - ******** - ******** + except: - 10.0.10.0/24 # k8s master CIDR - ****************/32 # c1 - ****************/32 # w1 - ***************/32 # w2 - ***************/32 # w3 - **************/32 # w4 - *************/32 # w5 - ************/32 # vice-w1 Run policy kubectl apply -f /k8s-resources/resources/networkpolicies/vice-apps.yml Apply roles \u00b6 kubectl apply -f /k8s-resources/resources/roles/vice-apps.yml Create porklock-config secrert for irods \u00b6 Create irods-config.properties \u00b6 Create a file irods-config.properties and add the values. porklock.irods-home = porklock.irods-user = porklock.irods-pass = porklock.irods-host = porklock.irods-port = porklock.irods-zone = porklock.irods-resc = Create secret from file \u00b6 kubectl -n vice-apps create secret generic porklock-config --from-file = irods-config.properties Restart the services \u00b6 kubectl rollout restart apps app-exposer templeton-incremental templeton-periodic -n NAMESPACE Install/configure ingress-nginx \u00b6 For installing and configuring the ingress-nginx have a look at ingress-nginx","title":"VICE"},{"location":"deployments/vice/#vice-apps","text":"Prerequisites","title":"vice apps"},{"location":"deployments/vice/#create-namespace","text":"kubectl create ns vice-apps","title":"Create namespace"},{"location":"deployments/vice/#create-a-secret-on-namespace-vice-apps-vice-image-pull-secret","text":"This secret has the harbor.org which allows the pod to pull docker images. kubectl create secret generic vice-image-pull-secret \\ --from-file = .dockerconfigjson = /root/.docker/config.json \\ --type = kubernetes.io/dockerconfigjson -n vice-apps","title":"Create a secret on namespace vice-apps vice-image-pull-secret"},{"location":"deployments/vice/#create-serviceaccounts","text":"## edit the namespace if you are running in a diffrent env ## vi /k8s-resources/resources/serviceaccounts/app-exposer.yml kubectl apply -f /k8s-resources/resources/serviceaccounts/app-exposer.yml kubectl apply -f /k8s-resources/resources/serviceaccounts/vice-app-runner.yml","title":"Create serviceAccounts"},{"location":"deployments/vice/#apply-clusterrolebindings","text":"## edit the namespace if you are running in a diffrent env ## vi /k8s-resources/resources/clusterrolebindings/app-exposer.yml kubectl apply -f /k8s-resources/resources/clusterrolebindings/app-exposer.yml","title":"Apply clusterrolebindings"},{"location":"deployments/vice/#apply-networkpolicies","text":"To apply networkpolicies we need to edit the file /k8s-resources/resources/networkpolicies/vice-apps.yml , and add all the worker nodes, and master nodes, to it if we are using a diffrent env rather than prod . e.g. - except: - ******** - ******** + except: - 10.0.10.0/24 # k8s master CIDR - ****************/32 # c1 - ****************/32 # w1 - ***************/32 # w2 - ***************/32 # w3 - **************/32 # w4 - *************/32 # w5 - ************/32 # vice-w1 Run policy kubectl apply -f /k8s-resources/resources/networkpolicies/vice-apps.yml","title":"Apply networkpolicies"},{"location":"deployments/vice/#apply-roles","text":"kubectl apply -f /k8s-resources/resources/roles/vice-apps.yml","title":"Apply roles"},{"location":"deployments/vice/#create-porklock-config-secrert-for-irods","text":"","title":"Create porklock-config secrert for irods"},{"location":"deployments/vice/#create-irods-configproperties","text":"Create a file irods-config.properties and add the values. porklock.irods-home = porklock.irods-user = porklock.irods-pass = porklock.irods-host = porklock.irods-port = porklock.irods-zone = porklock.irods-resc =","title":"Create irods-config.properties"},{"location":"deployments/vice/#create-secret-from-file","text":"kubectl -n vice-apps create secret generic porklock-config --from-file = irods-config.properties","title":"Create secret from file"},{"location":"deployments/vice/#restart-the-services","text":"kubectl rollout restart apps app-exposer templeton-incremental templeton-periodic -n NAMESPACE","title":"Restart the services"},{"location":"deployments/vice/#installconfigure-ingress-nginx","text":"For installing and configuring the ingress-nginx have a look at ingress-nginx","title":"Install/configure ingress-nginx"},{"location":"development/","text":"coming soon \u00b6","title":"coming soon"},{"location":"development/#coming-soon","text":"","title":"coming soon"},{"location":"guides/de/","text":"Discovery Environment Admin Panel Administrative Panel \u00b6 Apps \u00b6 Add/Edit/Delete applications Requires a pre-existing Tool (Container Template) DOI Requests \u00b6 Add/Edit DataCite Metadata templates to curated datasets and push to DataCommons Reference Genomes \u00b6 Hosted reference genomes for featured genomic analyses Subscriptions \u00b6 View/Modify User account subscriptions for CPU hours and data storage capacity Tools \u00b6 Create/Edit/Deprecate K8s templates for public Docker images hosted on public registries VICE Access \u00b6 Grant access to interactive (K8s) run applications Limited access to verified accounts only.","title":"Discovery Environment"},{"location":"guides/de/#administrative-panel","text":"","title":"Administrative Panel"},{"location":"guides/de/#apps","text":"Add/Edit/Delete applications Requires a pre-existing Tool (Container Template)","title":"Apps"},{"location":"guides/de/#doi-requests","text":"Add/Edit DataCite Metadata templates to curated datasets and push to DataCommons","title":"DOI Requests"},{"location":"guides/de/#reference-genomes","text":"Hosted reference genomes for featured genomic analyses","title":"Reference Genomes"},{"location":"guides/de/#subscriptions","text":"View/Modify User account subscriptions for CPU hours and data storage capacity","title":"Subscriptions"},{"location":"guides/de/#tools","text":"Create/Edit/Deprecate K8s templates for public Docker images hosted on public registries","title":"Tools"},{"location":"guides/de/#vice-access","text":"Grant access to interactive (K8s) run applications Limited access to verified accounts only.","title":"VICE Access"},{"location":"guides/devops/","text":"Ansible is an open source, agentless automation tool. The DE development team uses Ansible to provision/update our servers and deploy the DE. However, for this repository, we only expose the Ansible scripts that we use for deploying the DE. If you intend to use our ansible scripts, we highly suggest that you read the Ansible documentation . Installations \u00b6 Ansible Docker Kubernetes (K8s) - kubectl for managing K8s clusters Terraform VS Code Kubernetes Extension - optional (recommended) Setup \u00b6 Ansible Setup Docker-based Ansible Setup Design \u00b6 We have strived to follow Ansible's best practices . However, we have slightly diverged on the topics of directory layout and role-separated top level playbooks . Directory Layout \u00b6 The ansible best practices for directory layout suggests using a group_vars folder, but you may have noticed that the repo doesn't contain a ansible/group_vars folder. Our default group_vars folder resides in the inventories folder ansible/inventories/group_vars . Also, this folder contains a single file, all , which contains all of the variables used by the provided roles and playbooks with default values. This is done with the intent that developers will create their own group_vars/all file, which will override any of the defaults set in the inventories/group_vars/all file. You only need to include the variables you wish to override. Role-Separated Playbooks \u00b6 We maintain role-separated playbooks, but they are kept in the ansible/playbooks/ folder. The remaining playbooks in the root of the ansible/ folder are composite (utilize more than one role) or one off playbooks (do not use any roles). If you wish to use these playbooks, we have created the single-role.yaml playbook. The documentation on its use is contained within the playbook. Inventories \u00b6 We have provided an example inventory file; example.cfg . Our roles and playbooks are written against the host groups in this inventory. Files with a .cfg extension are ignored by git in the inventories folder. This is done to prevent us from accidentally exposing our inventories to the public. The host groups within the example inventory reference the host machines for the DE's underlying architecture, as well as host groups for the application itself. Each micro-service has a corresponding host group in the inventory. Refer to the example.cfg file for more info. Playbooks \u00b6 Updating Databases","title":"DevOps"},{"location":"guides/devops/#installations","text":"Ansible Docker Kubernetes (K8s) - kubectl for managing K8s clusters Terraform VS Code Kubernetes Extension - optional (recommended)","title":"Installations"},{"location":"guides/devops/#setup","text":"Ansible Setup Docker-based Ansible Setup","title":"Setup"},{"location":"guides/devops/#design","text":"We have strived to follow Ansible's best practices . However, we have slightly diverged on the topics of directory layout and role-separated top level playbooks .","title":"Design"},{"location":"guides/devops/#directory-layout","text":"The ansible best practices for directory layout suggests using a group_vars folder, but you may have noticed that the repo doesn't contain a ansible/group_vars folder. Our default group_vars folder resides in the inventories folder ansible/inventories/group_vars . Also, this folder contains a single file, all , which contains all of the variables used by the provided roles and playbooks with default values. This is done with the intent that developers will create their own group_vars/all file, which will override any of the defaults set in the inventories/group_vars/all file. You only need to include the variables you wish to override.","title":"Directory Layout"},{"location":"guides/devops/#role-separated-playbooks","text":"We maintain role-separated playbooks, but they are kept in the ansible/playbooks/ folder. The remaining playbooks in the root of the ansible/ folder are composite (utilize more than one role) or one off playbooks (do not use any roles). If you wish to use these playbooks, we have created the single-role.yaml playbook. The documentation on its use is contained within the playbook.","title":"Role-Separated Playbooks"},{"location":"guides/devops/#inventories","text":"We have provided an example inventory file; example.cfg . Our roles and playbooks are written against the host groups in this inventory. Files with a .cfg extension are ignored by git in the inventories folder. This is done to prevent us from accidentally exposing our inventories to the public. The host groups within the example inventory reference the host machines for the DE's underlying architecture, as well as host groups for the application itself. Each micro-service has a corresponding host group in the inventory. Refer to the example.cfg file for more info.","title":"Inventories"},{"location":"guides/devops/#playbooks","text":"Updating Databases","title":"Playbooks"},{"location":"guides/ds/","text":"Container Storage Interface (CSI) Driver \u00b6 GoCommands \u00b6 3 rd Party Applications \u00b6 Cyberduck \u00b6 FileZilla \u00b6 SFTP \u00b6 WebDAV \u00b6 File Explorers \u00b6 Sharing Folders \u00b6 Community Released Folder Creation \u00b6 Curated Folders \u00b6 DataCite DOI","title":"Data Store"},{"location":"guides/ds/#container-storage-interface-csi-driver","text":"","title":" Container Storage Interface (CSI) Driver"},{"location":"guides/ds/#gocommands","text":"","title":" GoCommands"},{"location":"guides/ds/#3rd-party-applications","text":"","title":"3rd Party Applications"},{"location":"guides/ds/#cyberduck","text":"","title":" Cyberduck"},{"location":"guides/ds/#filezilla","text":"","title":" FileZilla"},{"location":"guides/ds/#sftp","text":"","title":" SFTP"},{"location":"guides/ds/#webdav","text":"","title":" WebDAV"},{"location":"guides/ds/#file-explorers","text":"","title":" File Explorers"},{"location":"guides/ds/#sharing-folders","text":"","title":" Sharing Folders"},{"location":"guides/ds/#community-released-folder-creation","text":"","title":" Community Released Folder Creation"},{"location":"guides/ds/#curated-folders","text":"DataCite DOI","title":" Curated Folders"},{"location":"guides/faq/","text":"","title":"Frequently Asked Questions"},{"location":"guides/user_portal/","text":"CyVerse User Portal Admin Panel Administrative Panel \u00b6 Users \u00b6 Search across all CyVerse users and view details about individual users. Restricted Usernames \u00b6 Show and edit restricted usernames. Access Requests \u00b6 Search across all access requests and view/deny/approve individual requests. Services \u00b6 View and modify services. Workshops \u00b6 View, create, and modify workshops. Form Submissions \u00b6 Search across all form submissions and view individual submissions. Forms \u00b6 View and edit forms.","title":"User Portal"},{"location":"guides/user_portal/#administrative-panel","text":"","title":"Administrative Panel"},{"location":"guides/user_portal/#users","text":"Search across all CyVerse users and view details about individual users.","title":"Users"},{"location":"guides/user_portal/#restricted-usernames","text":"Show and edit restricted usernames.","title":"Restricted Usernames"},{"location":"guides/user_portal/#access-requests","text":"Search across all access requests and view/deny/approve individual requests.","title":"Access Requests"},{"location":"guides/user_portal/#services","text":"View and modify services.","title":"Services"},{"location":"guides/user_portal/#workshops","text":"View, create, and modify workshops.","title":"Workshops"},{"location":"guides/user_portal/#form-submissions","text":"Search across all form submissions and view individual submissions.","title":"Form Submissions"},{"location":"guides/user_portal/#forms","text":"View and edit forms.","title":"Forms"},{"location":"guides/setup/ansible/","text":"Installing Ansible \u00b6 See http://docs.ansible.com/intro_installation.html for the long instructions. There's multiple ways to install ansible. I used Python's 'pip' command to install it on OS X, but you can also use homebrew or a git checkout. pip sudo pip install ansible homebrew brew install ansible checkout git clone git://github.com/ansible/ansible.git cd ./ansible source ./hacking/env-setup If you use the checkout method, add a line to your ~/.profile (on OS X) or your ~/.bashrc file (all other sane OSes). You only need ansible on your local machine. It does not need to be installed on the servers. Installing Third Party Ansible Roles \u00b6 See http://docs.ansible.com/galaxy.html for more information about managing third party roles. In brief, you'll need to sign up for galaxy at https://galaxy.ansible.com and execute the following command in the de-ansible repo. ansible-galaxy install --force -r requirements.yaml Learning About Ansible \u00b6 If you just want to use ansible for DE related tasks, move on to the other sections. If you want to make your own playbooks for existing inventories of servers, read: http://docs.ansible.com/playbooks_intro.html . If you want to create a new inventory or modify an existing one, read: http://docs.ansible.com/intro_inventory.html . Each playbook uses one or more modules. Modules encapsulate operations that run on servers and make them idempotent. There are a lot of existing modules, which you can read about at http://docs.ansible.com/modules_by_category.html . Ansible Config Settings \u00b6 The design of the DE's ansible variables makes use of YAML \"hashes\" . If you ever intend to override a single value in a hash, you will need to set ansible's hash behaviour to \"merge\" . The easiest way to set these are with environment variables. For example, add these lines to your ~/.bash_profile or ~/.bashrc file: export ANSIBLE_HASH_BEHAVIOUR=\"merge\" Or these settings can be updated in a local ansible.cfg . Preparing Servers \u00b6 simplejson \u00b6 Each server needs to have the simplejson Python library available for the default Python installation. CentOS \u215a sudo yum install python-simplejson This should already be done for the development servers. curl \u00b6 Each server needs to have curl on it so ansible can send messages to chat. curl is provided by default on CentOS distributions, It needs to be manually installed on Ubuntu systems. Ubuntu sudo apt-get install curl httplib2 \u00b6 Each server needs to have httplib2 Python library available for the default Python installation. It needs to be manually installed on Ubuntu and CentOS 6 systems. TODO: Is manual installation still necessary? The private-registry-image-builder role installs it with pip (on CentOS7 systems at least). \u00b6 Ubuntu sudo apt-get install python-httplib2 CentOS 6 sudo yum install python-httplib2 Setting Up Your Accounts \u00b6 You will need passwordless ssh access to each of the servers listed in the inventory for the environment you're working in. Generate RSA private and public keys \u00b6 If you do not already have a ~/.ssh/id_rsa.pub file generated, then run this command to create it: ssh-keygen -t rsa Install ssh-copy-id \u00b6 See https://github.com/beautifulcode/ssh-copy-id-for-OSX for instructions on setting up ssh-copy-id on OS X. Copying ssh keys \u00b6 ssh-copy-id -i <path-to-public-key> <server> For example: ssh-copy-id -i ~/.ssh/id_rsa.pub user@example.iplantcollaborative.org Do that for each of the servers in the inventory. I'd recommend setting up an SSH config that has entries for each of the servers (include the fully-qualified domain name) first; it will allow you to skip a lot of typing. Next, you need to generate entries in ~/.ssh/known_hosts for each of the servers. The easiest way to do this is to ssh into each of the servers once after you've set up passwordless ssh access. You need to SSH into the fully-qualified domain name of the host. For example: ssh example.iplantcollaborative.org You will also need sudo access on those servers for some operations. You should already have this if you're in the dev group on the servers.","title":"Ansible"},{"location":"guides/setup/ansible/#installing-ansible","text":"See http://docs.ansible.com/intro_installation.html for the long instructions. There's multiple ways to install ansible. I used Python's 'pip' command to install it on OS X, but you can also use homebrew or a git checkout. pip sudo pip install ansible homebrew brew install ansible checkout git clone git://github.com/ansible/ansible.git cd ./ansible source ./hacking/env-setup If you use the checkout method, add a line to your ~/.profile (on OS X) or your ~/.bashrc file (all other sane OSes). You only need ansible on your local machine. It does not need to be installed on the servers.","title":"Installing Ansible"},{"location":"guides/setup/ansible/#installing-third-party-ansible-roles","text":"See http://docs.ansible.com/galaxy.html for more information about managing third party roles. In brief, you'll need to sign up for galaxy at https://galaxy.ansible.com and execute the following command in the de-ansible repo. ansible-galaxy install --force -r requirements.yaml","title":"Installing Third Party Ansible Roles"},{"location":"guides/setup/ansible/#learning-about-ansible","text":"If you just want to use ansible for DE related tasks, move on to the other sections. If you want to make your own playbooks for existing inventories of servers, read: http://docs.ansible.com/playbooks_intro.html . If you want to create a new inventory or modify an existing one, read: http://docs.ansible.com/intro_inventory.html . Each playbook uses one or more modules. Modules encapsulate operations that run on servers and make them idempotent. There are a lot of existing modules, which you can read about at http://docs.ansible.com/modules_by_category.html .","title":"Learning About Ansible"},{"location":"guides/setup/ansible/#ansible-config-settings","text":"The design of the DE's ansible variables makes use of YAML \"hashes\" . If you ever intend to override a single value in a hash, you will need to set ansible's hash behaviour to \"merge\" . The easiest way to set these are with environment variables. For example, add these lines to your ~/.bash_profile or ~/.bashrc file: export ANSIBLE_HASH_BEHAVIOUR=\"merge\" Or these settings can be updated in a local ansible.cfg .","title":"Ansible Config Settings"},{"location":"guides/setup/ansible/#preparing-servers","text":"","title":"Preparing Servers"},{"location":"guides/setup/ansible/#simplejson","text":"Each server needs to have the simplejson Python library available for the default Python installation. CentOS \u215a sudo yum install python-simplejson This should already be done for the development servers.","title":"simplejson"},{"location":"guides/setup/ansible/#curl","text":"Each server needs to have curl on it so ansible can send messages to chat. curl is provided by default on CentOS distributions, It needs to be manually installed on Ubuntu systems. Ubuntu sudo apt-get install curl","title":"curl"},{"location":"guides/setup/ansible/#httplib2","text":"Each server needs to have httplib2 Python library available for the default Python installation. It needs to be manually installed on Ubuntu and CentOS 6 systems.","title":"httplib2"},{"location":"guides/setup/ansible/#todo-is-manual-installation-still-necessary-the-private-registry-image-builder-role-installs-it-with-pip-on-centos7-systems-at-least","text":"Ubuntu sudo apt-get install python-httplib2 CentOS 6 sudo yum install python-httplib2","title":"TODO: Is manual installation still necessary? The private-registry-image-builder role installs it with pip (on CentOS7 systems at least)."},{"location":"guides/setup/ansible/#setting-up-your-accounts","text":"You will need passwordless ssh access to each of the servers listed in the inventory for the environment you're working in.","title":"Setting Up Your Accounts"},{"location":"guides/setup/ansible/#generate-rsa-private-and-public-keys","text":"If you do not already have a ~/.ssh/id_rsa.pub file generated, then run this command to create it: ssh-keygen -t rsa","title":"Generate RSA private and public keys"},{"location":"guides/setup/ansible/#install-ssh-copy-id","text":"See https://github.com/beautifulcode/ssh-copy-id-for-OSX for instructions on setting up ssh-copy-id on OS X.","title":"Install ssh-copy-id"},{"location":"guides/setup/ansible/#copying-ssh-keys","text":"ssh-copy-id -i <path-to-public-key> <server> For example: ssh-copy-id -i ~/.ssh/id_rsa.pub user@example.iplantcollaborative.org Do that for each of the servers in the inventory. I'd recommend setting up an SSH config that has entries for each of the servers (include the fully-qualified domain name) first; it will allow you to skip a lot of typing. Next, you need to generate entries in ~/.ssh/known_hosts for each of the servers. The easiest way to do this is to ssh into each of the servers once after you've set up passwordless ssh access. You need to SSH into the fully-qualified domain name of the host. For example: ssh example.iplantcollaborative.org You will also need sudo access on those servers for some operations. You should already have this if you're in the dev group on the servers.","title":"Copying ssh keys"},{"location":"guides/setup/database/","text":"Updating the DE database \u00b6 ansible-playbook -i inventories/... -K [-u <user>] db-migrations.yaml The -i , -K , and -u options are the same as in the other Ansible commands.","title":"Database"},{"location":"guides/setup/database/#updating-the-de-database","text":"ansible-playbook -i inventories/... -K [-u <user>] db-migrations.yaml The -i , -K , and -u options are the same as in the other Ansible commands.","title":"Updating the DE database"},{"location":"guides/setup/docker/","text":"OSX \u00b6 To build on OS X, see Get Started with Docker for Mac OS X for instructions on installing Docker Toolbox Running ansible inside a Docker container \u00b6 The Dockerfile at the top-level of the de-ansible checkout can be used to create a personal container that is able to run the de-ansible playbooks. DO NOT PUSH YOUR PERSONAL de-ansible CONTAINER! To build the container, first run the create-ssh-configs.sh script at the top-level of the de-ansible checkout: de-ansible> ./create-ssh-configs.sh Then run docker build: de-ansible> docker build . Make a note of the image ID and use it in a docker run command: de-ansible> docker run --rm -it -v $(pwd):/de-ansible -w /de-ansible <image ID> /bin/bash You should be able to run the ansible commands inside the container you created.","title":"Docker"},{"location":"guides/setup/docker/#osx","text":"To build on OS X, see Get Started with Docker for Mac OS X for instructions on installing Docker Toolbox","title":"OSX"},{"location":"guides/setup/docker/#running-ansible-inside-a-docker-container","text":"The Dockerfile at the top-level of the de-ansible checkout can be used to create a personal container that is able to run the de-ansible playbooks. DO NOT PUSH YOUR PERSONAL de-ansible CONTAINER! To build the container, first run the create-ssh-configs.sh script at the top-level of the de-ansible checkout: de-ansible> ./create-ssh-configs.sh Then run docker build: de-ansible> docker build . Make a note of the image ID and use it in a docker run command: de-ansible> docker run --rm -it -v $(pwd):/de-ansible -w /de-ansible <image ID> /bin/bash You should be able to run the ansible commands inside the container you created.","title":"Running ansible inside a Docker container"},{"location":"services/api_overview/","text":"Terrain \u00b6 Terrain provides the primary point of communication between the Discovery Environment (DE) UI and backend services. It's charged with two primary tasks: (1) to handle authentication and authorization for endpoints that require it, (2) to orchestrate calls to other lower-level services. The primary and most current source for Terrain API endpoint documentation are the Swagger docs hosted by the service itself: https://de.cyverse.org/terrain/docs Not all endpoints have been documented in those Swagger docs yet, and those endpoints will be listed under the default Swagger category. The documentation for those endpoints may still be found below. Endpoints Endpoints Index Errors Authentication \u00b6 Authentication to the DE services is currently handled by Keycloak , allowing clients to obtain OAuth or OIDC tokens for accessing secured API endpoints. See the /terrain/token/keycloak endpoint docs for details on obtaining tokens.","title":"Overview"},{"location":"services/api_overview/#terrain","text":"Terrain provides the primary point of communication between the Discovery Environment (DE) UI and backend services. It's charged with two primary tasks: (1) to handle authentication and authorization for endpoints that require it, (2) to orchestrate calls to other lower-level services. The primary and most current source for Terrain API endpoint documentation are the Swagger docs hosted by the service itself: https://de.cyverse.org/terrain/docs Not all endpoints have been documented in those Swagger docs yet, and those endpoints will be listed under the default Swagger category. The documentation for those endpoints may still be found below. Endpoints Endpoints Index Errors","title":" Terrain"},{"location":"services/api_overview/#authentication","text":"Authentication to the DE services is currently handled by Keycloak , allowing clients to obtain OAuth or OIDC tokens for accessing secured API endpoints. See the /terrain/token/keycloak endpoint docs for details on obtaining tokens.","title":"Authentication"},{"location":"services/bisque/","text":"The Bio-Image Semantic Query User Environment (BisQue) was originally developed at UC Santa Barbara Center for Bio-Image Informatics . Later, it became ViQi AI CyVerse maintains a fork of the UCSB version of BisQue Deployment of CyVerse BisQue \u00b6 UCSB BisQue iRODS","title":"Bisque"},{"location":"services/bisque/#deployment-of-cyverse-bisque","text":"UCSB BisQue iRODS","title":"Deployment of CyVerse BisQue"},{"location":"services/cloud/","text":"CyVerse has a long history working with OpenStack based clouds, including its deprecated \" Atmosphere \" and \"Jetstream\" User Interfaces. CyVerse is partnered with Jetstream2 to manage cloud-native applications such as Terraform, Argo Workflows, and Kubernetes. Access to Jetstream2 login is managed through ACCESS-CI . CyVerse CACAO documentation can also be found on Jetstream2 Project docs. Atmosphere (deprecated) \u00b6 Atmosphere Ansible Atmosphere Docker Atmosphere Guides Atmosphere Apache Guacamole Themes Cloud Automation & Continuous Analysis Orchestration (CACAO) \u00b6 CACAO is Infrastructure as Code (IaC) for multi-cloud deployment","title":"Cloud Services"},{"location":"services/cloud/#atmosphere-deprecated","text":"Atmosphere Ansible Atmosphere Docker Atmosphere Guides Atmosphere Apache Guacamole Themes","title":" Atmosphere (deprecated)"},{"location":"services/cloud/#cloud-automation-continuous-analysis-orchestration-cacao","text":"CACAO is Infrastructure as Code (IaC) for multi-cloud deployment","title":"Cloud Automation &amp; Continuous Analysis Orchestration (CACAO)"},{"location":"services/dc/","text":"DataCommons [Services]","title":"Data Publishing"},{"location":"services/de/","text":"Discovery Environment User Guide Deployment of the Discovery Environment (DE) is provided in the Deployments section Figure DE executable jobs use HTCondor Figure DE interactive jobs use K8s Infrastructure \u00b6 The following sections describe the key components of the infrastructure upon which the DE operates. Data Store \u00b6 The DE provides access and management of data via the CyVerse Data Store , which is built on top of iRODS . Compute Platform(s) \u00b6 The DE integrates the Data Store with HTCondor and the Agave Platform to provide a large set of tools for performing resource intense analyses. PostgreSQL \u00b6 Nearly all applications use a database. The DE is backed by a PostgreSQL database. The schema can be found in the de-database repository. RabbitMQ \u00b6 RabbitMQ is used throughout our services, but primarily to integrate our services with iRODS. Elasticsearch \u00b6 The DE uses Elasticsearch to provide search and other capabilities. Docker \u00b6 Docker is used throughout the DE architecture. Most importantly, all of the tools that run in the DE's HTCondor cluster run within docker containers, allowing us to integrate new tools without affecting existing tools. Additionally, the components of the DE application are packaged as Docker containers. All of these images can be accessed through our organization page on Docker Hub . Architecture \u00b6 The DE is composed of backend services and a user interface (UI). Backend Services \u00b6 The DE backend is built as a micro-services architecture. Each of these services are contained in the services/ folder. The functionality of the micro-service architecture is aggregated in the Terrain service, and exposed as a RESTful api . More information about the backend micro-service architecture and implementation may be found here . UI \u00b6 All of the UI services are provided by the DE api. The application itself is built with GXT , a UI component library built on top of GWT . Discovery Environment \u00b6 Here you will find all the GitHub repositories for all services: analyses : Provides a HTTP API for interacting with analyses in the Discovery Environment. app-exposer : This is a service that runs inside of a Kubernetes cluster namespace and implements CRUD operations for exposing interactive apps as a Service and Endpoint. apply-labels : A small service in the Discovery Environment backend that periodically hits the app-exposer service to trigger the application of labels on VICE-related K8s resources apps : apps is a platform for hosting App Services for the Discovery Environment web application. async-tasks : This service tracks and manages asynchronous tasks throughout the DE backend services. bulk-typer : Like info-typer, but a bunch at once. hopefully. check-resource-access : Looks up the permissions that a subject has for a resource. By default, the subject type is 'user' and the resource type is 'analysis'. Only performs look ups against the permissions service. clockwork : Scheduled jobs for the CyVerse Discovery Environment. dashboard-aggregator : Gathers data to populate the dashboard in Sonora. data-info : data-info is a RESTful frontend for getting information about and manipulating information in an iRODS data store. data-usage-api : A service that provides an API around data usage tracking, and updates data usage numbers on request or periodically. de-mailer : A go module that send email notifications to users. This module will support HTML and rich text emails.</del> de-nginx de-stats : Service for obtaining CyVerse Discovery Environment stats and metrics. de-webhooks : A service that listens to AMQP queues for DE notifications, check if the user has webhooks defined for that notification type and then post the notification to webhook if one is defined. dewey : An AMQP message based iRODS indexer for elasticsearch. email-requests : A simple service to wait for email requests to arrive over AMQP and forward them to cyverse-email. event-recorder : This service listens to an AMQP topic for events, and records events that may be of interest to users in the notifications database. get-analysis-id : TODO: FIND the repo. grouper : - grouper-loader & grouper-ws TODO: FIND the repo. - From kubectl apply info-typer : An AMQP message based info type detector infosquito2 : TODO FIND description. iplant-groups : A RESTful facade in front of Grouper . jex-adapter : TODO FIND description. job-status-recorder : TODO FIND description. job-status-listener : Listens over HTTP for job status updates, then publishes them to AMQP. kifshare : A simple web page that allows users to download publicly available files without a CyVerse account. local-exim (exim-sender) : TODO: FIND the repo. - From kubectl metadata : The REST API for the Discovery Environment Metadata services. monkey : This is a service that synchronizes the tag documents in the data search index with the metadata database notifications : This service provides the RESTful API for the revised notification system. permissions : This service manages permissions for the Discovery environment. requests : Service for managing administrative requests in the CyVerse Discovery Environment. terrain : Terrain provides the primary REST API used by the Discovery Environment. Its role is to validate user authentication and to coordinate calls to other web services. For more information, please see the Discovery Environment API Documentation . resource-usage-api : is a microservice developed as part of the CyVerse Discovery Environment that provides access to resource usage values (CPU hours, memory, etc.) consumed by users over a customizable time period. saved-searches : A service for the CyVerse Discovery Environment that provides CRUD access to a user's saved searches. search : This is a service which serves as a search facade for the DE and others to use. It uses the querydsl library under the covers to translate requests and provide documentation, then passes off queries to configured elasticsearch servers. sonora : UI for the Discovery Environment templeton : includes templeton-incremental & templeton-periodic TODO: add description. timelord : timelord periodically queries the DE database for running applications and kills any of them that have gone over their time limit. unleash : TODO: FIND the repo. - From kubectl apply user-info : A service for getting user-related information like sessions and preferences. vice-default-backend : Provides a default backend handler for the Kubernetes Ingress that handles routing for VICE apps. This backend decides whether to redirect requests to the loading page service, the landing page service, or to a 404 page depending on whether the URL is valid or not. qms-adapter : Forwards usage information gathered within the Discovery Environment to the Quota Management System [QMS]( https://github.com/cyverse/QMS qms : QMS is the CyVerse Quota Management System. Its purpose is to keep track of resource usage limits and totals for CyVerse users. irods-csi-driver : iRODS Container Storage Interface (CSI) Driver implements the CSI Specification to provide container orchestration engines (like Kubernetes) iRODS access. dataone-indexer : Event indexer for the DataONE member node service.</de Non-Core Services \u00b6 redis-ha redis-haproxy elasticsearch (opensearch) keycloak openebs","title":"Discovery Environment"},{"location":"services/de/#infrastructure","text":"The following sections describe the key components of the infrastructure upon which the DE operates.","title":"Infrastructure"},{"location":"services/de/#data-store","text":"The DE provides access and management of data via the CyVerse Data Store , which is built on top of iRODS .","title":"Data Store"},{"location":"services/de/#compute-platforms","text":"The DE integrates the Data Store with HTCondor and the Agave Platform to provide a large set of tools for performing resource intense analyses.","title":"Compute Platform(s)"},{"location":"services/de/#postgresql","text":"Nearly all applications use a database. The DE is backed by a PostgreSQL database. The schema can be found in the de-database repository.","title":"PostgreSQL"},{"location":"services/de/#rabbitmq","text":"RabbitMQ is used throughout our services, but primarily to integrate our services with iRODS.","title":"RabbitMQ"},{"location":"services/de/#elasticsearch","text":"The DE uses Elasticsearch to provide search and other capabilities.","title":"Elasticsearch"},{"location":"services/de/#docker","text":"Docker is used throughout the DE architecture. Most importantly, all of the tools that run in the DE's HTCondor cluster run within docker containers, allowing us to integrate new tools without affecting existing tools. Additionally, the components of the DE application are packaged as Docker containers. All of these images can be accessed through our organization page on Docker Hub .","title":"Docker"},{"location":"services/de/#architecture","text":"The DE is composed of backend services and a user interface (UI).","title":"Architecture"},{"location":"services/de/#backend-services","text":"The DE backend is built as a micro-services architecture. Each of these services are contained in the services/ folder. The functionality of the micro-service architecture is aggregated in the Terrain service, and exposed as a RESTful api . More information about the backend micro-service architecture and implementation may be found here .","title":"Backend Services"},{"location":"services/de/#ui","text":"All of the UI services are provided by the DE api. The application itself is built with GXT , a UI component library built on top of GWT .","title":"UI"},{"location":"services/de/#discovery-environment","text":"Here you will find all the GitHub repositories for all services: analyses : Provides a HTTP API for interacting with analyses in the Discovery Environment. app-exposer : This is a service that runs inside of a Kubernetes cluster namespace and implements CRUD operations for exposing interactive apps as a Service and Endpoint. apply-labels : A small service in the Discovery Environment backend that periodically hits the app-exposer service to trigger the application of labels on VICE-related K8s resources apps : apps is a platform for hosting App Services for the Discovery Environment web application. async-tasks : This service tracks and manages asynchronous tasks throughout the DE backend services. bulk-typer : Like info-typer, but a bunch at once. hopefully. check-resource-access : Looks up the permissions that a subject has for a resource. By default, the subject type is 'user' and the resource type is 'analysis'. Only performs look ups against the permissions service. clockwork : Scheduled jobs for the CyVerse Discovery Environment. dashboard-aggregator : Gathers data to populate the dashboard in Sonora. data-info : data-info is a RESTful frontend for getting information about and manipulating information in an iRODS data store. data-usage-api : A service that provides an API around data usage tracking, and updates data usage numbers on request or periodically. de-mailer : A go module that send email notifications to users. This module will support HTML and rich text emails.</del> de-nginx de-stats : Service for obtaining CyVerse Discovery Environment stats and metrics. de-webhooks : A service that listens to AMQP queues for DE notifications, check if the user has webhooks defined for that notification type and then post the notification to webhook if one is defined. dewey : An AMQP message based iRODS indexer for elasticsearch. email-requests : A simple service to wait for email requests to arrive over AMQP and forward them to cyverse-email. event-recorder : This service listens to an AMQP topic for events, and records events that may be of interest to users in the notifications database. get-analysis-id : TODO: FIND the repo. grouper : - grouper-loader & grouper-ws TODO: FIND the repo. - From kubectl apply info-typer : An AMQP message based info type detector infosquito2 : TODO FIND description. iplant-groups : A RESTful facade in front of Grouper . jex-adapter : TODO FIND description. job-status-recorder : TODO FIND description. job-status-listener : Listens over HTTP for job status updates, then publishes them to AMQP. kifshare : A simple web page that allows users to download publicly available files without a CyVerse account. local-exim (exim-sender) : TODO: FIND the repo. - From kubectl metadata : The REST API for the Discovery Environment Metadata services. monkey : This is a service that synchronizes the tag documents in the data search index with the metadata database notifications : This service provides the RESTful API for the revised notification system. permissions : This service manages permissions for the Discovery environment. requests : Service for managing administrative requests in the CyVerse Discovery Environment. terrain : Terrain provides the primary REST API used by the Discovery Environment. Its role is to validate user authentication and to coordinate calls to other web services. For more information, please see the Discovery Environment API Documentation . resource-usage-api : is a microservice developed as part of the CyVerse Discovery Environment that provides access to resource usage values (CPU hours, memory, etc.) consumed by users over a customizable time period. saved-searches : A service for the CyVerse Discovery Environment that provides CRUD access to a user's saved searches. search : This is a service which serves as a search facade for the DE and others to use. It uses the querydsl library under the covers to translate requests and provide documentation, then passes off queries to configured elasticsearch servers. sonora : UI for the Discovery Environment templeton : includes templeton-incremental & templeton-periodic TODO: add description. timelord : timelord periodically queries the DE database for running applications and kills any of them that have gone over their time limit. unleash : TODO: FIND the repo. - From kubectl apply user-info : A service for getting user-related information like sessions and preferences. vice-default-backend : Provides a default backend handler for the Kubernetes Ingress that handles routing for VICE apps. This backend decides whether to redirect requests to the loading page service, the landing page service, or to a 404 page depending on whether the URL is valid or not. qms-adapter : Forwards usage information gathered within the Discovery Environment to the Quota Management System [QMS]( https://github.com/cyverse/QMS qms : QMS is the CyVerse Quota Management System. Its purpose is to keep track of resource usage limits and totals for CyVerse users. irods-csi-driver : iRODS Container Storage Interface (CSI) Driver implements the CSI Specification to provide container orchestration engines (like Kubernetes) iRODS access. dataone-indexer : Event indexer for the DataONE member node service.</de","title":"Discovery Environment"},{"location":"services/de/#non-core-services","text":"redis-ha redis-haproxy elasticsearch (opensearch) keycloak openebs","title":"Non-Core Services"},{"location":"services/dnasubway/","text":"DNA Subway","title":"DNA Subway"},{"location":"services/ds/","text":"Data Storage in CyVerse is managed both internally and through federation. What is Federation? Data federation allows institutions to manage their own hardware, or buy space on commercial cloud, and integrate it with our Data Store. Data that are federated do not appear any differently than data that are hosted in the primary CyVerse Data Store. This is not federation in the iRODS sense, where the the federated storage is managed in a separate zone. The storage managed by the institution is in the same zone, iplant , as the CyVerse managed storage. Figure : Access to the Data Store can be accomplished over HTTPS ( https:// ), SFTP ( sftp:// ) or iRODS Protocol iRODS \u00b6 CyVerse uses iRODS (integrated Rule Oriented Data System) to manage its users data and uses PostgreSQL to manage the catalog (ICAT) database for iRODS. iRODS Documentation PostgreSQL Documentation SFTP \u00b6 CyVerse uses SFTPGo along with a custom iRODS storage backend to provide SFTP access to data in the Data Store. SFTPGo Documentation SFTGO with iRODS Storage Backend Documentation WebDAV \u00b6 CyVerse uses apache along with the davrods module to manage the WebDAV service for accessing the Data Store over the HTTPS and WebDAVS protocols. Varnish HTTP Cache is used internally for caching requested files. Apache Documentation Davrods Documentation Varnish HTTP Cache Documentation HAProxy \u00b6 CyVerse uses HAProxy to provide a common point of entry, data.cyverse.org, for clients to access all of the Data Store services. HAProxy Documentation AMQP \u00b6 CyVerse uses RabbitMQ to provide an AMQP-based message bus for notifying other CyVerse services like DataWatch about data related events. RabbitMQ Documentation","title":"Data Storage"},{"location":"services/ds/#irods","text":"CyVerse uses iRODS (integrated Rule Oriented Data System) to manage its users data and uses PostgreSQL to manage the catalog (ICAT) database for iRODS. iRODS Documentation PostgreSQL Documentation","title":" iRODS"},{"location":"services/ds/#sftp","text":"CyVerse uses SFTPGo along with a custom iRODS storage backend to provide SFTP access to data in the Data Store. SFTPGo Documentation SFTGO with iRODS Storage Backend Documentation","title":" SFTP"},{"location":"services/ds/#webdav","text":"CyVerse uses apache along with the davrods module to manage the WebDAV service for accessing the Data Store over the HTTPS and WebDAVS protocols. Varnish HTTP Cache is used internally for caching requested files. Apache Documentation Davrods Documentation Varnish HTTP Cache Documentation","title":" WebDAV"},{"location":"services/ds/#haproxy","text":"CyVerse uses HAProxy to provide a common point of entry, data.cyverse.org, for clients to access all of the Data Store services. HAProxy Documentation","title":" HAProxy"},{"location":"services/ds/#amqp","text":"CyVerse uses RabbitMQ to provide an AMQP-based message bus for notifying other CyVerse services like DataWatch about data related events. RabbitMQ Documentation","title":" AMQP"},{"location":"services/faq/","text":"Frequently Asked Questions","title":"Faq"},{"location":"services/getting_started/","text":"Prerequisites Access to the internet and a web-enabled browser Access to bare metal hardware, OpenStack cloud, or commercial provider ( AWS, Google Cloud, Microsoft Azure) GitHub or GitLab with private repositories for managing sensitive credentials and configurations Advanced understanding of Linux file permissions and Operating Systems Advanced understanding of Virtual Machines and access via ssh Patience & Perserverance Installation \u00b6 The DevOps Guide section provies a list of required software for managing a CyVerse deployment Authentication \u00b6 CyVerse authentication relies upon LDAP , OAUTH 2.0 protocol , and CILogon Security Configurations \u00b6 Experience Living in a Science DMZ is beneficial for deploying CyVerse on University hardware APIs \u00b6 Terrain API is the backbone service which manages the Discovery Environment data science workbench. Details about Terrain are presented in the API endpoints section Products & Services \u00b6 Authentication - managed user authentication uses Keycloak, CI Logon, and OAUTH 2.0 BisQue - large image analyses in the browser DataCommons - Community data sharing, DataCite data publishing with DOI Data Store - data storage, hosting, & sharing DNA Subway - educational software for high school and undergradautes in bioinformatics Core Services - Core Services for managing CyVerse KeyCloak - Identity and Access management Deployments \u00b6 Deployments are managed via Kuberentes (K8s) Databases \u00b6 CyVerse uses PostgreSQL as its primary Database platform. Each Database is provisioned separately","title":"Getting Started"},{"location":"services/getting_started/#installation","text":"The DevOps Guide section provies a list of required software for managing a CyVerse deployment","title":"Installation"},{"location":"services/getting_started/#authentication","text":"CyVerse authentication relies upon LDAP , OAUTH 2.0 protocol , and CILogon","title":"Authentication"},{"location":"services/getting_started/#security-configurations","text":"Experience Living in a Science DMZ is beneficial for deploying CyVerse on University hardware","title":" Security Configurations"},{"location":"services/getting_started/#apis","text":"Terrain API is the backbone service which manages the Discovery Environment data science workbench. Details about Terrain are presented in the API endpoints section","title":"APIs"},{"location":"services/getting_started/#products-services","text":"Authentication - managed user authentication uses Keycloak, CI Logon, and OAUTH 2.0 BisQue - large image analyses in the browser DataCommons - Community data sharing, DataCite data publishing with DOI Data Store - data storage, hosting, & sharing DNA Subway - educational software for high school and undergradautes in bioinformatics Core Services - Core Services for managing CyVerse KeyCloak - Identity and Access management","title":" Products &amp; Services"},{"location":"services/getting_started/#deployments","text":"Deployments are managed via Kuberentes (K8s)","title":" Deployments"},{"location":"services/getting_started/#databases","text":"CyVerse uses PostgreSQL as its primary Database platform. Each Database is provisioned separately","title":" Databases"},{"location":"services/keycloak/","text":"CyVerse Authentication via Keycloak, CILogon, & OAUTH sequenceDiagram autonumber User->>Browser: Click on external Auth Browser-->>Keycloak: Authentication request (TOKEN) loop Keycloak-->>Browser: Browser opened with ../auth?=client_id=de-prod=TOKEN end Keycloak-->>CILogon: Auth Request User->>CILogon: Enter Credentials Keycloak-->>OAUTH: Auth Response CILogon-->>Keycloak: Auth Response Browser-->>OAUTH: Ask for Token OAUTH-->>Browser: Retrieve Token Mermaid Diagram Users authenticate (starting on left side) via their browser, which passes through Keycloack to either CILogon or OAUTH. Keycloak service manages authenticaation via CILogon and OAUTH . Keycloak \u00b6 Keycloak is provisioned and deployed as part of the main K8s Keycloak provisioning with K8s","title":"Authentication"},{"location":"services/keycloak/#keycloak","text":"Keycloak is provisioned and deployed as part of the main K8s Keycloak provisioning with K8s","title":"Keycloak"},{"location":"services/services_overview/","text":"Products & Services Overview \u00b6 CyVerse provides multiple end-user services. The top of the CyVerse layer cake includes its Products & Services Primarily the Products and Services layers are served directly over https:// websites which then link to the CyVerse Terrain API, K8s clusters, Data Store, and Authentication layers. API (Terrain) Authentication BisQue Cloud Services Data Publishing Data Storage Discovery Environment DNA Subway","title":"Overview"},{"location":"services/services_overview/#products-services-overview","text":"CyVerse provides multiple end-user services. The top of the CyVerse layer cake includes its Products & Services Primarily the Products and Services layers are served directly over https:// websites which then link to the CyVerse Terrain API, K8s clusters, Data Store, and Authentication layers. API (Terrain) Authentication BisQue Cloud Services Data Publishing Data Storage Discovery Environment DNA Subway","title":"Products &amp; Services Overview"},{"location":"services/subscriptions/","text":"Subscriptions \u00b6 Subscriptions are managed in partnership with Phoenix Bioinformatics Subscription Enrollment Form Externally, credit-card billing and invoices are managed by Phoenix Bio. Internally, CyVerse user's account status and allocation settings and monitoring (compute hours, storage) are managed within the Discovery Environment Administrator Panel - TODO","title":"Subscriptions"},{"location":"services/subscriptions/#subscriptions","text":"Subscriptions are managed in partnership with Phoenix Bioinformatics Subscription Enrollment Form Externally, credit-card billing and invoices are managed by Phoenix Bio. Internally, CyVerse user's account status and allocation settings and monitoring (compute hours, storage) are managed within the Discovery Environment Administrator Panel - TODO","title":"Subscriptions"},{"location":"services/system_overview/","text":"CyVerse is both a Software as a Service (SaaS) and the Infrastructure as Code (IaC) necessary to manage a full stack cyberinfrastructure. The US public CyVerse primarily runs on hardware located at The University of Arizona, with a full data store mirror at the Texas Advanced Computing Center (TACC), and federated compute resources located across the US. The full CyVerse SaaS stack can be deployed either on-premises consumer hardware or on cloud resources. Data storage is managed by an iRODS Data Store . Computing can be done in either the Discovery Environment (DE) data science workbench or with the CACAO IaC which leverages both public research computing and commercial cloud. Event-based triggers are accomplished through the DataWatch API. CyVerse's Infrastructure as Code (IaC) provides computing, storage, and event-based components researchers rely upon for data intensive science. Application Programming Interfaces (APIs) \u00b6 All CyVerse APIs are OpenAPI compliant. Terrain API is the main API for Discovery Environment and uses a Swagger interface. Terrain API Jupyter Notebooks - provide an introduction to Terrain and show how to start and stop analyses. https://de.cyverse.org/terrain/swagger.json CACAO API - Infrastructure as Code API for cloud automation with OpenAPI Data Watch API - event based triggers for workflows with OpenAPI CyVerse public-facing APIs are frequently leveraged by \" Powered-by-CyVerse \" projects which utilize specific parts of the platform. Cloud Services \u00b6 Continous Automation / Continuous Analysis & Orchestration (CACAO) - Infrastructure as Code for multi-cloud deployments CACAO Terraform Templates DataWatch - a notification system for reporting data events Compute Resources \u00b6 The DE runs on-premises hardware located at University of Arizona (UArizona) in the UITS colocation space at the high performance computing center. The data store is mirrored nightly at TACC. CyVerse staff maintain over XXX servers at UArizona and 1 server at TACC. Hardware is added, replaced, or upgraded every few months. Table values below may not be up-to-date. Primary Hardware Specifications Compute Nodes (XXX nodes) System Configuration Aggregate information Per Node (Compute Node) Machine types Dell, SuperMicro, XXX Operating Systems Centos, Rocky Centos, Rocky Processor cores XX,XXX average XX CPUs 128, 64, 40, 32, 16 1, 2 RAM XXX TiB 256, 128, 64, 32 GiB Network 100 Gbps to Internet2 10 Gpbs to switch Storage X PB X TB GPU Nodes (XXX nodes) System Configuration Aggregate information Per Node (Compute Node) Machine types Dell, SuperMicro, XXX Operating Systems Centos, Rocky Centos, Rocky Processor cores 256 CPUs 2 RAM 1 TB, 512 GB GPUs NVIDIA (A100 80GB), (Tesla T4 16GB) 4 Network 100 Gbps to Internet2 10 Gpbs to switch Storage XXX TB 28 TB SSD, 21 TB NVMe Storage Resource Nodes (44 nodes) System Configuration Aggregate information Per Node (Compute Node) Machine types Dell, SuperMicro, XXX Operating Systems Centos, Ubuntu Centos, Ubuntu Processor cores 1506 average XX CPUs 128, 64, 40, 32, 16 1, 2 RAM 11 TiB 256, 128, 64, 32 GiB Network 100 Gbps to Internet2 10 Gpbs to switch Storage 18 PB X TB Federated Kubernetes Clusters \u00b6 CyVerse runs mainly on a locally managed K8s cluster, but it can be federated to other K8s clusters. The National Research Platform offers federated K8s resources. These resources are currently in development. OpenStack Cloud \u00b6 CyVerse maintains its own OpenStack Cloud (formerly \"Atmosphere\") for internal use and development of CACAO. Jetstream2 is primarily operated at Indiana University, but test clusters are shared across other universities in the US High Throughput Computing Environments: \u00b6 DE uses HTCondor for executable jobs on CyVerse resources and osg jobs on the OpenScienceGrid Federation to the OpenScienceGrid can be accomplished in the DE High Performance Computing Environments \u00b6 University of Arizona resources are colocated with the CyVerse data store and compute clusters CyVerse is partnered with Texas Advanced Computing Center (TACC) where its data store is replicated nightly. US based researchers can request access to HPC via: ACCESS-CI TACC Allocation request Data Storage \u00b6 The CyVerse Data Store manages over 6 PB data via iRODS (integrated Rule Oriented Data System) within the iplant zone. The zone name is related to the original project name. It is retained to preserve access to data through URLs published during the time period of the original project. Data storage is organized into resources. The main resource is named CyVerseRes , and it holds all user data and most project data. Data on CyVerseRes are stored ath the University of Arizona. This resource is mirrored with a second resource named taccRes which is backed by storage located at the Texas Advanced Computing Center (TACC). There are also special purpose resources dedicated to certain projects. The data in these resources are stored on hardware owned by these projects. Interfaces \u00b6 User Portal - a User Portal for creating and managing accounts, requesting and granting access to platforms, and a user management space for individuals and groups and workshops. Discovery Environment - Custom interactive web based data science workbench KeyCloak - federated OAUTH to CyVerse resources, including Google, GitHub, ORCID,& CILogon WebDAV - A service that provides secure HTTP/WebDAV access to the Data Store. It provides anonymous access to public data and authenticated access to private and shared data. SFTP - A service that provides secure FTP access to the Data Store. The service can be accessed through sftp://data.cyverse.org. Data Commons - This service provides secure HTTP access to published datasets that are hosted in the CyVerse Data Store. The Data Commons presents any metadata which have been added by the owners to their datasets. Monitoring Services \u00b6 Health Status - system status monitor perfSONAR web toolkit - network measurement toolkit","title":"System Overview"},{"location":"services/system_overview/#application-programming-interfaces-apis","text":"All CyVerse APIs are OpenAPI compliant. Terrain API is the main API for Discovery Environment and uses a Swagger interface. Terrain API Jupyter Notebooks - provide an introduction to Terrain and show how to start and stop analyses. https://de.cyverse.org/terrain/swagger.json CACAO API - Infrastructure as Code API for cloud automation with OpenAPI Data Watch API - event based triggers for workflows with OpenAPI CyVerse public-facing APIs are frequently leveraged by \" Powered-by-CyVerse \" projects which utilize specific parts of the platform.","title":" Application Programming Interfaces (APIs)"},{"location":"services/system_overview/#cloud-services","text":"Continous Automation / Continuous Analysis & Orchestration (CACAO) - Infrastructure as Code for multi-cloud deployments CACAO Terraform Templates DataWatch - a notification system for reporting data events","title":" Cloud Services"},{"location":"services/system_overview/#compute-resources","text":"The DE runs on-premises hardware located at University of Arizona (UArizona) in the UITS colocation space at the high performance computing center. The data store is mirrored nightly at TACC. CyVerse staff maintain over XXX servers at UArizona and 1 server at TACC. Hardware is added, replaced, or upgraded every few months. Table values below may not be up-to-date. Primary Hardware Specifications Compute Nodes (XXX nodes) System Configuration Aggregate information Per Node (Compute Node) Machine types Dell, SuperMicro, XXX Operating Systems Centos, Rocky Centos, Rocky Processor cores XX,XXX average XX CPUs 128, 64, 40, 32, 16 1, 2 RAM XXX TiB 256, 128, 64, 32 GiB Network 100 Gbps to Internet2 10 Gpbs to switch Storage X PB X TB GPU Nodes (XXX nodes) System Configuration Aggregate information Per Node (Compute Node) Machine types Dell, SuperMicro, XXX Operating Systems Centos, Rocky Centos, Rocky Processor cores 256 CPUs 2 RAM 1 TB, 512 GB GPUs NVIDIA (A100 80GB), (Tesla T4 16GB) 4 Network 100 Gbps to Internet2 10 Gpbs to switch Storage XXX TB 28 TB SSD, 21 TB NVMe Storage Resource Nodes (44 nodes) System Configuration Aggregate information Per Node (Compute Node) Machine types Dell, SuperMicro, XXX Operating Systems Centos, Ubuntu Centos, Ubuntu Processor cores 1506 average XX CPUs 128, 64, 40, 32, 16 1, 2 RAM 11 TiB 256, 128, 64, 32 GiB Network 100 Gbps to Internet2 10 Gpbs to switch Storage 18 PB X TB","title":" Compute Resources"},{"location":"services/system_overview/#federated-kubernetes-clusters","text":"CyVerse runs mainly on a locally managed K8s cluster, but it can be federated to other K8s clusters. The National Research Platform offers federated K8s resources. These resources are currently in development.","title":" Federated Kubernetes Clusters"},{"location":"services/system_overview/#openstack-cloud","text":"CyVerse maintains its own OpenStack Cloud (formerly \"Atmosphere\") for internal use and development of CACAO. Jetstream2 is primarily operated at Indiana University, but test clusters are shared across other universities in the US","title":" OpenStack Cloud"},{"location":"services/system_overview/#high-throughput-computing-environments","text":"DE uses HTCondor for executable jobs on CyVerse resources and osg jobs on the OpenScienceGrid Federation to the OpenScienceGrid can be accomplished in the DE","title":" High Throughput Computing Environments:"},{"location":"services/system_overview/#high-performance-computing-environments","text":"University of Arizona resources are colocated with the CyVerse data store and compute clusters CyVerse is partnered with Texas Advanced Computing Center (TACC) where its data store is replicated nightly. US based researchers can request access to HPC via: ACCESS-CI TACC Allocation request","title":" High Performance Computing Environments"},{"location":"services/system_overview/#data-storage","text":"The CyVerse Data Store manages over 6 PB data via iRODS (integrated Rule Oriented Data System) within the iplant zone. The zone name is related to the original project name. It is retained to preserve access to data through URLs published during the time period of the original project. Data storage is organized into resources. The main resource is named CyVerseRes , and it holds all user data and most project data. Data on CyVerseRes are stored ath the University of Arizona. This resource is mirrored with a second resource named taccRes which is backed by storage located at the Texas Advanced Computing Center (TACC). There are also special purpose resources dedicated to certain projects. The data in these resources are stored on hardware owned by these projects.","title":" Data Storage"},{"location":"services/system_overview/#interfaces","text":"User Portal - a User Portal for creating and managing accounts, requesting and granting access to platforms, and a user management space for individuals and groups and workshops. Discovery Environment - Custom interactive web based data science workbench KeyCloak - federated OAUTH to CyVerse resources, including Google, GitHub, ORCID,& CILogon WebDAV - A service that provides secure HTTP/WebDAV access to the Data Store. It provides anonymous access to public data and authenticated access to private and shared data. SFTP - A service that provides secure FTP access to the Data Store. The service can be accessed through sftp://data.cyverse.org. Data Commons - This service provides secure HTTP access to published datasets that are hosted in the CyVerse Data Store. The Data Commons presents any metadata which have been added by the owners to their datasets.","title":" Interfaces"},{"location":"services/system_overview/#monitoring-services","text":"Health Status - system status monitor perfSONAR web toolkit - network measurement toolkit","title":" Monitoring Services"},{"location":"services/api/endpoint-index/","text":"Jump to: /admin /apps /coge /favorites /filesystem /permanent-id-requests /secured /send-notification /uuid get \u00b6 GET / admin \u00b6 GET /admin/apps/categories GET /admin/apps/categories/search POST /admin/apps/categories/{system-id} DELETE /admin/apps/categories/{system-id}/{category-id} PATCH /admin/apps/categories/{system-id}/{category-id} DELETE /admin/apps/{app-id}/comments/{comment-id} PATCH /admin/apps/{app-id}/comments/{comment-id} GET /admin/apps/{app-id}/metadata POST /admin/apps/{app-id}/metadata PUT /admin/apps/{app-id}/metadata DELETE /admin/filesystem/entry/{entry-id}/comments/{comment-id} PATCH /admin/filesystem/entry/{entry-id}/comments/{comment-id} GET /admin/filesystem/metadata/templates POST /admin/filesystem/metadata/templates DELETE /admin/filesystem/metadata/templates/{template-id} POST /admin/filesystem/metadata/templates/{template-id} GET /admin/notifications/system PUT /admin/notifications/system GET /admin/notifications/system-types DELETE /admin/notifications/system/:uuid GET /admin/notifications/system/:uuid POST /admin/notifications/system/:uuid GET /admin/ontologies POST /admin/ontologies DELETE /admin/ontologies/{ontology-version} GET /admin/ontologies/{ontology-version} POST /admin/ontologies/{ontology-version} DELETE /admin/ontologies/{ontology-version}/{root-iri} GET /admin/ontologies/{ontology-version}/{root-iri} PUT /admin/ontologies/{ontology-version}/{root-iri} GET /admin/ontologies/{ontology-version}/{root-iri}/apps GET /admin/ontologies/{ontology-version}/{root-iri}/unclassified GET /admin/permanent-id-requests GET /admin/permanent-id-requests/{request-id} POST /admin/permanent-id-requests/{request-id}/ezid POST /admin/permanent-id-requests/{request-id}/status DELETE /admin/workspaces GET /admin/workspaces apps \u00b6 GET /apps/{app-id}/comments POST /apps/{app-id}/comments PATCH /apps/{app-id}/comments/{comment-id} PATCH /apps/{app-id}/comments/{comment-id} coge \u00b6 GET /coge/genomes POST /coge/genomes/load POST /coge/genomes/{genome-id}/export-fasta favorites \u00b6 GET /favorites/filesystem filesystem \u00b6 PATCH /filesystem/entry/{entry-id}/comments/{comment-id} permanent-id-requests \u00b6 GET /permanent-id-requests POST /permanent-id-requests GET /permanent-id-requests/status-codes GET /permanent-id-requests/types GET /permanent-id-requests/{request-id} secured \u00b6 GET /secured/favorites/filesystem DELETE /secured/favorites/filesystem/{favorite} PUT /secured/favorites/filesystem/{favorite} POST /secured/favorites/filter GET /secured/fileio/download POST /secured/fileio/save POST /secured/fileio/saveas POST /secured/fileio/upload POST /secured/fileio/urlupload POST /secured/filesystem/delete POST /secured/filesystem/delete-contents POST /secured/filesystem/delete-tickets POST /secured/filesystem/directories GET /secured/filesystem/directory POST /secured/filesystem/directory/create GET /secured/filesystem/display-download GET /secured/filesystem/entry/{entry-id}/comments POST /secured/filesystem/entry/{entry-id}/comments PATCH /secured/filesystem/entry/{entry-id}/comments/{comment-id} POST /secured/filesystem/exists GET /secured/filesystem/file/manifest GET /secured/filesystem/index POST /secured/filesystem/list-tickets POST /secured/filesystem/metadata/csv-parser GET /secured/filesystem/metadata/template/attr/{attribute-id} GET /secured/filesystem/metadata/template/{template-id} GET /secured/filesystem/metadata/template/{template-id}/blank-csv GET /secured/filesystem/metadata/template/{template-id}/guide-csv GET /secured/filesystem/metadata/templates POST /secured/filesystem/move POST /secured/filesystem/move-contents GET /secured/filesystem/paged-directory [ POST /secured/filesystem/path-list-creator ](endpoints/filesystem/path-lists.md#ht-path-list-creator` POST /secured/filesystem/read-chunk POST /secured/filesystem/read-csv-chunk POST /secured/filesystem/rename [ POST /secured/filesystem/restore ](endpoints/filesystem/restore.md#restoring-a-file-or-directory-from-a-users-trash` [ POST /secured/filesystem/restore-all ](endpoints/filesystem/restore.md#restoring-all-items-in-a-users-trash` GET /secured/filesystem/root POST /secured/filesystem/stat POST /secured/filesystem/tickets DELETE /secured/filesystem/trash POST /secured/filesystem/user-permissions GET /secured/filesystem/{data-id}/metadata POST /secured/filesystem/{data-id}/metadata [ POST /secured/filesystem/{data-id}/metadata/copy ](endpoints/filesystem/metadata.md#copying-all-metadata-from-a-filefolder` POST /secured/filesystem/{data-id}/metadata/save POST /secured/filesystem/{data-id}/ore/save GET /secured/notifications/count-messages POST /secured/notifications/delete DELETE /secured/notifications/delete-all GET /secured/notifications/last-ten-messages POST /secured/notifications/mark-all-seen GET /secured/notifications/messages POST /secured/notifications/seen POST /secured/notifications/system/delete DELETE /secured/notifications/system/delete-all POST /secured/notifications/system/mark-all-received POST /secured/notifications/system/mark-all-seen GET /secured/notifications/system/messages GET /secured/notifications/system/new-messages POST /secured/notifications/system/received POST /secured/notifications/system/seen GET /secured/notifications/system/unseen-messages GET /secured/notifications/unseen-messages GET /secured/oauth/access-code/{api-name} DELETE /secured/preferences GET /secured/preferences POST /secured/preferences DELETE /secured/sessions GET /secured/sessions POST /secured/sessions send-notification \u00b6 POST /send-notification. uuid \u00b6 GET /uuid","title":"Endpoints Index"},{"location":"services/api/endpoint-index/#get","text":"GET /","title":"get"},{"location":"services/api/endpoint-index/#admin","text":"GET /admin/apps/categories GET /admin/apps/categories/search POST /admin/apps/categories/{system-id} DELETE /admin/apps/categories/{system-id}/{category-id} PATCH /admin/apps/categories/{system-id}/{category-id} DELETE /admin/apps/{app-id}/comments/{comment-id} PATCH /admin/apps/{app-id}/comments/{comment-id} GET /admin/apps/{app-id}/metadata POST /admin/apps/{app-id}/metadata PUT /admin/apps/{app-id}/metadata DELETE /admin/filesystem/entry/{entry-id}/comments/{comment-id} PATCH /admin/filesystem/entry/{entry-id}/comments/{comment-id} GET /admin/filesystem/metadata/templates POST /admin/filesystem/metadata/templates DELETE /admin/filesystem/metadata/templates/{template-id} POST /admin/filesystem/metadata/templates/{template-id} GET /admin/notifications/system PUT /admin/notifications/system GET /admin/notifications/system-types DELETE /admin/notifications/system/:uuid GET /admin/notifications/system/:uuid POST /admin/notifications/system/:uuid GET /admin/ontologies POST /admin/ontologies DELETE /admin/ontologies/{ontology-version} GET /admin/ontologies/{ontology-version} POST /admin/ontologies/{ontology-version} DELETE /admin/ontologies/{ontology-version}/{root-iri} GET /admin/ontologies/{ontology-version}/{root-iri} PUT /admin/ontologies/{ontology-version}/{root-iri} GET /admin/ontologies/{ontology-version}/{root-iri}/apps GET /admin/ontologies/{ontology-version}/{root-iri}/unclassified GET /admin/permanent-id-requests GET /admin/permanent-id-requests/{request-id} POST /admin/permanent-id-requests/{request-id}/ezid POST /admin/permanent-id-requests/{request-id}/status DELETE /admin/workspaces GET /admin/workspaces","title":"admin"},{"location":"services/api/endpoint-index/#apps","text":"GET /apps/{app-id}/comments POST /apps/{app-id}/comments PATCH /apps/{app-id}/comments/{comment-id} PATCH /apps/{app-id}/comments/{comment-id}","title":"apps"},{"location":"services/api/endpoint-index/#coge","text":"GET /coge/genomes POST /coge/genomes/load POST /coge/genomes/{genome-id}/export-fasta","title":"coge"},{"location":"services/api/endpoint-index/#favorites","text":"GET /favorites/filesystem","title":"favorites"},{"location":"services/api/endpoint-index/#filesystem","text":"PATCH /filesystem/entry/{entry-id}/comments/{comment-id}","title":"filesystem"},{"location":"services/api/endpoint-index/#permanent-id-requests","text":"GET /permanent-id-requests POST /permanent-id-requests GET /permanent-id-requests/status-codes GET /permanent-id-requests/types GET /permanent-id-requests/{request-id}","title":"permanent-id-requests"},{"location":"services/api/endpoint-index/#secured","text":"GET /secured/favorites/filesystem DELETE /secured/favorites/filesystem/{favorite} PUT /secured/favorites/filesystem/{favorite} POST /secured/favorites/filter GET /secured/fileio/download POST /secured/fileio/save POST /secured/fileio/saveas POST /secured/fileio/upload POST /secured/fileio/urlupload POST /secured/filesystem/delete POST /secured/filesystem/delete-contents POST /secured/filesystem/delete-tickets POST /secured/filesystem/directories GET /secured/filesystem/directory POST /secured/filesystem/directory/create GET /secured/filesystem/display-download GET /secured/filesystem/entry/{entry-id}/comments POST /secured/filesystem/entry/{entry-id}/comments PATCH /secured/filesystem/entry/{entry-id}/comments/{comment-id} POST /secured/filesystem/exists GET /secured/filesystem/file/manifest GET /secured/filesystem/index POST /secured/filesystem/list-tickets POST /secured/filesystem/metadata/csv-parser GET /secured/filesystem/metadata/template/attr/{attribute-id} GET /secured/filesystem/metadata/template/{template-id} GET /secured/filesystem/metadata/template/{template-id}/blank-csv GET /secured/filesystem/metadata/template/{template-id}/guide-csv GET /secured/filesystem/metadata/templates POST /secured/filesystem/move POST /secured/filesystem/move-contents GET /secured/filesystem/paged-directory [ POST /secured/filesystem/path-list-creator ](endpoints/filesystem/path-lists.md#ht-path-list-creator` POST /secured/filesystem/read-chunk POST /secured/filesystem/read-csv-chunk POST /secured/filesystem/rename [ POST /secured/filesystem/restore ](endpoints/filesystem/restore.md#restoring-a-file-or-directory-from-a-users-trash` [ POST /secured/filesystem/restore-all ](endpoints/filesystem/restore.md#restoring-all-items-in-a-users-trash` GET /secured/filesystem/root POST /secured/filesystem/stat POST /secured/filesystem/tickets DELETE /secured/filesystem/trash POST /secured/filesystem/user-permissions GET /secured/filesystem/{data-id}/metadata POST /secured/filesystem/{data-id}/metadata [ POST /secured/filesystem/{data-id}/metadata/copy ](endpoints/filesystem/metadata.md#copying-all-metadata-from-a-filefolder` POST /secured/filesystem/{data-id}/metadata/save POST /secured/filesystem/{data-id}/ore/save GET /secured/notifications/count-messages POST /secured/notifications/delete DELETE /secured/notifications/delete-all GET /secured/notifications/last-ten-messages POST /secured/notifications/mark-all-seen GET /secured/notifications/messages POST /secured/notifications/seen POST /secured/notifications/system/delete DELETE /secured/notifications/system/delete-all POST /secured/notifications/system/mark-all-received POST /secured/notifications/system/mark-all-seen GET /secured/notifications/system/messages GET /secured/notifications/system/new-messages POST /secured/notifications/system/received POST /secured/notifications/system/seen GET /secured/notifications/system/unseen-messages GET /secured/notifications/unseen-messages GET /secured/oauth/access-code/{api-name} DELETE /secured/preferences GET /secured/preferences POST /secured/preferences DELETE /secured/sessions GET /secured/sessions POST /secured/sessions","title":"secured"},{"location":"services/api/endpoint-index/#send-notification","text":"POST /send-notification.","title":"send-notification"},{"location":"services/api/endpoint-index/#uuid","text":"GET /uuid","title":"uuid"},{"location":"services/api/errors/","text":"Errors \u00b6 If a service call causes an exception that is not caught by the service itself then Terrain will respond with a standardized error message: { \"error_code\" : \"ERR_UNCHECKED_EXCEPTION\" , \"message\" : reaso n - f or -e rror } The HTTP status code that is returned will either be a 400 or a 500, depending on which type of exception is caught. In either case, the reason for the error should be examined. If the logging level is set to error or lower then the exception will be logged in Terrain's log file along with a stack trace. This can be helpful in cases where the true cause of the error isn't obvious at first.","title":"Errors"},{"location":"services/api/errors/#errors","text":"If a service call causes an exception that is not caught by the service itself then Terrain will respond with a standardized error message: { \"error_code\" : \"ERR_UNCHECKED_EXCEPTION\" , \"message\" : reaso n - f or -e rror } The HTTP status code that is returned will either be a 400 or a 500, depending on which type of exception is caught. In either case, the reason for the error should be examined. If the logging level is set to error or lower then the exception will be logged in Terrain's log file along with a stack trace. This can be helpful in cases where the true cause of the error isn't obvious at first.","title":"Errors"},{"location":"services/api/endpoints/app-metadata/","text":"Jump to: Application Metadata Endpoints Adding Categories Searching for Categories by Name Deleting a Category by ID Updating an App Category Managing App AVU Metadata Listing App Categories Listing Workspaces Deleting Workspaces Application Metadata Endpoints \u00b6 Note that secured endpoints in Terrain and apps are a little different from each other. Please see Terrain Vs. Apps for more information. Adding Categories \u00b6 POST /admin/apps/categories/{system-id} This endpoint is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more information. Searching for Categories by Name \u00b6 GET /admin/apps/categories/search This endpoint is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more information. Deleting a Category by ID \u00b6 DELETE /admin/apps/categories/{system-id}/{category-id} This endpoint is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more information. Updating an App Category \u00b6 PATCH /admin/apps/categories/{system-id}/{category-id} This endpoint is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more information. Managing App AVU Metadata \u00b6 GET /admin/apps/{app-id}/metadata POST /admin/apps/{app-id}/metadata PUT /admin/apps/{app-id}/metadata These endpoints are passthroughs to the apps endpoints using the same paths. Please see the corresponding service documentation for more information. Listing App Categories \u00b6 GET /admin/apps/categories Delegates to apps: GET /admin/apps/categories These endpoints are passthroughs to the apps endpoints using the same path. Please see the apps service documentation for more information. Listing Workspaces \u00b6 GET /admin/workspaces This service is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more details. Deleting Workspaces \u00b6 DELETE /admin/workspaces This service is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more details.","title":"App Metadata"},{"location":"services/api/endpoints/app-metadata/#application-metadata-endpoints","text":"Note that secured endpoints in Terrain and apps are a little different from each other. Please see Terrain Vs. Apps for more information.","title":"Application Metadata Endpoints"},{"location":"services/api/endpoints/app-metadata/#adding-categories","text":"POST /admin/apps/categories/{system-id} This endpoint is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more information.","title":"Adding Categories"},{"location":"services/api/endpoints/app-metadata/#searching-for-categories-by-name","text":"GET /admin/apps/categories/search This endpoint is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more information.","title":"Searching for Categories by Name"},{"location":"services/api/endpoints/app-metadata/#deleting-a-category-by-id","text":"DELETE /admin/apps/categories/{system-id}/{category-id} This endpoint is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more information.","title":"Deleting a Category by ID"},{"location":"services/api/endpoints/app-metadata/#updating-an-app-category","text":"PATCH /admin/apps/categories/{system-id}/{category-id} This endpoint is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more information.","title":"Updating an App Category"},{"location":"services/api/endpoints/app-metadata/#managing-app-avu-metadata","text":"GET /admin/apps/{app-id}/metadata POST /admin/apps/{app-id}/metadata PUT /admin/apps/{app-id}/metadata These endpoints are passthroughs to the apps endpoints using the same paths. Please see the corresponding service documentation for more information.","title":"Managing App AVU Metadata"},{"location":"services/api/endpoints/app-metadata/#listing-app-categories","text":"GET /admin/apps/categories Delegates to apps: GET /admin/apps/categories These endpoints are passthroughs to the apps endpoints using the same path. Please see the apps service documentation for more information.","title":"Listing App Categories"},{"location":"services/api/endpoints/app-metadata/#listing-workspaces","text":"GET /admin/workspaces This service is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more details.","title":"Listing Workspaces"},{"location":"services/api/endpoints/app-metadata/#deleting-workspaces","text":"DELETE /admin/workspaces This service is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more details.","title":"Deleting Workspaces"},{"location":"services/api/endpoints/app-ontologies/","text":"Table of Contents \u00b6 Categorizing Apps with Ontology Hierarchies Save an Ontology XML Document Listing Saved Ontology Details Logically Deleting an Ontology Set Active Ontology Version Save an Ontology Hierarchy Deleting an Ontology Hierarchy Listing Hierarchies for any Ontology Listing Filtered Hierarchies for any Ontology Listing Apps in Hierarchies for any Ontology Listing Unclassified Apps for any Ontology Categorizing Apps with Ontology Hierarchies \u00b6 First an admin must upload an ontology XML document, which will be stored in the metadata database, and later parsed with the OWLAPI libraries in order to build the app categories. Save an Ontology XML Document Once 1 or more ontology XML documents have been uploaded, the details of each available ontology may be listed. Each uploaded document is stored with a unique \"version\" ID. This version ID is used in the remaining endpoints in order to save and list ontology classes and hierarchies parsed from the associated ontology. Listing Saved Ontology Details Ontologies may be removed from this listing by marking them as deleted: Logically Deleting an Ontology Next an admin must choose which hierarchies (and the ontology classes in them) will be available for use in the DE. Save an Ontology Hierarchy An admin may delete all classes under any ontology class hierarchy which will allow the admin to selectively remove sub-hierarchies, or the entire hierarchy from the root. Deleting an Ontology Hierarchy Deleted classes and sub-hierarchies can easily be re-added by re-saving the hierarchy root (only classes and hierarchies that are not already saved under the given root are added). Finally, the admin sets an ontology version as the default, active version used by the DE when listing hierarchies or ontology classes for regular users. Set Active Ontology Version This active version is used by the apps service in the following endpoints: Listing Ontology Hierarchies Listing Filtered Ontology Hierarchies Listing Unclassified Apps for the Active Ontology Admins or users with app \"write\" permissions may categorize those apps under ontology classes by attaching the class IRIs as metadata. Managing App AVU Metadata Apps with an ontology class attached as metadata, or any class under the hierarchy of that root class, may be listed with the following endpoint: Listing Apps in Hierarchies for the Active Ontology An admin may add hierarchies for other ontology versions and preview their filtered hierarchies and app listings, without affecting the active version in use by normal users. Listing Hierarchies for any Ontology Listing Filtered Hierarchies for any Ontology Listing Apps in Hierarchies for any Ontology Listing Unclassified Apps for any Ontology Save an Ontology XML Document \u00b6 Secured Endpoint: POST /admin/ontologies Delegates to metadata: POST /admin/ontologies This endpoint is a passthrough to the metadata endpoint using the same path. Please see the metadata service documentation for more information. Listing Saved Ontology Details \u00b6 Secured Endpoint: GET /admin/ontologies Delegates to apps: GET /admin/ontologies This endpoint is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more information. Logically Deleting an Ontology \u00b6 Secured Endpoint: DELETE /admin/ontologies/{ontology-version} Delegates to apps: DELETE /admin/ontologies/{ontology-version} This endpoint is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more information. Set Active Ontology Version \u00b6 Secured Endpoint: POST /admin/ontologies/{ontology-version} Delegates to apps: POST /admin/ontologies/{ontology-version} This endpoint is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more information. Save an Ontology Hierarchy \u00b6 Secured Endpoint: PUT /admin/ontologies/{ontology-version}/{root-iri} Delegates to metadata: PUT /admin/ontologies/{ontology-version}/{root-iri} This endpoint is a passthrough to the metadata endpoint using the same path. Please see the metadata service documentation for more information. Deleting an Ontology Hierarchy \u00b6 Secured Endpoint: DELETE /admin/ontologies/{ontology-version}/{root-iri} Delegates to metadata: DELETE /admin/ontologies/{ontology-version}/{root-iri} This endpoint is a passthrough to the metadata endpoint using the same path. Please see the metadata service documentation for more information. Listing Hierarchies for any Ontology \u00b6 Secured Endpoint: GET /admin/ontologies/{ontology-version} Delegates to metadata: GET /ontologies/{ontology-version} This endpoint is a passthrough to the metadata endpoint using the same path. Please see the metadata service documentation for more information. Listing Filtered Hierarchies for any Ontology \u00b6 Secured Endpoint: GET /admin/ontologies/{ontology-version}/{root-iri} Delegates to apps: GET /admin/ontologies/{ontology-version}/{root-iri} This endpoint is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more information. Listing Apps in Hierarchies for any Ontology \u00b6 Secured Endpoint: GET /admin/ontologies/{ontology-version}/{root-iri}/apps Delegates to apps: GET /admin/ontologies/{ontology-version}/{root-iri}/apps This endpoint is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more information. Listing Unclassified Apps for any Ontology \u00b6 Secured Endpoint: GET /admin/ontologies/{ontology-version}/{root-iri}/unclassified Delegates to apps: GET /admin/ontologies/{ontology-version}/{root-iri}/unclassified This endpoint is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more information.","title":"Ontology Hierarchies"},{"location":"services/api/endpoints/app-ontologies/#table-of-contents","text":"Categorizing Apps with Ontology Hierarchies Save an Ontology XML Document Listing Saved Ontology Details Logically Deleting an Ontology Set Active Ontology Version Save an Ontology Hierarchy Deleting an Ontology Hierarchy Listing Hierarchies for any Ontology Listing Filtered Hierarchies for any Ontology Listing Apps in Hierarchies for any Ontology Listing Unclassified Apps for any Ontology","title":"Table of Contents"},{"location":"services/api/endpoints/app-ontologies/#categorizing-apps-with-ontology-hierarchies","text":"First an admin must upload an ontology XML document, which will be stored in the metadata database, and later parsed with the OWLAPI libraries in order to build the app categories. Save an Ontology XML Document Once 1 or more ontology XML documents have been uploaded, the details of each available ontology may be listed. Each uploaded document is stored with a unique \"version\" ID. This version ID is used in the remaining endpoints in order to save and list ontology classes and hierarchies parsed from the associated ontology. Listing Saved Ontology Details Ontologies may be removed from this listing by marking them as deleted: Logically Deleting an Ontology Next an admin must choose which hierarchies (and the ontology classes in them) will be available for use in the DE. Save an Ontology Hierarchy An admin may delete all classes under any ontology class hierarchy which will allow the admin to selectively remove sub-hierarchies, or the entire hierarchy from the root. Deleting an Ontology Hierarchy Deleted classes and sub-hierarchies can easily be re-added by re-saving the hierarchy root (only classes and hierarchies that are not already saved under the given root are added). Finally, the admin sets an ontology version as the default, active version used by the DE when listing hierarchies or ontology classes for regular users. Set Active Ontology Version This active version is used by the apps service in the following endpoints: Listing Ontology Hierarchies Listing Filtered Ontology Hierarchies Listing Unclassified Apps for the Active Ontology Admins or users with app \"write\" permissions may categorize those apps under ontology classes by attaching the class IRIs as metadata. Managing App AVU Metadata Apps with an ontology class attached as metadata, or any class under the hierarchy of that root class, may be listed with the following endpoint: Listing Apps in Hierarchies for the Active Ontology An admin may add hierarchies for other ontology versions and preview their filtered hierarchies and app listings, without affecting the active version in use by normal users. Listing Hierarchies for any Ontology Listing Filtered Hierarchies for any Ontology Listing Apps in Hierarchies for any Ontology Listing Unclassified Apps for any Ontology","title":"Categorizing Apps with Ontology Hierarchies"},{"location":"services/api/endpoints/app-ontologies/#save-an-ontology-xml-document","text":"Secured Endpoint: POST /admin/ontologies Delegates to metadata: POST /admin/ontologies This endpoint is a passthrough to the metadata endpoint using the same path. Please see the metadata service documentation for more information.","title":"Save an Ontology XML Document"},{"location":"services/api/endpoints/app-ontologies/#listing-saved-ontology-details","text":"Secured Endpoint: GET /admin/ontologies Delegates to apps: GET /admin/ontologies This endpoint is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more information.","title":"Listing Saved Ontology Details"},{"location":"services/api/endpoints/app-ontologies/#logically-deleting-an-ontology","text":"Secured Endpoint: DELETE /admin/ontologies/{ontology-version} Delegates to apps: DELETE /admin/ontologies/{ontology-version} This endpoint is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more information.","title":"Logically Deleting an Ontology"},{"location":"services/api/endpoints/app-ontologies/#set-active-ontology-version","text":"Secured Endpoint: POST /admin/ontologies/{ontology-version} Delegates to apps: POST /admin/ontologies/{ontology-version} This endpoint is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more information.","title":"Set Active Ontology Version"},{"location":"services/api/endpoints/app-ontologies/#save-an-ontology-hierarchy","text":"Secured Endpoint: PUT /admin/ontologies/{ontology-version}/{root-iri} Delegates to metadata: PUT /admin/ontologies/{ontology-version}/{root-iri} This endpoint is a passthrough to the metadata endpoint using the same path. Please see the metadata service documentation for more information.","title":"Save an Ontology Hierarchy"},{"location":"services/api/endpoints/app-ontologies/#deleting-an-ontology-hierarchy","text":"Secured Endpoint: DELETE /admin/ontologies/{ontology-version}/{root-iri} Delegates to metadata: DELETE /admin/ontologies/{ontology-version}/{root-iri} This endpoint is a passthrough to the metadata endpoint using the same path. Please see the metadata service documentation for more information.","title":"Deleting an Ontology Hierarchy"},{"location":"services/api/endpoints/app-ontologies/#listing-hierarchies-for-any-ontology","text":"Secured Endpoint: GET /admin/ontologies/{ontology-version} Delegates to metadata: GET /ontologies/{ontology-version} This endpoint is a passthrough to the metadata endpoint using the same path. Please see the metadata service documentation for more information.","title":"Listing Hierarchies for any Ontology"},{"location":"services/api/endpoints/app-ontologies/#listing-filtered-hierarchies-for-any-ontology","text":"Secured Endpoint: GET /admin/ontologies/{ontology-version}/{root-iri} Delegates to apps: GET /admin/ontologies/{ontology-version}/{root-iri} This endpoint is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more information.","title":"Listing Filtered Hierarchies for any Ontology"},{"location":"services/api/endpoints/app-ontologies/#listing-apps-in-hierarchies-for-any-ontology","text":"Secured Endpoint: GET /admin/ontologies/{ontology-version}/{root-iri}/apps Delegates to apps: GET /admin/ontologies/{ontology-version}/{root-iri}/apps This endpoint is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more information.","title":"Listing Apps in Hierarchies for any Ontology"},{"location":"services/api/endpoints/app-ontologies/#listing-unclassified-apps-for-any-ontology","text":"Secured Endpoint: GET /admin/ontologies/{ontology-version}/{root-iri}/unclassified Delegates to apps: GET /admin/ontologies/{ontology-version}/{root-iri}/unclassified This endpoint is a passthrough to the apps endpoint using the same path. Please see the apps service documentation for more information.","title":"Listing Unclassified Apps for any Ontology"},{"location":"services/api/endpoints/callbacks/","text":"Callback Endpoints \u00b6 These endpoints listed in this document accept callbacks from other services indicating that some event has occurred. Obtaining OAuth Authorization Codes \u00b6 Secured Endpoint: GET /secured/oauth/access-code/{api-name} Delegates to apps: GET /secured/oauth/access-code/{api-name} The only API name that is currently supported is agave . The DE calls this endpoint after the user gives the DE permission to access a third-party API on his or her behalf. Agave sends a callback to a servlet in the DE, which obtains a CAS proxy ticket and sends a request to this service to obtain an access token for the user. This service requires the following query parameters: Parameter Description code The authorization code provided by the OAuth server. state A UUID referring to the original authorization request. Upon success, the response to this service is in the following format: { \"state_info\" : \"{arbitrary-state-information}\" } This service is used any time another service determines that an authorization code is required in order to proceed. The steps are described in detail below: Step 1: The other service determines that authorization is required. \u00b6 This can happen in two different ways: No access token is found in the database for the user. An access token is found, but attempts to use or refresh the token failed. Step 2: The other service stores state information in the database. \u00b6 This state information is specific to each service, and enables the DE to route the user back to the same window in the DE after the authorization process is complete. The state information is associated with a UUID and the username of the current user. This adds an extra layer of security because it allows this service (the service that accepts the authorization code) to ensure that the user who obtained the authorization code is the same as the user who initiated the request that required it. Step 3: The user is redirected to the OAuth authorization service. \u00b6 This redirection contains several query parameters that are either required or recommended by the OAuth 2.0 specification: Parameter Description response_type The type of response requested (always set to \"code\"). client_id The DE's OAuth API client identifier. redirect_uri The URI to redirect the user to after authorization state The UUID associated with the state information from step 2. Step 4: The user either grants or denies authorization. \u00b6 This step happens outside of the DE. The user authenticates to the external service and either grants or denies the authorization request. Step 5: The user is redirected back to the DE. \u00b6 After granting or denying authorization, the user is redirected to the URI specified in the redirect_uri parameter described above. There are two possibilities for this step: either the user granted or denied authorization. If the user granted authorization then two query parameters will be included in the request: Parameter Description code The authorization code provided by the OAuth server. state The same value that was sent to the authorization service. In this case, a request is sent to Terrain so that an access token can be obtained. If the user denied authorization or an error occurred, then the response will contain at least two and, possibly up to four query parameters: Parameter Required error Yes An error code indicating what went wrong. error_description No A human-readable description of the error. error_uri No A link to a human-readable description of the error. state Yes The same value that was sent to the authorization service. In this case, the user is redirected back to the main page in the DE along with some query parameters indicating that authorization was not obtained. This request is handled by a servlet that fields requests to any URI path that begins with the context path followed by oauth/callback/ . There should be one additional path element: the name of the API. For example, assuming that the context path is /de and the API name is agave , the path in the request would be /de/oauth/callback/agave . Step 6: A request is sent to the access-code service. \u00b6 This step only occurs if the user granted authorization to the DE. The servlet handling the redirection in the previous step extracts the API name from the URI path and the authorization code and state identifier from the query string. The API name is used to construct the URI path for the service request. For example, if the API name is agave then the URI path for the service request is /secured/oauth/access-code/agave . The query string parameters are passed to the service in the query string along with the usual CAS proxy token. Step 7: The state identifier is validated. \u00b6 This step only occurs if the user granted authorization to the DE. The /secured/oauth/access-code/{api-name} service obtains the user information using the CAS proxy token and gets the state identifier from the query string. The state information is then retrieved from the database using the state identifier. If the state information isn't found or is associated with a different user, the service responds with an error status. Otherwise, the state information is removed from the database and the serivce proceeds to the next step. Step 8: The service requests a token from the OAuth server. \u00b6 This step only occurs if the user granted authorization to the DE. The authorization code that was provided by the OAuth server is sent to the OAuth access token endpoint. This request includes four query parameters: Parameter Description grant_type Always set to \"authorization_code\". code The authorization code provided by the OAuth server. redirect_uri The same redirect URI sent to the authorization endpoint. client_id The API client identifier associated with the DE. If the OAuth server responds with an error status then this service also responds with an error status. Otherwise, the response will contain the access token information. Step 9: The service stores the token information in the database. \u00b6 This step only occurs if the user granted authorization and an access token was successfully obtained. The response from the OAuth server's token endpoint will contain the access token, the number of seconds until the token expires, and a refresh token that can be used to obtain a new access token when the current one expires. This information is stored in the database associated with the API name and the current user. After storing the token information, the service responds with a JSON object containing the state information that was stored in the database. Step 10: The user is redirected back to the main page in the DE. \u00b6 This step occurs whether or not authorization was obtained. If authorization was obtained the state information from the service call is passed to the main DE page in the query string. If authorization was not obtained then some query parameters indicating what went wrong are included in the query string.","title":"Callbacks"},{"location":"services/api/endpoints/callbacks/#callback-endpoints","text":"These endpoints listed in this document accept callbacks from other services indicating that some event has occurred.","title":"Callback Endpoints"},{"location":"services/api/endpoints/callbacks/#obtaining-oauth-authorization-codes","text":"Secured Endpoint: GET /secured/oauth/access-code/{api-name} Delegates to apps: GET /secured/oauth/access-code/{api-name} The only API name that is currently supported is agave . The DE calls this endpoint after the user gives the DE permission to access a third-party API on his or her behalf. Agave sends a callback to a servlet in the DE, which obtains a CAS proxy ticket and sends a request to this service to obtain an access token for the user. This service requires the following query parameters: Parameter Description code The authorization code provided by the OAuth server. state A UUID referring to the original authorization request. Upon success, the response to this service is in the following format: { \"state_info\" : \"{arbitrary-state-information}\" } This service is used any time another service determines that an authorization code is required in order to proceed. The steps are described in detail below:","title":"Obtaining OAuth Authorization Codes"},{"location":"services/api/endpoints/callbacks/#step-1-the-other-service-determines-that-authorization-is-required","text":"This can happen in two different ways: No access token is found in the database for the user. An access token is found, but attempts to use or refresh the token failed.","title":"Step 1: The other service determines that authorization is required."},{"location":"services/api/endpoints/callbacks/#step-2-the-other-service-stores-state-information-in-the-database","text":"This state information is specific to each service, and enables the DE to route the user back to the same window in the DE after the authorization process is complete. The state information is associated with a UUID and the username of the current user. This adds an extra layer of security because it allows this service (the service that accepts the authorization code) to ensure that the user who obtained the authorization code is the same as the user who initiated the request that required it.","title":"Step 2: The other service stores state information in the database."},{"location":"services/api/endpoints/callbacks/#step-3-the-user-is-redirected-to-the-oauth-authorization-service","text":"This redirection contains several query parameters that are either required or recommended by the OAuth 2.0 specification: Parameter Description response_type The type of response requested (always set to \"code\"). client_id The DE's OAuth API client identifier. redirect_uri The URI to redirect the user to after authorization state The UUID associated with the state information from step 2.","title":"Step 3: The user is redirected to the OAuth authorization service."},{"location":"services/api/endpoints/callbacks/#step-4-the-user-either-grants-or-denies-authorization","text":"This step happens outside of the DE. The user authenticates to the external service and either grants or denies the authorization request.","title":"Step 4: The user either grants or denies authorization."},{"location":"services/api/endpoints/callbacks/#step-5-the-user-is-redirected-back-to-the-de","text":"After granting or denying authorization, the user is redirected to the URI specified in the redirect_uri parameter described above. There are two possibilities for this step: either the user granted or denied authorization. If the user granted authorization then two query parameters will be included in the request: Parameter Description code The authorization code provided by the OAuth server. state The same value that was sent to the authorization service. In this case, a request is sent to Terrain so that an access token can be obtained. If the user denied authorization or an error occurred, then the response will contain at least two and, possibly up to four query parameters: Parameter Required error Yes An error code indicating what went wrong. error_description No A human-readable description of the error. error_uri No A link to a human-readable description of the error. state Yes The same value that was sent to the authorization service. In this case, the user is redirected back to the main page in the DE along with some query parameters indicating that authorization was not obtained. This request is handled by a servlet that fields requests to any URI path that begins with the context path followed by oauth/callback/ . There should be one additional path element: the name of the API. For example, assuming that the context path is /de and the API name is agave , the path in the request would be /de/oauth/callback/agave .","title":"Step 5: The user is redirected back to the DE."},{"location":"services/api/endpoints/callbacks/#step-6-a-request-is-sent-to-the-access-code-service","text":"This step only occurs if the user granted authorization to the DE. The servlet handling the redirection in the previous step extracts the API name from the URI path and the authorization code and state identifier from the query string. The API name is used to construct the URI path for the service request. For example, if the API name is agave then the URI path for the service request is /secured/oauth/access-code/agave . The query string parameters are passed to the service in the query string along with the usual CAS proxy token.","title":"Step 6: A request is sent to the access-code service."},{"location":"services/api/endpoints/callbacks/#step-7-the-state-identifier-is-validated","text":"This step only occurs if the user granted authorization to the DE. The /secured/oauth/access-code/{api-name} service obtains the user information using the CAS proxy token and gets the state identifier from the query string. The state information is then retrieved from the database using the state identifier. If the state information isn't found or is associated with a different user, the service responds with an error status. Otherwise, the state information is removed from the database and the serivce proceeds to the next step.","title":"Step 7: The state identifier is validated."},{"location":"services/api/endpoints/callbacks/#step-8-the-service-requests-a-token-from-the-oauth-server","text":"This step only occurs if the user granted authorization to the DE. The authorization code that was provided by the OAuth server is sent to the OAuth access token endpoint. This request includes four query parameters: Parameter Description grant_type Always set to \"authorization_code\". code The authorization code provided by the OAuth server. redirect_uri The same redirect URI sent to the authorization endpoint. client_id The API client identifier associated with the DE. If the OAuth server responds with an error status then this service also responds with an error status. Otherwise, the response will contain the access token information.","title":"Step 8: The service requests a token from the OAuth server."},{"location":"services/api/endpoints/callbacks/#step-9-the-service-stores-the-token-information-in-the-database","text":"This step only occurs if the user granted authorization and an access token was successfully obtained. The response from the OAuth server's token endpoint will contain the access token, the number of seconds until the token expires, and a refresh token that can be used to obtain a new access token when the current one expires. This information is stored in the database associated with the API name and the current user. After storing the token information, the service responds with a JSON object containing the state information that was stored in the database.","title":"Step 9: The service stores the token information in the database."},{"location":"services/api/endpoints/callbacks/#step-10-the-user-is-redirected-back-to-the-main-page-in-the-de","text":"This step occurs whether or not authorization was obtained. If authorization was obtained the state information from the service call is passed to the main DE page in the query string. If authorization was not obtained then some query parameters indicating what went wrong are included in the query string.","title":"Step 10: The user is redirected back to the main page in the DE."},{"location":"services/api/endpoints/comments/","text":"This document describes the comments resource. A comment is something that a user records about a given app, file, or folder. Comment Endpoints \u00b6 Creating a comment \u00b6 POST /apps/{app-id}/comments Delegates to metadata: POST /apps/{app-id}/comments POST /secured/filesystem/entry/{entry-id}/comments Delegates to metadata: POST /filesystem/entry/{entry-id}/comments This endpoint allows an authenticated user to post a comment on any app, accessible file, or accessible folder. {app-id} is the UUID of the app being commented on. {entry-id} is the UUID of the file or folder being commented on. These endpoints forward requests to their corresponding metadata service endpoints. Please see the metadata documentation for more information. Request \u00b6 A request to this endpoint requires no parameters. The user that owns the comment is determined from the authentication. Any query parameters will be ignored. Response \u00b6 Status Code Cause 201 The comment was successfully posted 400 The request body wasn't syntactically correct 401 Either the authentication header was not provided, or its value wasn't correct. 404 From the /apps/{app-id}/comments endpoint, the app-id didn't correspond to an existing app. From the /secured/filesystem/entry/{entry-id}/comments endpoint, entry-id didn't correspond to an existing file or folder, or is not accessible by the user 413 The request body is too large Error responses may include a reason field, providing a short, human readable explanation of the failure. Example \u00b6 ? curl -X POST -H \"$AUTH_HEADER\" -d '{ \"comment\" : \"That was interesting.\" }' localhost/secured/filesystem/entry/f86700ac-df88-11e3-bf3b-6abdce5a08d5/comments { \"comment\" : { \"id\" : \"68a6a1f0-f745-11e3-b125-73dece2a6978\" , \"commenter\" : \"tedgin\" , \"post_time\" : 1403093431927 , \"retracted\" : false , \"comment\" : \"That was interesting.\" } } Listing comments \u00b6 GET /apps/{app-id}/comments Delegates to metadata: GET /apps/{app-id}/comments GET /secured/filesystem/entry/{entry-id}/comments Delegates to metadata: GET /filesystem/entry/{entry-id}/comments This endpoint allows an authenticated user to retrieve all of the comments made on an accessible file or folder, or any app. {app-id} is the UUID associated with the app. entry-id is the UUID associated with the file or folder. These endpoints forward requests to their corresponding metadata service endpoints. Please see the metadata documentation for more information. Request \u00b6 A request to this endpoint requires no parameters. The user that owns the comment is determined from the authentication. Any query parameters will be ignored. Any attached request body will be ignored. Response \u00b6 Status Code Cause 200 The list of comments will be in the response body 401 Either the authentication header was not provided, or its value wasn't correct. 404 From the /apps/{app-id}/comments endpoint, the app-id didn't correspond to an existing app. From the /secured/filesystem/entry/{entry-id}/comments endpoint, entry-id didn't correspond to an existing file or folder, or is not accessible by the user Upon failure, a JSON document with a reason field will the returned. The reason field will provide a short, human readable explanation of the failure. Example \u00b6 ? curl -H \"$AUTH_HEADER\" localhost/secured/filesystem/entry/f86700ac-df88-11e3-bf3b-6abdce5a08d5/comments { \"comments\" : [ { \"id\" : \"68a6a1f0-f745-11e3-b125-73dece2a6978\" , \"commenter\" : \"tedgin\" , \"post_time\" : 1403093431927 , \"retracted\" : false , \"comment\" : \"That was interesting.\" }, { \"id\" : \"79a6a1f0-0745-21e3-c125-73dece2a6989\" , \"commenter\" : \"fool\" , \"post_time\" : 1403093433001 , \"retracted\" : false , \"comment\" : \"No it wasn't.\" } ] } Retracting/Readmitting a comment \u00b6 PATCH /apps/{app-id}/comments/{comment-id} PATCH /secured/filesystem/entry/{entry-id}/comments/{comment-id} Delegates to metadata: PATCH /apps/{app-id}/comments/{comment-id} PATCH /admin/apps/{app-id}/comments/{comment-id} PATCH /filesystem/entry/{entry-id}/comments/{comment-id} PATCH /admin/filesystem/entry/{entry-id}/comments/{comment-id} These endpoints allow an authenticated user to retract a given comment or to readmit a retracted comment on a given app, file, or folder. app-id is the UUID of the app. entry-id is the UUID of the file or folder. comment-id is the UUID of the comment. These endpoints forward requests to their corresponding metadata service endpoints. In the case that the user is the owner of the app, file, or folder, then the request is forwarded to the corresponding metadata service /admin endpoint. Please see the metadata documentation for more information. Request \u00b6 This endpoint requires a boolean retracted parameter. Any body attached to the request will be ignored. Response \u00b6 Status Code Cause 200 The comment corresponding to the comment-id UUID has been marked as retracted. 400 The retracted parameter was missing or had a value other than true or false . 401 Either the authentication header was not provided, or its value wasn't correct. 403 See 403 . 404 comment-id doesn't exist, app-id doesn't exist, or entry-id doesn't exist or isn't accessible by the user. 409 One of the query parameters was passed more than once with different values. Error responses may include a reason field, providing a short, human readable explanation of the failure. 403 \u00b6 When a comment is being retracted, a 403 will result if the app, file, or folder is not owned by the user and the comment was not created by the user. When a comment is being readmitted, a 403 will result if the comment wasn't originally retracted by the user. Example \u00b6 curl -X PATCH -H \"$AUTH_HEADER\" \"localhost/secured/filesystem/f86700ac-df88-11e3-bf3b-6abdce5a08d5/comments/79a6a1f0-0745-21e3-c125-73dece2a6989?retracted=true\" Administratively deleting a comment \u00b6 DELETE /admin/apps/{app-id}/comments/{comment-id} Delegates to metadata: DELETE /admin/apps/{app-id}/comments/{comment-id} DELETE /admin/filesystem/entry/{entry-id}/comments/{comment-id} Delegates to metadata: DELETE /admin/filesystem/entry/{entry-id}/comments/{comment-id} This endpoint allows an administrative user to delete a given comment on a given app, file, or folder. comment-id is the UUID of the comment. app-id is the UUID of the app. entry-id is the UUID of the file or folder. These endpoints forward requests to their corresponding metadata service endpoints. Please see the metadata documentation for more information. Request \u00b6 A request to this endpoint requires no parameters. Any query parameters that are provided will be ignored. Any body attached to the request will be ignored. Response \u00b6 Status Code Cause 200 The comment corresponding to the comment-id UUID has been deleted. 401 Either the authentication header was not provided, or its value wasn't correct. 404 Either the app corresponding to app-id doesn't exist, the file or folder corresponding to entry-id doesn't exist, or the comment corresponding to comment-id doesn't. Error responses may include a reason field, providing a short, human readable explanation of the failure. Example \u00b6 curl -X DELETE -H \"$AUTH_HEADER\" \"localhost/admin/filesystem/entry/f86700ac-df88-11e3-bf3b-6abdce5a08d5/comments/79a6a1f0-0745-21e3-c125-73dece2a6989\"","title":"Comments"},{"location":"services/api/endpoints/comments/#comment-endpoints","text":"","title":"Comment Endpoints"},{"location":"services/api/endpoints/comments/#creating-a-comment","text":"POST /apps/{app-id}/comments Delegates to metadata: POST /apps/{app-id}/comments POST /secured/filesystem/entry/{entry-id}/comments Delegates to metadata: POST /filesystem/entry/{entry-id}/comments This endpoint allows an authenticated user to post a comment on any app, accessible file, or accessible folder. {app-id} is the UUID of the app being commented on. {entry-id} is the UUID of the file or folder being commented on. These endpoints forward requests to their corresponding metadata service endpoints. Please see the metadata documentation for more information.","title":"Creating a comment"},{"location":"services/api/endpoints/comments/#request","text":"A request to this endpoint requires no parameters. The user that owns the comment is determined from the authentication. Any query parameters will be ignored.","title":"Request"},{"location":"services/api/endpoints/comments/#response","text":"Status Code Cause 201 The comment was successfully posted 400 The request body wasn't syntactically correct 401 Either the authentication header was not provided, or its value wasn't correct. 404 From the /apps/{app-id}/comments endpoint, the app-id didn't correspond to an existing app. From the /secured/filesystem/entry/{entry-id}/comments endpoint, entry-id didn't correspond to an existing file or folder, or is not accessible by the user 413 The request body is too large Error responses may include a reason field, providing a short, human readable explanation of the failure.","title":"Response"},{"location":"services/api/endpoints/comments/#example","text":"? curl -X POST -H \"$AUTH_HEADER\" -d '{ \"comment\" : \"That was interesting.\" }' localhost/secured/filesystem/entry/f86700ac-df88-11e3-bf3b-6abdce5a08d5/comments { \"comment\" : { \"id\" : \"68a6a1f0-f745-11e3-b125-73dece2a6978\" , \"commenter\" : \"tedgin\" , \"post_time\" : 1403093431927 , \"retracted\" : false , \"comment\" : \"That was interesting.\" } }","title":"Example"},{"location":"services/api/endpoints/comments/#listing-comments","text":"GET /apps/{app-id}/comments Delegates to metadata: GET /apps/{app-id}/comments GET /secured/filesystem/entry/{entry-id}/comments Delegates to metadata: GET /filesystem/entry/{entry-id}/comments This endpoint allows an authenticated user to retrieve all of the comments made on an accessible file or folder, or any app. {app-id} is the UUID associated with the app. entry-id is the UUID associated with the file or folder. These endpoints forward requests to their corresponding metadata service endpoints. Please see the metadata documentation for more information.","title":"Listing comments"},{"location":"services/api/endpoints/comments/#request_1","text":"A request to this endpoint requires no parameters. The user that owns the comment is determined from the authentication. Any query parameters will be ignored. Any attached request body will be ignored.","title":"Request"},{"location":"services/api/endpoints/comments/#response_1","text":"Status Code Cause 200 The list of comments will be in the response body 401 Either the authentication header was not provided, or its value wasn't correct. 404 From the /apps/{app-id}/comments endpoint, the app-id didn't correspond to an existing app. From the /secured/filesystem/entry/{entry-id}/comments endpoint, entry-id didn't correspond to an existing file or folder, or is not accessible by the user Upon failure, a JSON document with a reason field will the returned. The reason field will provide a short, human readable explanation of the failure.","title":"Response"},{"location":"services/api/endpoints/comments/#example_1","text":"? curl -H \"$AUTH_HEADER\" localhost/secured/filesystem/entry/f86700ac-df88-11e3-bf3b-6abdce5a08d5/comments { \"comments\" : [ { \"id\" : \"68a6a1f0-f745-11e3-b125-73dece2a6978\" , \"commenter\" : \"tedgin\" , \"post_time\" : 1403093431927 , \"retracted\" : false , \"comment\" : \"That was interesting.\" }, { \"id\" : \"79a6a1f0-0745-21e3-c125-73dece2a6989\" , \"commenter\" : \"fool\" , \"post_time\" : 1403093433001 , \"retracted\" : false , \"comment\" : \"No it wasn't.\" } ] }","title":"Example"},{"location":"services/api/endpoints/comments/#retractingreadmitting-a-comment","text":"PATCH /apps/{app-id}/comments/{comment-id} PATCH /secured/filesystem/entry/{entry-id}/comments/{comment-id} Delegates to metadata: PATCH /apps/{app-id}/comments/{comment-id} PATCH /admin/apps/{app-id}/comments/{comment-id} PATCH /filesystem/entry/{entry-id}/comments/{comment-id} PATCH /admin/filesystem/entry/{entry-id}/comments/{comment-id} These endpoints allow an authenticated user to retract a given comment or to readmit a retracted comment on a given app, file, or folder. app-id is the UUID of the app. entry-id is the UUID of the file or folder. comment-id is the UUID of the comment. These endpoints forward requests to their corresponding metadata service endpoints. In the case that the user is the owner of the app, file, or folder, then the request is forwarded to the corresponding metadata service /admin endpoint. Please see the metadata documentation for more information.","title":"Retracting/Readmitting a comment"},{"location":"services/api/endpoints/comments/#request_2","text":"This endpoint requires a boolean retracted parameter. Any body attached to the request will be ignored.","title":"Request"},{"location":"services/api/endpoints/comments/#response_2","text":"Status Code Cause 200 The comment corresponding to the comment-id UUID has been marked as retracted. 400 The retracted parameter was missing or had a value other than true or false . 401 Either the authentication header was not provided, or its value wasn't correct. 403 See 403 . 404 comment-id doesn't exist, app-id doesn't exist, or entry-id doesn't exist or isn't accessible by the user. 409 One of the query parameters was passed more than once with different values. Error responses may include a reason field, providing a short, human readable explanation of the failure.","title":"Response"},{"location":"services/api/endpoints/comments/#403","text":"When a comment is being retracted, a 403 will result if the app, file, or folder is not owned by the user and the comment was not created by the user. When a comment is being readmitted, a 403 will result if the comment wasn't originally retracted by the user.","title":"403"},{"location":"services/api/endpoints/comments/#example_2","text":"curl -X PATCH -H \"$AUTH_HEADER\" \"localhost/secured/filesystem/f86700ac-df88-11e3-bf3b-6abdce5a08d5/comments/79a6a1f0-0745-21e3-c125-73dece2a6989?retracted=true\"","title":"Example"},{"location":"services/api/endpoints/comments/#administratively-deleting-a-comment","text":"DELETE /admin/apps/{app-id}/comments/{comment-id} Delegates to metadata: DELETE /admin/apps/{app-id}/comments/{comment-id} DELETE /admin/filesystem/entry/{entry-id}/comments/{comment-id} Delegates to metadata: DELETE /admin/filesystem/entry/{entry-id}/comments/{comment-id} This endpoint allows an administrative user to delete a given comment on a given app, file, or folder. comment-id is the UUID of the comment. app-id is the UUID of the app. entry-id is the UUID of the file or folder. These endpoints forward requests to their corresponding metadata service endpoints. Please see the metadata documentation for more information.","title":"Administratively deleting a comment"},{"location":"services/api/endpoints/comments/#request_3","text":"A request to this endpoint requires no parameters. Any query parameters that are provided will be ignored. Any body attached to the request will be ignored.","title":"Request"},{"location":"services/api/endpoints/comments/#response_3","text":"Status Code Cause 200 The comment corresponding to the comment-id UUID has been deleted. 401 Either the authentication header was not provided, or its value wasn't correct. 404 Either the app corresponding to app-id doesn't exist, the file or folder corresponding to entry-id doesn't exist, or the comment corresponding to comment-id doesn't. Error responses may include a reason field, providing a short, human readable explanation of the failure.","title":"Response"},{"location":"services/api/endpoints/comments/#example_3","text":"curl -X DELETE -H \"$AUTH_HEADER\" \"localhost/admin/filesystem/entry/f86700ac-df88-11e3-bf3b-6abdce5a08d5/comments/79a6a1f0-0745-21e3-c125-73dece2a6989\"","title":"Example"},{"location":"services/api/endpoints/endpoints/","text":"Terrain Endpoints \u00b6 All URLs referenced in endpoint documentation are listed as relative URLs with value names enclosed in braces. For example, the service to get a list of workflow elements is accessed using the URL, /get-workflow-elements/{element-type} . Where {element-type} refers to the type of workflow element that is being retrieved. For example, to get a list of known property types, you can access the URL, /get-workflow-elements/property-types . On the other hand, all examples use fully qualified URLs. Request and response bodies are in JSON format unless otherwise noted. App Metadata Callback Categorizing Apps with Ontology Hierarchies Comments Data Search Favorites Notification Permanent ID Requests Quick Launches Miscellaneous","title":"Overview"},{"location":"services/api/endpoints/endpoints/#terrain-endpoints","text":"All URLs referenced in endpoint documentation are listed as relative URLs with value names enclosed in braces. For example, the service to get a list of workflow elements is accessed using the URL, /get-workflow-elements/{element-type} . Where {element-type} refers to the type of workflow element that is being retrieved. For example, to get a list of known property types, you can access the URL, /get-workflow-elements/property-types . On the other hand, all examples use fully qualified URLs. Request and response bodies are in JSON format unless otherwise noted. App Metadata Callback Categorizing Apps with Ontology Hierarchies Comments Data Search Favorites Notification Permanent ID Requests Quick Launches Miscellaneous","title":"Terrain Endpoints"},{"location":"services/api/endpoints/favorites/","text":"This document describes the favorites resource. A favorite is something that a user has decided is important enough that it should be more readily accessible than other resources of the same time. Resources \u00b6 Favorite \u00b6 A favorite is modeled as a UUID having the same value has the important resource's UUID. Example f86700ac-df88-11e3-bf3b-6abdce5a08d1 Data Id Collection \u00b6 A collection of data Ids is its own resource. It is a JSON document (media type application/json ) with a single filesystem field that holds an array of UUIDs, each for a file or folder. Example { \"filesystem\" : [ \"f86700ac-df88-11e3-bf3b-6abdce5a08d1\" , \"9cdb9492-f288-11e3-ab3e-13be1320bb50\" ] } Favorite Data Collection \u00b6 A collection of favorite data or filesystem entities is its own resource. It is a JSON document (media type application/json ) with the following fields. Field Type Description files array an array of favorite file objects folders array an array of favorite folder objects total number the total number of favorite files and folders for a given authenticated user Favorite File Object \u00b6 A favorite file object has the following fields. Field Type Description date-created number the time when the file was created in ms since the POSIX epoch date-modified number the time when the file was last modified in ms since the POSIX epoch file-size number the size in bytes of the file id string the absolute path to the file infoType string the semantic type of the content of the file isFavorite boolean true label string the name of file relative to its containing folder mime-type string the media type of the file path string the absolute path to the file permission string the permission the authenticated user has on the file, \"read\"|\"write\"|\"own\" share-count number the number of other users this file has been shared with type string \"file\" uuid string the file's UUID Favorite Folder Object \u00b6 A favorite folder object has the following fields. Field Type Description date-created number the time when the folder was created in ms since the POSIX epoch date-modified number the time when the folder was last modified (i.e. had a member file or folder added or removed) in ms since the POSIX epoch dir-count number the number of member folders file-count number the number of member files file-size number 0 id string the absolute path to the folder isFavorite boolean true label string the name of folder relative to its containing folder path string the absolute path to the folder permission string the permission the authenticated user has on the folder, \"read\"|\"write\"|\"own\" share-count number the number of other users this folder has been shared with type string \"dir\" uuid string the folder's UUID Endpoints \u00b6 Marking a Data Resource as Favorite \u00b6 PUT /secured/favorites/filesystem/{favorite} Delegates to metadata: PUT /favorites/filesystem/{favorite} This endpoint marks a given file or folder a favorite of the authenticated user. {favorite} is the UUID of the file or folder being marked. This endpoint forwards requests to the corresponding metadata service endpoint. Please see the metadata documentation for more information. Request \u00b6 A request to this endpoint requires no parameters. The user that owns the favorite is determined from the authentication. Any query parameters will be ignored. Any body attached to the request will be ignored. Response \u00b6 Status Code Cause 200 The file or folder corresponding to the <favorite> UUID has been marked as a favorite of the authenticated user. 401 Either the authentication header was not provided, or its value wasn't correct. 404 The {favorite} UUID doesn't belong to a known file or folder or the file or folder isn't readable by the authenticated user. Error responses may contain a \"reason\" field, providing a short, human readable explanation of the failure. Example \u00b6 ? curl -XPUT -H \"$AUTH_HEADER\" localhost/secured/favorites/filesystem/f86700ac-df88-11e3-bf3b-6abdce5a08d1 Removing a Data Resource from Being a Favorite \u00b6 DELETE /secured/favorites/filesystem/{favorite} Delegates to metadata: DELETE /favorites/filesystem/{favorite} This endpoint removes a given file or folder from the authenticated user's favorites. <favorite> is the UUID of the file or folder. This endpoint forwards requests to the corresponding metadata service endpoint. Please see the metadata documentation for more information. Request \u00b6 A request to this endpoint requires no parameters. The user that owns the favorite is determined from the authentication. Any query parameters will be ignored. Any body attached to the request will be ignored. Response \u00b6 Status Code Cause 200 The file or folder corresponding to the favorite UUID has been marked as a favorite of the authenticated user. 401 Either the authentication header was not provided, or its value wasn't correct. 404 The file or folder corresponding to the favorite UUID wasn't marked as a favorite. Error responses may include a reason field, providing a short, human readable explanation of the failure. Example \u00b6 ? curl -H \"$AUTH_HEADER\" -XDELETE localhost/secured/favorites/filesystem/f86700ac-df88-11e3-bf3b-6abdce5a08d1 Listing Stat Info for Favorite Data \u00b6 GET /secured/favorites/filesystem Requires metadata endpoint: GET /favorites/filesystem This endpoint lists stat information for the authenticated user's favorite files and folders. Only files and folders accessible to the user will be listed. The result set is paged. This endpoint fetches favorite information from the corresponding metadata service endpoint before applying filters, sorting, and additional stat information. Please see the metadata documentation for more information. Request \u00b6 A request to this endpoint requires the parameters in the following table. Parameter Required? Default Description sort-col yes the field used to sort the filesystem entries in the result set. This can be NAME|ID|LASTMODIFIED|DATECREATED|SIZE . All values are case insensitive. sort-dir yes the sorting direction. It can be ASC|DESC . Both values are case insensitive. limit yes the maximum number of filesystem entries to return offset yes the number entries in the sorted total result set to skip before including entries in the response document. entity-type no ANY the type of entities to return. It can be ANY|FILE|FOLDER . All values are case-insensitive. info-type no no filtering filter the files portion of the result set so that only files with this info type are returned. To return multiple info types, and this parameter more than once. Any additional parameters will be ignored. Any body attached to the request will be ignored. Response \u00b6 Status Code Cause 200 The list of stat info was obtained and is included in the response body. 400 one of the parameters was missing or had a nonsensical value. 401 Either the authentication header was not provided, or its value wasn't correct. Upon success, the response body will be a data collection JSON document containing the stat information of the favorite files and folders. Error responses may include a reason field, providing a short, human readable explanation of the failure. Example \u00b6 ? curl -H \"$AUTH_HEADER\" \"localhost/secured/favorites/filesystem/favorites?sort-col=ID&sort-dir=ASC&limit=1&offset=0\" { \"filesystem\" : { \"files\" : [], \"folders\" : [ { \"date-created\" : 1.397233899e+12 , \"date-modified\" : 1.397233899e+12 , \"dir-count\" : 256 , \"file-count\" : 0 , \"id\" : \"/iplant/home/wregglej/analyses\" , \"isFavorite\" : true , \"label\" : \"analyses\" , \"path\" : \"/iplant/home/wregglej/analyses\" , \"permission\" : \"own\" , \"share-count\" : 1 , \"type\" : \"dir\" , \"uuid\" : \"0d880c78-df8a-11e3-bfa5-6abdce5a08d5\" } ], \"total\" : 3 } } Filter a Set of Resources for Favorites \u00b6 POST /secured/favorites/filter Delegates to metadata: POST /favorites/filter This endpoint forwards requests to the corresponding metadata service endpoint. Please see the metadata documentation for more information. This endpoint fetches the filtered favorite IDs from the corresponding metadata service endpoint before further filtering only UUIDs for files and folders accessible to the user. Request \u00b6 A request to this endpoint requires no parameters. The user that owns the favorite is determined from the authentication. Any query parameters will be ignored. Response \u00b6 Status Code Cause 200 The filtered list of UUIDs was generated and is included in the response body. 400 The request body wasn't a syntactically correct data id collection . 401 Either the authentication header was not provided, or its value wasn't correct. Upon success, the response body will be a data id collection JSON document containing the UUIDs from the request body that correspond to favorite files and folders of the user. Error responses may include a reason field, providing a short, human readable explanation of the failure. Example \u00b6 ? curl -XPOST -H \"$AUTH_HEADER\" -d ' { \"filesystem\" : [ \"f86700ac-df88-11e3-bf3b-6abdce5a08d1\", \"f86700ac-df88-11e3-bf3b-6abdce5a08d5\" ] }' localhost/secured/favorites/filter { \"filesystem\" : [ \"f86700ac-df88-11e3-bf3b-6abdce5a08d1\" ] }","title":"Favorites"},{"location":"services/api/endpoints/favorites/#resources","text":"","title":"Resources"},{"location":"services/api/endpoints/favorites/#favorite","text":"A favorite is modeled as a UUID having the same value has the important resource's UUID. Example f86700ac-df88-11e3-bf3b-6abdce5a08d1","title":"Favorite"},{"location":"services/api/endpoints/favorites/#data-id-collection","text":"A collection of data Ids is its own resource. It is a JSON document (media type application/json ) with a single filesystem field that holds an array of UUIDs, each for a file or folder. Example { \"filesystem\" : [ \"f86700ac-df88-11e3-bf3b-6abdce5a08d1\" , \"9cdb9492-f288-11e3-ab3e-13be1320bb50\" ] }","title":"Data Id Collection"},{"location":"services/api/endpoints/favorites/#favorite-data-collection","text":"A collection of favorite data or filesystem entities is its own resource. It is a JSON document (media type application/json ) with the following fields. Field Type Description files array an array of favorite file objects folders array an array of favorite folder objects total number the total number of favorite files and folders for a given authenticated user","title":"Favorite Data Collection"},{"location":"services/api/endpoints/favorites/#favorite-file-object","text":"A favorite file object has the following fields. Field Type Description date-created number the time when the file was created in ms since the POSIX epoch date-modified number the time when the file was last modified in ms since the POSIX epoch file-size number the size in bytes of the file id string the absolute path to the file infoType string the semantic type of the content of the file isFavorite boolean true label string the name of file relative to its containing folder mime-type string the media type of the file path string the absolute path to the file permission string the permission the authenticated user has on the file, \"read\"|\"write\"|\"own\" share-count number the number of other users this file has been shared with type string \"file\" uuid string the file's UUID","title":"Favorite File Object"},{"location":"services/api/endpoints/favorites/#favorite-folder-object","text":"A favorite folder object has the following fields. Field Type Description date-created number the time when the folder was created in ms since the POSIX epoch date-modified number the time when the folder was last modified (i.e. had a member file or folder added or removed) in ms since the POSIX epoch dir-count number the number of member folders file-count number the number of member files file-size number 0 id string the absolute path to the folder isFavorite boolean true label string the name of folder relative to its containing folder path string the absolute path to the folder permission string the permission the authenticated user has on the folder, \"read\"|\"write\"|\"own\" share-count number the number of other users this folder has been shared with type string \"dir\" uuid string the folder's UUID","title":"Favorite Folder Object"},{"location":"services/api/endpoints/favorites/#endpoints","text":"","title":"Endpoints"},{"location":"services/api/endpoints/favorites/#marking-a-data-resource-as-favorite","text":"PUT /secured/favorites/filesystem/{favorite} Delegates to metadata: PUT /favorites/filesystem/{favorite} This endpoint marks a given file or folder a favorite of the authenticated user. {favorite} is the UUID of the file or folder being marked. This endpoint forwards requests to the corresponding metadata service endpoint. Please see the metadata documentation for more information.","title":"Marking a Data Resource as Favorite"},{"location":"services/api/endpoints/favorites/#request","text":"A request to this endpoint requires no parameters. The user that owns the favorite is determined from the authentication. Any query parameters will be ignored. Any body attached to the request will be ignored.","title":"Request"},{"location":"services/api/endpoints/favorites/#response","text":"Status Code Cause 200 The file or folder corresponding to the <favorite> UUID has been marked as a favorite of the authenticated user. 401 Either the authentication header was not provided, or its value wasn't correct. 404 The {favorite} UUID doesn't belong to a known file or folder or the file or folder isn't readable by the authenticated user. Error responses may contain a \"reason\" field, providing a short, human readable explanation of the failure.","title":"Response"},{"location":"services/api/endpoints/favorites/#example","text":"? curl -XPUT -H \"$AUTH_HEADER\" localhost/secured/favorites/filesystem/f86700ac-df88-11e3-bf3b-6abdce5a08d1","title":"Example"},{"location":"services/api/endpoints/favorites/#removing-a-data-resource-from-being-a-favorite","text":"DELETE /secured/favorites/filesystem/{favorite} Delegates to metadata: DELETE /favorites/filesystem/{favorite} This endpoint removes a given file or folder from the authenticated user's favorites. <favorite> is the UUID of the file or folder. This endpoint forwards requests to the corresponding metadata service endpoint. Please see the metadata documentation for more information.","title":"Removing a Data Resource from Being a Favorite"},{"location":"services/api/endpoints/favorites/#request_1","text":"A request to this endpoint requires no parameters. The user that owns the favorite is determined from the authentication. Any query parameters will be ignored. Any body attached to the request will be ignored.","title":"Request"},{"location":"services/api/endpoints/favorites/#response_1","text":"Status Code Cause 200 The file or folder corresponding to the favorite UUID has been marked as a favorite of the authenticated user. 401 Either the authentication header was not provided, or its value wasn't correct. 404 The file or folder corresponding to the favorite UUID wasn't marked as a favorite. Error responses may include a reason field, providing a short, human readable explanation of the failure.","title":"Response"},{"location":"services/api/endpoints/favorites/#example_1","text":"? curl -H \"$AUTH_HEADER\" -XDELETE localhost/secured/favorites/filesystem/f86700ac-df88-11e3-bf3b-6abdce5a08d1","title":"Example"},{"location":"services/api/endpoints/favorites/#listing-stat-info-for-favorite-data","text":"GET /secured/favorites/filesystem Requires metadata endpoint: GET /favorites/filesystem This endpoint lists stat information for the authenticated user's favorite files and folders. Only files and folders accessible to the user will be listed. The result set is paged. This endpoint fetches favorite information from the corresponding metadata service endpoint before applying filters, sorting, and additional stat information. Please see the metadata documentation for more information.","title":"Listing Stat Info for Favorite Data"},{"location":"services/api/endpoints/favorites/#request_2","text":"A request to this endpoint requires the parameters in the following table. Parameter Required? Default Description sort-col yes the field used to sort the filesystem entries in the result set. This can be NAME|ID|LASTMODIFIED|DATECREATED|SIZE . All values are case insensitive. sort-dir yes the sorting direction. It can be ASC|DESC . Both values are case insensitive. limit yes the maximum number of filesystem entries to return offset yes the number entries in the sorted total result set to skip before including entries in the response document. entity-type no ANY the type of entities to return. It can be ANY|FILE|FOLDER . All values are case-insensitive. info-type no no filtering filter the files portion of the result set so that only files with this info type are returned. To return multiple info types, and this parameter more than once. Any additional parameters will be ignored. Any body attached to the request will be ignored.","title":"Request"},{"location":"services/api/endpoints/favorites/#response_2","text":"Status Code Cause 200 The list of stat info was obtained and is included in the response body. 400 one of the parameters was missing or had a nonsensical value. 401 Either the authentication header was not provided, or its value wasn't correct. Upon success, the response body will be a data collection JSON document containing the stat information of the favorite files and folders. Error responses may include a reason field, providing a short, human readable explanation of the failure.","title":"Response"},{"location":"services/api/endpoints/favorites/#example_2","text":"? curl -H \"$AUTH_HEADER\" \"localhost/secured/favorites/filesystem/favorites?sort-col=ID&sort-dir=ASC&limit=1&offset=0\" { \"filesystem\" : { \"files\" : [], \"folders\" : [ { \"date-created\" : 1.397233899e+12 , \"date-modified\" : 1.397233899e+12 , \"dir-count\" : 256 , \"file-count\" : 0 , \"id\" : \"/iplant/home/wregglej/analyses\" , \"isFavorite\" : true , \"label\" : \"analyses\" , \"path\" : \"/iplant/home/wregglej/analyses\" , \"permission\" : \"own\" , \"share-count\" : 1 , \"type\" : \"dir\" , \"uuid\" : \"0d880c78-df8a-11e3-bfa5-6abdce5a08d5\" } ], \"total\" : 3 } }","title":"Example"},{"location":"services/api/endpoints/favorites/#filter-a-set-of-resources-for-favorites","text":"POST /secured/favorites/filter Delegates to metadata: POST /favorites/filter This endpoint forwards requests to the corresponding metadata service endpoint. Please see the metadata documentation for more information. This endpoint fetches the filtered favorite IDs from the corresponding metadata service endpoint before further filtering only UUIDs for files and folders accessible to the user.","title":"Filter a Set of Resources for Favorites"},{"location":"services/api/endpoints/favorites/#request_3","text":"A request to this endpoint requires no parameters. The user that owns the favorite is determined from the authentication. Any query parameters will be ignored.","title":"Request"},{"location":"services/api/endpoints/favorites/#response_3","text":"Status Code Cause 200 The filtered list of UUIDs was generated and is included in the response body. 400 The request body wasn't a syntactically correct data id collection . 401 Either the authentication header was not provided, or its value wasn't correct. Upon success, the response body will be a data id collection JSON document containing the UUIDs from the request body that correspond to favorite files and folders of the user. Error responses may include a reason field, providing a short, human readable explanation of the failure.","title":"Response"},{"location":"services/api/endpoints/favorites/#example_3","text":"? curl -XPOST -H \"$AUTH_HEADER\" -d ' { \"filesystem\" : [ \"f86700ac-df88-11e3-bf3b-6abdce5a08d1\", \"f86700ac-df88-11e3-bf3b-6abdce5a08d5\" ] }' localhost/secured/favorites/filter { \"filesystem\" : [ \"f86700ac-df88-11e3-bf3b-6abdce5a08d1\" ] }","title":"Example"},{"location":"services/api/endpoints/fileio/","text":"File IO \u00b6 Provides a REST-like API for uploading and downloading files to and from iRODS. Error handling \u00b6 If you try to hit an endpoint that doesn't exist, you'll get a 404. For all other errors, you should receive a 500 HTTP status code and a JSON body in the following format: { \"error_code\" : \"<Scruffian error code>\" } Most errors will return other contextual fields, but they will vary from error to error. For programmatic usage, only depend on the three fields listed above. Each section listed below lists the error codes that you may encounter. In addition to these, you may run into the ERR_UNCHECKED_EXCEPTION, which means that an uncaught exception was encountered. Downloading \u00b6 URL Path : /secured/fileio/download HTTP Method : GET URL Path : /secured/filesystem/display-download HTTP Method : GET Request Query Parameters : path - The path to the file in iRODS to be downloaded. Error Codes : ERR_INVALID_JSON (wrong content-type or JSON syntax errors) ERR_BAD_OR_MISSING_FIELD (JSON field is missing or has an invalid value) ERR_MISSING_QUERY_PARAMETER (Query parameter is missing) ERR_NOT_A_USER (invalid user specified) ERR_DOES_NOT_EXIST (File request doesn't exist) ERR_NOT_READABLE (File requested isn't readable by the specified user) Curl Command : curl -H \"$AUTH_HEADER\" 'http://127.0.0.1:31370/secured/fileio/download?path=/iplant/home/testuser/myfile.txt' This will result is the file contents being printed out to stdout. Redirect to a file to actually get the file. When using fileio/download, it's always downloaded as an attachment and the content-type is always application/octet-stream. When using display-download, attachment is a parameter that can be passed along. These endpoints delegate to data-info's GET /data/path/:zone/:path endpoint, with some processing. Uploading \u00b6 URL Path : /secured/fileio/upload HTTP Method : POST Request Query Parameters : dest - The path to the iRODS folder where the file(s) will be uploaded. Request Form Fields : file - The contents of the file to be uploaded. Error Codes : ERR_MISSING_FORM_FIELD (One of the form data fields is missing) ERR_MISSING_QUERY_PARAMETER (Query parameter is missing) ERR_NOT_A_USER (Invalid user specified) ERR_DOES_NOT_EXIST (Destination directory doesn't exist) ERR_NOT_WRITEABLE (Destination directory isn't writeable) Response Body : A success will return JSON like this: { \"file\" : { \"id\" : \"<path to the file>\" , \"path\" : \"<path to the file>\" , \"label\" : \"<basename of the file path>\" , \"permission\" : \"own\" , \"date-created\" : <seco n ds si n ce t he epoch> , \"date-modified\" : <seco n ds si n ce t he epoch> , \"file-size\" : <size i n by tes > } } Curl Command : Uploading is handled through multipart requests: curl -H \"$AUTH_HEADER\" -F file=@testfile.txt \"localhost:31325/secured/fileio/upload?dest=/iplant/home/testuser\" Notice that the dest value points to a directory and not a file. This endpoint delegates to data-info's POST /data endpoint. URL Uploads \u00b6 URL Path : /secured/fileio/urlupload HTTP Method : POST Error codes : ERR_INVALID_JSON (Missing content-type or JSON syntax error) ERR_BAD_OR_MISSING_FIELD (Missing JSON field or invalid JSON field value) ERR_MISSING_QUERY_PARAMETER (One of the query parameters is missing) ERR_NOT_A_USER (Invalid user specified) ERR_NOT_WRITEABLE (Destination directory isn't writeable by the specified user) ERR_EXISTS (Destination file already exists) ERR_REQUEST_FAILED (General failure to spawn upload thread) ERR_INVALID_URL (URL that was passed in couldn't be used) Request Query Parameters : Request Body : { \"dest\" : \"/iplant/home/testuser/\", \"address\" : \"http://www.google.com/index.html\" } Response Body : On success you should get JSON that looks like this: { \"msg\" : \"Upload scheduled.\", \"url\" : \"<URL>\", \"label\" : \"<URL base filename>\", \"dest\" : \"<destination in irods>\" } On on error, you'll either get a stacktrace or JSON that looks like this: { \"msg\" : \"<JSON passed in through the request>\", \"error_code\" : \"ERR_REQUEST_FAILED\" } If the URL passed in is incorrect, then the error message will look like this: { \"error_code\" : \"ERR_INVALID_URL\", \"url\" : \"<URL Passed in>\" } Curl Command : curl -H \"$AUTH_HEADER\" -d '{\"dest\" : \"/iplant/home/testuser/\", \"address\" : \"http://www.google.com/index.html\"}' http://127.0.0.1:31370/secured/fileio/urlupload The 'dest' value in the JSON refers to the path to the directory in iRODS that the file will be saved off to. The filename of the file will be extracted from the path portion of the URL. Save \u00b6 URL Path : /secured/fileio/save HTTP Method : POST Error Codes : ERR_INVALID_JSON (Missing content-type or JSON syntax error) ERR_BAD_OR_MISSING_FIELD (Missing JSON field or invalid JSON field value) ERR_NOT_A_USER (Invalid user specified) ERR_DOES_NOT_EXIST (The destination directory does not exist) ERR_NOT_WRITEABLE (The destination directory is not writable by the user) ERR_FILE_SIZE_TOO_LARGE (The size of the \"content\" field is larger than the terrain.fileio.max-edit-file-size config setting) Request Body : { \"content\" : \"This is the content for the file.\" , \"dest\" : \"/iplant/home/testuser/savedfile.txt\" } Response Body : { \"file\" : { \"id\" : \"<path to the file>\" , \"path\" : \"<path to the file>\" , \"label\" : \"<basename of the file path>\" , \"permission\" : \"own\" , \"date-created\" : <seco n ds si n ce t he epoch> , \"date-modified\" : <seco n ds si n ce t he epoch> , \"file-size\" : <size i n by tes > } } Curl Command : curl -H \"$AUTH_HEADER\" -d '{\"dest\" : \"/iplant/home/testuser/savedfile.txt\", \"content\" : \"This is the content for the file.\"}' 'http://127.0.0.1:31370/secured/fileio/save' Save As \u00b6 URL Path : /secured/fileio/saveas HTTP Method : POST Error Codes : ERR_INVALID_JSON (Missing content-type or JSON syntax error) ERR_BAD_OR_MISSING_FIELD (Missing JSON field or invalid JSON field value) ERR_NOT_A_USER (Invalid user specified) ERR_DOES_NOT_EXIST (The destination directory does not exist) ERR_NOT_WRITEABLE (The destination directory is not writable by the user) ERR_EXISTS (The destination file already exists) Request Body : { \"content\" : \"This is the content for the file.\" , \"dest\" : \"/iplant/home/testuser/savedfile.txt\" } Response Body : { \"file\" : { \"id\" : \"<path to the file>\" , \"path\" : \"<path to the file>\" , \"label\" : \"<basename of the file path>\" , \"permission\" : \"own\" , \"date-created\" : <seco n ds si n ce t he epoch> , \"date-modified\" : <seco n ds si n ce t he epoch> , \"file-size\" : <size i n by tes > } } Curl Command : curl -H \"$AUTH_HEADER\" -d '{\"content\" : \"This is the content for the file.\", \"dest\" : \"/iplant/home/testuser/savedfile.txt\"}' 'http://127.0.0.1:31370/secured/fileio/saveas' This endpoint delegates to data-info's POST /data endpoint, similarly to the upload endpoint, with some processing.","title":"File IO"},{"location":"services/api/endpoints/fileio/#file-io","text":"Provides a REST-like API for uploading and downloading files to and from iRODS.","title":"File IO"},{"location":"services/api/endpoints/fileio/#error-handling","text":"If you try to hit an endpoint that doesn't exist, you'll get a 404. For all other errors, you should receive a 500 HTTP status code and a JSON body in the following format: { \"error_code\" : \"<Scruffian error code>\" } Most errors will return other contextual fields, but they will vary from error to error. For programmatic usage, only depend on the three fields listed above. Each section listed below lists the error codes that you may encounter. In addition to these, you may run into the ERR_UNCHECKED_EXCEPTION, which means that an uncaught exception was encountered.","title":"Error handling"},{"location":"services/api/endpoints/fileio/#downloading","text":"URL Path : /secured/fileio/download HTTP Method : GET URL Path : /secured/filesystem/display-download HTTP Method : GET Request Query Parameters : path - The path to the file in iRODS to be downloaded. Error Codes : ERR_INVALID_JSON (wrong content-type or JSON syntax errors) ERR_BAD_OR_MISSING_FIELD (JSON field is missing or has an invalid value) ERR_MISSING_QUERY_PARAMETER (Query parameter is missing) ERR_NOT_A_USER (invalid user specified) ERR_DOES_NOT_EXIST (File request doesn't exist) ERR_NOT_READABLE (File requested isn't readable by the specified user) Curl Command : curl -H \"$AUTH_HEADER\" 'http://127.0.0.1:31370/secured/fileio/download?path=/iplant/home/testuser/myfile.txt' This will result is the file contents being printed out to stdout. Redirect to a file to actually get the file. When using fileio/download, it's always downloaded as an attachment and the content-type is always application/octet-stream. When using display-download, attachment is a parameter that can be passed along. These endpoints delegate to data-info's GET /data/path/:zone/:path endpoint, with some processing.","title":"Downloading"},{"location":"services/api/endpoints/fileio/#uploading","text":"URL Path : /secured/fileio/upload HTTP Method : POST Request Query Parameters : dest - The path to the iRODS folder where the file(s) will be uploaded. Request Form Fields : file - The contents of the file to be uploaded. Error Codes : ERR_MISSING_FORM_FIELD (One of the form data fields is missing) ERR_MISSING_QUERY_PARAMETER (Query parameter is missing) ERR_NOT_A_USER (Invalid user specified) ERR_DOES_NOT_EXIST (Destination directory doesn't exist) ERR_NOT_WRITEABLE (Destination directory isn't writeable) Response Body : A success will return JSON like this: { \"file\" : { \"id\" : \"<path to the file>\" , \"path\" : \"<path to the file>\" , \"label\" : \"<basename of the file path>\" , \"permission\" : \"own\" , \"date-created\" : <seco n ds si n ce t he epoch> , \"date-modified\" : <seco n ds si n ce t he epoch> , \"file-size\" : <size i n by tes > } } Curl Command : Uploading is handled through multipart requests: curl -H \"$AUTH_HEADER\" -F file=@testfile.txt \"localhost:31325/secured/fileio/upload?dest=/iplant/home/testuser\" Notice that the dest value points to a directory and not a file. This endpoint delegates to data-info's POST /data endpoint.","title":"Uploading"},{"location":"services/api/endpoints/fileio/#url-uploads","text":"URL Path : /secured/fileio/urlupload HTTP Method : POST Error codes : ERR_INVALID_JSON (Missing content-type or JSON syntax error) ERR_BAD_OR_MISSING_FIELD (Missing JSON field or invalid JSON field value) ERR_MISSING_QUERY_PARAMETER (One of the query parameters is missing) ERR_NOT_A_USER (Invalid user specified) ERR_NOT_WRITEABLE (Destination directory isn't writeable by the specified user) ERR_EXISTS (Destination file already exists) ERR_REQUEST_FAILED (General failure to spawn upload thread) ERR_INVALID_URL (URL that was passed in couldn't be used) Request Query Parameters : Request Body : { \"dest\" : \"/iplant/home/testuser/\", \"address\" : \"http://www.google.com/index.html\" } Response Body : On success you should get JSON that looks like this: { \"msg\" : \"Upload scheduled.\", \"url\" : \"<URL>\", \"label\" : \"<URL base filename>\", \"dest\" : \"<destination in irods>\" } On on error, you'll either get a stacktrace or JSON that looks like this: { \"msg\" : \"<JSON passed in through the request>\", \"error_code\" : \"ERR_REQUEST_FAILED\" } If the URL passed in is incorrect, then the error message will look like this: { \"error_code\" : \"ERR_INVALID_URL\", \"url\" : \"<URL Passed in>\" } Curl Command : curl -H \"$AUTH_HEADER\" -d '{\"dest\" : \"/iplant/home/testuser/\", \"address\" : \"http://www.google.com/index.html\"}' http://127.0.0.1:31370/secured/fileio/urlupload The 'dest' value in the JSON refers to the path to the directory in iRODS that the file will be saved off to. The filename of the file will be extracted from the path portion of the URL.","title":"URL Uploads"},{"location":"services/api/endpoints/fileio/#save","text":"URL Path : /secured/fileio/save HTTP Method : POST Error Codes : ERR_INVALID_JSON (Missing content-type or JSON syntax error) ERR_BAD_OR_MISSING_FIELD (Missing JSON field or invalid JSON field value) ERR_NOT_A_USER (Invalid user specified) ERR_DOES_NOT_EXIST (The destination directory does not exist) ERR_NOT_WRITEABLE (The destination directory is not writable by the user) ERR_FILE_SIZE_TOO_LARGE (The size of the \"content\" field is larger than the terrain.fileio.max-edit-file-size config setting) Request Body : { \"content\" : \"This is the content for the file.\" , \"dest\" : \"/iplant/home/testuser/savedfile.txt\" } Response Body : { \"file\" : { \"id\" : \"<path to the file>\" , \"path\" : \"<path to the file>\" , \"label\" : \"<basename of the file path>\" , \"permission\" : \"own\" , \"date-created\" : <seco n ds si n ce t he epoch> , \"date-modified\" : <seco n ds si n ce t he epoch> , \"file-size\" : <size i n by tes > } } Curl Command : curl -H \"$AUTH_HEADER\" -d '{\"dest\" : \"/iplant/home/testuser/savedfile.txt\", \"content\" : \"This is the content for the file.\"}' 'http://127.0.0.1:31370/secured/fileio/save'","title":"Save"},{"location":"services/api/endpoints/fileio/#save-as","text":"URL Path : /secured/fileio/saveas HTTP Method : POST Error Codes : ERR_INVALID_JSON (Missing content-type or JSON syntax error) ERR_BAD_OR_MISSING_FIELD (Missing JSON field or invalid JSON field value) ERR_NOT_A_USER (Invalid user specified) ERR_DOES_NOT_EXIST (The destination directory does not exist) ERR_NOT_WRITEABLE (The destination directory is not writable by the user) ERR_EXISTS (The destination file already exists) Request Body : { \"content\" : \"This is the content for the file.\" , \"dest\" : \"/iplant/home/testuser/savedfile.txt\" } Response Body : { \"file\" : { \"id\" : \"<path to the file>\" , \"path\" : \"<path to the file>\" , \"label\" : \"<basename of the file path>\" , \"permission\" : \"own\" , \"date-created\" : <seco n ds si n ce t he epoch> , \"date-modified\" : <seco n ds si n ce t he epoch> , \"file-size\" : <size i n by tes > } } Curl Command : curl -H \"$AUTH_HEADER\" -d '{\"content\" : \"This is the content for the file.\", \"dest\" : \"/iplant/home/testuser/savedfile.txt\"}' 'http://127.0.0.1:31370/secured/fileio/saveas' This endpoint delegates to data-info's POST /data endpoint, similarly to the upload endpoint, with some processing.","title":"Save As"},{"location":"services/api/endpoints/misc/","text":"Jump to: Miscellaneous Terrain Endpoints Verifying that Terrain is Running Saving User Session Data Retrieving User Session Data Removing User Session Data Saving User Preferences Retrieving User Preferences Removing User Preferences Obtaining Identifiers Miscellaneous Terrain Endpoints \u00b6 Note that secured endpoints in Terrain and apps are a little different from each other. Please see Terrain Vs. Apps for more information. Verifying that Terrain is Running \u00b6 Unsecured Endpoint: GET / The root path in Terrain can be used to verify that Terrain is actually running and is responding. Currently, the response to this URL contains only a welcome message. Here's an example: $ curl -s http://by-tor:8888/ The infinite is attainable with Terrain! Saving User Session Data \u00b6 Secured Endpoint: POST /secured/sessions This service can be used to save arbitrary JSON user session information. The post body is stored as-is and can be retrieved by sending an HTTP GET request to the same URL. Here's an example: $ curl -sH \"$AUTH_HEADER\" -d '{\"foo\":\"bar\"}' \"http://by-tor:8888/secured/sessions\" Retrieving User Session Data \u00b6 Secured Endpoint: GET /secured/sessions This service can be used to retrieve user session information that was previously saved by sending a POST request to the same service. Here's an example: $ curl -H \"$AUTH_HEADER\" \"http://by-tor:8888/secured/sessions\" {\"foo\":\"bar\"} Removing User Session Data \u00b6 Secured Endpoint: DELETE /secured/sessions This service can be used to remove saved user session information. This is helpful in cases where the user's session is in an unusable state and saving the session information keeps all of the user's future sessions in an unusable state. Here's an example: $ curl -XDELETE -H \"$AUTH_HEADER\" \"http://by-tor:8888/secured/sessions\" Check the HTTP status of the response to tell if it succeeded. It should return a status in the 200 range. An attempt to remove session data that doesn't already exist will be silently ignored and return a 200 range HTTP status code. Saving User Preferences \u00b6 Secured Endpoint: POST /secured/preferences This service can be used to save arbitrary user preferences. The body must contain all of the preferences for the user; any key-value pairs that are missing will be removed from the preferences. Please note that the \"defaultOutputDir\" and the \"systemDefaultOutputDir\" will always be present, even if not included in the JSON passed in. Example: $ curl -sH \"$AUTH_HEADER\" -d '{\"appsKBShortcut\":\"A\",\"rememberLastPath\":true,\"closeKBShortcut\":\"Q\",\"defaultOutputFolder\":{\"id\":\"/iplant/home/wregglej/analyses\",\"path\":\"/iplant/home/wregglej/analyses\"},\"dataKBShortcut\":\"D\",\"systemDefaultOutputDir\":{\"id\":\"/iplant/home/wregglej/analyses\",\"path\":\"/iplant/home/wregglej/analyses\"},\"saveSession\":true,\"enableEmailNotification\":true,\"lastPathId\":\"/iplant/home/wregglej\",\"notificationKBShortcut\":\"N\",\"defaultFileSelectorPath\":\"/iplant/home/wregglej\",\"analysisKBShortcut\":\"Y\"}' \"http://by-tor:8888/secured/preferences\" | squiggles { \"preferences\": { \"analysisKBShortcut\": \"Y\", \"appsKBShortcut\": \"A\", \"closeKBShortcut\": \"Q\", \"dataKBShortcut\": \"D\", \"defaultFileSelectorPath\": \"/iplant/home/wregglej\", \"defaultOutputFolder\": { \"id\": \"/iplant/home/wregglej/analyses\", \"path\": \"/iplant/home/wregglej/analyses\" }, \"enableEmailNotification\": true, \"lastPathId\": \"/iplant/home/wregglej\", \"notificationKBShortcut\": \"N\", \"rememberLastPath\": true, \"saveSession\": true, \"systemDefaultOutputDir\": { \"id\": \"/iplant/home/wregglej/analyses\", \"path\": \"/iplant/home/wregglej/analyses\" } } } Retrieving User Preferences \u00b6 Secured Endpoint: GET /secured/preferences This service can be used to retrieve a user's preferences. Example: $ curl -sH \"$AUTH_HEADER\" \"http://by-tor:8888/secured/preferences\" | squiggles { \"analysisKBShortcut\": \"Y\", \"appsKBShortcut\": \"A\", \"closeKBShortcut\": \"Q\", \"dataKBShortcut\": \"D\", \"defaultFileSelectorPath\": \"/iplant/home/test\", \"defaultOutputFolder\": { \"id\": \"/iplant/home/test/analyses\", \"path\": \"/iplant/home/test/analyses\" }, \"enableEmailNotification\": true, \"lastPathId\": \"/iplant/home/test\", \"notificationKBShortcut\": \"N\", \"rememberLastPath\": true, \"saveSession\": true, \"systemDefaultOutputDir\": { \"id\": \"/iplant/home/test/analyses\", \"path\": \"/iplant/home/test/analyses\" } } Removing User Preferences \u00b6 Secured Endpoint: DELETE /secured/preferences This service can be used to remove a user's preferences. Please note that the \"defaultOutputDir\" and the \"systemDefaultOutputDir\" will still be present in the preferences after a deletion. Example: $ curl -X DELETE -H \"$AUTH_HEADER\" \"http://by-tor:8888/secured/preferences\" Check the HTTP status code of the response to determine success. It should be in the 200 range. An attempt to remove preference data that doesn't already exist will be silently ignored. Obtaining Identifiers \u00b6 Unsecured Endpoint: GET /uuid In some cases, it's difficult for the UI client code to generate UUIDs for objects that require them. This service returns a single UUID in the response body. The UUID is returned as a plain text string.","title":"Miscellaneous"},{"location":"services/api/endpoints/misc/#miscellaneous-terrain-endpoints","text":"Note that secured endpoints in Terrain and apps are a little different from each other. Please see Terrain Vs. Apps for more information.","title":"Miscellaneous Terrain Endpoints"},{"location":"services/api/endpoints/misc/#verifying-that-terrain-is-running","text":"Unsecured Endpoint: GET / The root path in Terrain can be used to verify that Terrain is actually running and is responding. Currently, the response to this URL contains only a welcome message. Here's an example: $ curl -s http://by-tor:8888/ The infinite is attainable with Terrain!","title":"Verifying that Terrain is Running"},{"location":"services/api/endpoints/misc/#saving-user-session-data","text":"Secured Endpoint: POST /secured/sessions This service can be used to save arbitrary JSON user session information. The post body is stored as-is and can be retrieved by sending an HTTP GET request to the same URL. Here's an example: $ curl -sH \"$AUTH_HEADER\" -d '{\"foo\":\"bar\"}' \"http://by-tor:8888/secured/sessions\"","title":"Saving User Session Data"},{"location":"services/api/endpoints/misc/#retrieving-user-session-data","text":"Secured Endpoint: GET /secured/sessions This service can be used to retrieve user session information that was previously saved by sending a POST request to the same service. Here's an example: $ curl -H \"$AUTH_HEADER\" \"http://by-tor:8888/secured/sessions\" {\"foo\":\"bar\"}","title":"Retrieving User Session Data"},{"location":"services/api/endpoints/misc/#removing-user-session-data","text":"Secured Endpoint: DELETE /secured/sessions This service can be used to remove saved user session information. This is helpful in cases where the user's session is in an unusable state and saving the session information keeps all of the user's future sessions in an unusable state. Here's an example: $ curl -XDELETE -H \"$AUTH_HEADER\" \"http://by-tor:8888/secured/sessions\" Check the HTTP status of the response to tell if it succeeded. It should return a status in the 200 range. An attempt to remove session data that doesn't already exist will be silently ignored and return a 200 range HTTP status code.","title":"Removing User Session Data"},{"location":"services/api/endpoints/misc/#saving-user-preferences","text":"Secured Endpoint: POST /secured/preferences This service can be used to save arbitrary user preferences. The body must contain all of the preferences for the user; any key-value pairs that are missing will be removed from the preferences. Please note that the \"defaultOutputDir\" and the \"systemDefaultOutputDir\" will always be present, even if not included in the JSON passed in. Example: $ curl -sH \"$AUTH_HEADER\" -d '{\"appsKBShortcut\":\"A\",\"rememberLastPath\":true,\"closeKBShortcut\":\"Q\",\"defaultOutputFolder\":{\"id\":\"/iplant/home/wregglej/analyses\",\"path\":\"/iplant/home/wregglej/analyses\"},\"dataKBShortcut\":\"D\",\"systemDefaultOutputDir\":{\"id\":\"/iplant/home/wregglej/analyses\",\"path\":\"/iplant/home/wregglej/analyses\"},\"saveSession\":true,\"enableEmailNotification\":true,\"lastPathId\":\"/iplant/home/wregglej\",\"notificationKBShortcut\":\"N\",\"defaultFileSelectorPath\":\"/iplant/home/wregglej\",\"analysisKBShortcut\":\"Y\"}' \"http://by-tor:8888/secured/preferences\" | squiggles { \"preferences\": { \"analysisKBShortcut\": \"Y\", \"appsKBShortcut\": \"A\", \"closeKBShortcut\": \"Q\", \"dataKBShortcut\": \"D\", \"defaultFileSelectorPath\": \"/iplant/home/wregglej\", \"defaultOutputFolder\": { \"id\": \"/iplant/home/wregglej/analyses\", \"path\": \"/iplant/home/wregglej/analyses\" }, \"enableEmailNotification\": true, \"lastPathId\": \"/iplant/home/wregglej\", \"notificationKBShortcut\": \"N\", \"rememberLastPath\": true, \"saveSession\": true, \"systemDefaultOutputDir\": { \"id\": \"/iplant/home/wregglej/analyses\", \"path\": \"/iplant/home/wregglej/analyses\" } } }","title":"Saving User Preferences"},{"location":"services/api/endpoints/misc/#retrieving-user-preferences","text":"Secured Endpoint: GET /secured/preferences This service can be used to retrieve a user's preferences. Example: $ curl -sH \"$AUTH_HEADER\" \"http://by-tor:8888/secured/preferences\" | squiggles { \"analysisKBShortcut\": \"Y\", \"appsKBShortcut\": \"A\", \"closeKBShortcut\": \"Q\", \"dataKBShortcut\": \"D\", \"defaultFileSelectorPath\": \"/iplant/home/test\", \"defaultOutputFolder\": { \"id\": \"/iplant/home/test/analyses\", \"path\": \"/iplant/home/test/analyses\" }, \"enableEmailNotification\": true, \"lastPathId\": \"/iplant/home/test\", \"notificationKBShortcut\": \"N\", \"rememberLastPath\": true, \"saveSession\": true, \"systemDefaultOutputDir\": { \"id\": \"/iplant/home/test/analyses\", \"path\": \"/iplant/home/test/analyses\" } }","title":"Retrieving User Preferences"},{"location":"services/api/endpoints/misc/#removing-user-preferences","text":"Secured Endpoint: DELETE /secured/preferences This service can be used to remove a user's preferences. Please note that the \"defaultOutputDir\" and the \"systemDefaultOutputDir\" will still be present in the preferences after a deletion. Example: $ curl -X DELETE -H \"$AUTH_HEADER\" \"http://by-tor:8888/secured/preferences\" Check the HTTP status code of the response to determine success. It should be in the 200 range. An attempt to remove preference data that doesn't already exist will be silently ignored.","title":"Removing User Preferences"},{"location":"services/api/endpoints/misc/#obtaining-identifiers","text":"Unsecured Endpoint: GET /uuid In some cases, it's difficult for the UI client code to generate UUIDs for objects that require them. This service returns a single UUID in the response body. The UUID is returned as a plain text string.","title":"Obtaining Identifiers"},{"location":"services/api/endpoints/notifications/","text":"Jump to: Notification Endpoints Obtaining Notifications Obtaining Notification Counts Obtaining Unseen Notifications Obtaining the Ten Most Recent Notifications Marking Notifications as Seen Marking All Notifications as Seen Marking Notifications as Deleted Marking All Notifications as Deleted Sending an Arbitrary Notification Endpoints for System Messages (a.k.a. System Notifications) Notification Endpoints \u00b6 Obtaining Notifications \u00b6 Secured Endpoint: GET /secured/notifications/messages This endpoint is primarily a passthrough endpoint to the notification agent's /messages endpoint, but it does make calls into the apps service in order to add the app description to job status update notifications. Notifications in the DE are used to inform users when some event (for example a job status change or the completion of a file upload) has occurred. This service provides a way for the DE to retrieve notifications that the user may or may not have seen before. This service accepts five different query-string parameters: Name Description Required/Optional limit The maximum number of notifications to return at a time or `0` if there is no limit. Optional (defaults to `0`) offset The index of the starting message. Optional (defaults to `0`) seen Indicates whether messages that the user has seen, messages that the user has not seen, or both should be returned. If this parameter is equal to `true` then only messages that the user has seen will be returned. If it is equal to `false` then only messages that the user has not seen will be returned. If this parameter is not specified at all then both messages that have been seen and messages that have not been seen will be returned. Optional sortField The field to use when sorting messages. Currently, the only supported value for this field is `timestamp`. Optional (defaults to `timestamp`) sortDir The sorting direction, which can be `asc` (ascending) or `desc` (descending). Optional (defaults to `desc`) filter Specifies the type of notification messages to return, which can be `data`, `analysis` or `tool`. Other types of notifications may be added in the future. If this parameter it not specified then all types of notifications will be returned. Optional The response body for this service is in the following format: { \"messages\" : [ { \"deleted\" : \"deleted-flag\" , \"message\" : { \"id\" : \"message-id\" , \"text\" : \"message-text\" , \"timestamp\" : \"milliseconds-since-epoch\" , } \"outputDir\" : \"output-directory-path\" , \"outputManifest\" : \"list-of-output-files\" , \"payload\" : {} \"seen\" : \"seen-flag\" , \"type\" : \"notification-type-code\" , \"user\" : \"username\" }, ], \"total\" : \"message-count\" } The payload object in each message is a JSON object with a format that is specific to the notification type, and its format will vary. There are currently three types of notifications that we support: data , analysis and tool . The data and analysis notification types have the same payload format: { \"action\" : \"action-code\" , \"analysis-details\" : \"analysis-description\" , \"analysis_id\" : \"analysis-id\" , \"analyis_name\" : \"analysis-name\" , \"description\" : \"job-description\" , \"enddate\" : \"end-date-in-milliseconds-since-epoch\" , \"id\" : \"job-id\" , \"name\" : \"job-name\" , \"resultfolderid\" : \"result-folder-path\" , \"startdate\" : \"start-date-in-milliseconds-since-epoch\" , \"status\" : \"job-status-code\" , \"user\" : \"username\" } The payload format for the tool notification type is a little simpler: { \"email_address\" : \"email-address\" , \"toolname\" : \"tool-name\" , \"tooldirectory\" : \"tool-directory\" , \"tooldescription\" : \"tool-description\" , \"toolattribution\" : \"tool-attribution\" , \"toolversion\" : \"tool-version\" } Here's an example: $ curl -sH \"$AUTH_HEADER\" \"http://by-tor:8888/secured/notifications/messages?limit=1&offset=0\" | python -mjson.tool { \"messages\": [ { \"deleted\": false, \"message\": { \"id\": \"C15763CF-A5C9-48F5-BE4F-9FB3CB1897EB\", \"text\": \"URL Import of somefile.txt from http://snow-dog.iplantcollaborative.org/somefile.txt completed\", \"timestamp\": 1331068427000 }, \"outputDir\": \"/iplant/home/nobody\", \"outputManifest\": [], \"payload\": { \"action\": \"job_status_change\", \"analysis-details\": \"\", \"analysis_id\": \"\", \"analysis_name\": \"\", \"description\": \"URL Import of somefile.txt from http://snow-dog.iplantcollaborative.org/somefile.txt\", \"enddate\": 1331068427000, \"id\": \"40115C19-AFBC-4CAE-9738-324DD8B18FDC\", \"name\": \"URL Import of somefile.txt from http://snow-dog.iplantcollaborative.org/somefile.txt\", \"resultfolderid\": \"/iplant/home/nobody\", \"startdate\": \"1331068414712\", \"status\": \"Completed\", \"user\": \"nobody\" }, \"seen\": true, \"type\": \"data\", \"user\": \"nobody\" } ], \"total\": \"37\" } Obtaining Notification Counts \u00b6 Secured Endpoint: GET /secured/notifications/count-messages This endpoint is a passthrough to the notification agent endpoint, /count-messages . Please see the notification agent documentation for more details. This service takes a subset of the query-string parameters as the /messages service, and returns the number of messages that match the criteria specified in the query-string parameters. Here's the list of supported query-string parameters: Name Description Required/Optional seen Indicates whether messages that the user has seen, messages that the user has not seen, or both should be counted. If this parameter is equal to `true` then only messages that the user has seen will be counted. If it is equal to `false` then only messages that the user has not seen will be counted. If this parameter is not specified at all then both messages that have been seen and messages that have not been seen will be counted. Optional filter Specifies the type of notification messages to return, which can be `data`, `analysis` or `tool`. Other types of notifications may be added in the future. If this parameter it not specified then all types of notifications will be returned. Optional The response body consists of a JSON object containing four fields: user-total , contains the number of user messages that have not been marked as deleted and match the criteria specified in the query string, system-total contains the number of system messages that are active and have not been dismissed by the user, system-total-new contains the number of system messages that have not been marked as received by the user, and system-total-unseen contains the number of system messages that have not been marked as seen by the user. { \"user-total\" : cou nt , \"system-total\" : cou nt , \"system-total-new\" : cou nt , \"system-total-unseen\" : cou nt } Here are some examples: $ curl -sH \"$AUTH_HEADER\" \"http://by-tor:8888/secured/notifications/count-messages\" | python -mjson.tool { \"user-total\": 409, \"system-total\": 10, \"system-total-new\": 0, \"system-total-unseen\": 1 } In this example, all messages that are available for the user that have not been marked as deleted are counted. $ curl -sH \"$AUTH_HEADER\" \"http://by-tor:8888/secured/notifications/count-messages?filter=data\" | python -mjson.tool { \"user-total\": 91, \"system-total\": 10, \"system-total-new\": 0, \"system-total-unseen\": 1 } In this example only the data notifications that have not been marked as deleted are counted. Obtaining Unseen Notifications \u00b6 Secured Endpoint: GET /secured/notifications/unseen-messages This endpoint is primarily a passthrough endpoint to the notification agent's /unseen-messages endpoint, but it does make calls into the apps service in order to add the app description to job status update notifications. This service is used to obtain notifications that the user hasn't seen yet. This service takes no query-string parameters. Here's an example: $ curl -sH \"$AUTH_HEADER\" \"http://by-tor:8888/secured/notifications/unseen-messages\" | python -mjson.tool { \"messages\": [] } Obtaining the Ten Most Recent Notifications \u00b6 Secured Endpoint: GET /secured/notifications/last-ten-messages This endpoint is primarily a passthrough endpoint to the notification agent's /last-ten-messages endpoint, but it does make calls into the apps service in order to add the app description to job status update notifications. This endpoint returns the ten most recent messages for the authenticated user in ascending order by message timestamp. Obtaining the ten most recent messages in ascending order is difficult using other endpoints. Examples are omitted for this endpoint because the response body is identical to that of the other endpoints used to obtain notifications. Marking Notifications as Seen \u00b6 Secured Endpoint: POST /secured/notifications/seen This endpoint is a passthrough to the notification agent's /seen endpoint. Please see the notification agent documentation for more details. Once a user has seen a notification, the notification should be marked as seen to prevent it from being returned by the /notifications/unseen-messages endpoint. This service provides a way to mark notifications as seen. The request body for this service is in the following format: { \"uuids\": [ \"uuid-1\", \"uuid-2\", \"uuid-n\" ] } The response body for this service is a simple JSON object that indicates whether or not the service call succeeded and contains the number of messages that are still marked as unseen. Here's an example: $ curl -sH \"$AUTH_HEADER\" -d ' { \"uuids\": [ \"C15763CF-A5C9-48F5-BE4F-9FB3CB1897EB\" ] } ' \"http://by-tor:8888/secured/notifications/seen\" | python -mjson.tool { \"count\": 0 } Note that the UUIDs provided in the request body must be obtained from the message -> id element of the notification the user wishes to mark as seen. Marking All Notifications as Seen \u00b6 Secured Endpoint: POST /secured/notifications/mark-all-seen This endpoint is a passthrough to the notification agent's /mark-all-seen endpoint. Please see the notification agent documentation for more information about the format of the request body. This endpoint will add or overwrite the \"user\" field in the request body forwarded to the NotificationAgent with the username of the authenticated user making the request. The response body for this service is a simple JSON object that indicates whether or not the service call succeeded and contains the number of messages that are still marked as unseen. Here's an example: $ curl -sH \"$AUTH_HEADER\" -d '{}' \"http://by-tor:8888/secured/notifications/mark-all-seen\" | python -mjson.tool { \"count\": 0 } Marking Notifications as Deleted \u00b6 Secured Endpoint: POST /secured/notifications/delete This endpoint is a passthrough to the notification agent's /delete endpoint. Please see the notification agent documentation for more details. Users may wish to dismiss notifications that they've already seen. This service marks one or more notifications as deleted so that neither the /notfications/messages endpoint nor the /notifications/unseen-messages endpoint will return them. The request body for this service is in the following format: { \"uuids\": [ \"uuid-1\", \"uuid-2\", \"uuid-n\" ] } The response body for this service is a simple JSON object that indicates whether or not the service call succeeded. Here's an example: $ curl -sH \"$AUTH_HEADER\" -d ' { \"uuids\": [ \"C15763CF-A5C9-48F5-BE4F-9FB3CB1897EB\" ] } ' \"http://by-tor:8888/secured/notifications/delete\" Note that the UUIDs provided in the request body must be obtained from the message -> id element of the notification the user wishes to delete. Marking All Notifications as Deleted \u00b6 Secured Endpoint: DELETE /secured/notifications/delete-all This endpoint is a passthrough to the notification agent's /delete-all endpoint. Please see the notification agent documentation for more details. This endpoint will add or replace the \"user\" parameter in the request forwarded to the NotificationAgent with the username of the authenticated user making the request. The response body for this service is a simple JSON object that indicates whether or not the service call succeeded and contains the number of messages that are still marked as unseen. Here's an example: $ curl -X DELETE -sH \"$AUTH_HEADER\" \"http://by-tor:8888/secured/notifications/delete-all\" | python -mjson.tool { \"count\": 0 } Sending an Arbitrary Notification \u00b6 Unsecured Endpoint: POST /send-notification. This endpoint is a passthrough to the notification agent's /notification endpoint. Please see the notification agent documentation for more details. Endpoints for System Messages (a.k.a. System Notifications) \u00b6 The endpoints for the system messages are straight pass throughs to the corresponding calls in the Notification Agent. The only difference is that the endpoints in Terrain are prefixed with /secured/notifications or /admin/notifications and that endpoints that require the user query string parameter instead take the authentication header. Terrain Endpoint Notification Agent Endpoint GET /secured/notifications/system/messages GET /system/messages GET /secured/notifications/system/new-messages GET /system/new-messages GET /secured/notifications/system/unseen-messages GET /system/unseen-messages POST /secured/notifications/system/received POST /system/received POST /secured/notifications/system/mark-all-received POST /system/mark-all-received POST /secured/notifications/system/seen POST /system/seen POST /secured/notifications/system/mark-all-seen POST /system/mark-all-seen POST /secured/notifications/system/delete POST /system/delete DELETE /secured/notifications/system/delete-all DELETE /system/delete-all PUT /admin/notifications/system PUT /admin/system GET /admin/notifications/system GET /admin/system GET /admin/notifications/system/:uuid GET /admin/system/:uuid POST /admin/notifications/system/:uuid POST /admin/system/:uuid DELETE /admin/notifications/system/:uuid DELETE /admin/system/:uuid GET /admin/notifications/system-types GET /admin/system-types","title":"Notification"},{"location":"services/api/endpoints/notifications/#notification-endpoints","text":"","title":"Notification Endpoints"},{"location":"services/api/endpoints/notifications/#obtaining-notifications","text":"Secured Endpoint: GET /secured/notifications/messages This endpoint is primarily a passthrough endpoint to the notification agent's /messages endpoint, but it does make calls into the apps service in order to add the app description to job status update notifications. Notifications in the DE are used to inform users when some event (for example a job status change or the completion of a file upload) has occurred. This service provides a way for the DE to retrieve notifications that the user may or may not have seen before. This service accepts five different query-string parameters: Name Description Required/Optional limit The maximum number of notifications to return at a time or `0` if there is no limit. Optional (defaults to `0`) offset The index of the starting message. Optional (defaults to `0`) seen Indicates whether messages that the user has seen, messages that the user has not seen, or both should be returned. If this parameter is equal to `true` then only messages that the user has seen will be returned. If it is equal to `false` then only messages that the user has not seen will be returned. If this parameter is not specified at all then both messages that have been seen and messages that have not been seen will be returned. Optional sortField The field to use when sorting messages. Currently, the only supported value for this field is `timestamp`. Optional (defaults to `timestamp`) sortDir The sorting direction, which can be `asc` (ascending) or `desc` (descending). Optional (defaults to `desc`) filter Specifies the type of notification messages to return, which can be `data`, `analysis` or `tool`. Other types of notifications may be added in the future. If this parameter it not specified then all types of notifications will be returned. Optional The response body for this service is in the following format: { \"messages\" : [ { \"deleted\" : \"deleted-flag\" , \"message\" : { \"id\" : \"message-id\" , \"text\" : \"message-text\" , \"timestamp\" : \"milliseconds-since-epoch\" , } \"outputDir\" : \"output-directory-path\" , \"outputManifest\" : \"list-of-output-files\" , \"payload\" : {} \"seen\" : \"seen-flag\" , \"type\" : \"notification-type-code\" , \"user\" : \"username\" }, ], \"total\" : \"message-count\" } The payload object in each message is a JSON object with a format that is specific to the notification type, and its format will vary. There are currently three types of notifications that we support: data , analysis and tool . The data and analysis notification types have the same payload format: { \"action\" : \"action-code\" , \"analysis-details\" : \"analysis-description\" , \"analysis_id\" : \"analysis-id\" , \"analyis_name\" : \"analysis-name\" , \"description\" : \"job-description\" , \"enddate\" : \"end-date-in-milliseconds-since-epoch\" , \"id\" : \"job-id\" , \"name\" : \"job-name\" , \"resultfolderid\" : \"result-folder-path\" , \"startdate\" : \"start-date-in-milliseconds-since-epoch\" , \"status\" : \"job-status-code\" , \"user\" : \"username\" } The payload format for the tool notification type is a little simpler: { \"email_address\" : \"email-address\" , \"toolname\" : \"tool-name\" , \"tooldirectory\" : \"tool-directory\" , \"tooldescription\" : \"tool-description\" , \"toolattribution\" : \"tool-attribution\" , \"toolversion\" : \"tool-version\" } Here's an example: $ curl -sH \"$AUTH_HEADER\" \"http://by-tor:8888/secured/notifications/messages?limit=1&offset=0\" | python -mjson.tool { \"messages\": [ { \"deleted\": false, \"message\": { \"id\": \"C15763CF-A5C9-48F5-BE4F-9FB3CB1897EB\", \"text\": \"URL Import of somefile.txt from http://snow-dog.iplantcollaborative.org/somefile.txt completed\", \"timestamp\": 1331068427000 }, \"outputDir\": \"/iplant/home/nobody\", \"outputManifest\": [], \"payload\": { \"action\": \"job_status_change\", \"analysis-details\": \"\", \"analysis_id\": \"\", \"analysis_name\": \"\", \"description\": \"URL Import of somefile.txt from http://snow-dog.iplantcollaborative.org/somefile.txt\", \"enddate\": 1331068427000, \"id\": \"40115C19-AFBC-4CAE-9738-324DD8B18FDC\", \"name\": \"URL Import of somefile.txt from http://snow-dog.iplantcollaborative.org/somefile.txt\", \"resultfolderid\": \"/iplant/home/nobody\", \"startdate\": \"1331068414712\", \"status\": \"Completed\", \"user\": \"nobody\" }, \"seen\": true, \"type\": \"data\", \"user\": \"nobody\" } ], \"total\": \"37\" }","title":"Obtaining Notifications"},{"location":"services/api/endpoints/notifications/#obtaining-notification-counts","text":"Secured Endpoint: GET /secured/notifications/count-messages This endpoint is a passthrough to the notification agent endpoint, /count-messages . Please see the notification agent documentation for more details. This service takes a subset of the query-string parameters as the /messages service, and returns the number of messages that match the criteria specified in the query-string parameters. Here's the list of supported query-string parameters: Name Description Required/Optional seen Indicates whether messages that the user has seen, messages that the user has not seen, or both should be counted. If this parameter is equal to `true` then only messages that the user has seen will be counted. If it is equal to `false` then only messages that the user has not seen will be counted. If this parameter is not specified at all then both messages that have been seen and messages that have not been seen will be counted. Optional filter Specifies the type of notification messages to return, which can be `data`, `analysis` or `tool`. Other types of notifications may be added in the future. If this parameter it not specified then all types of notifications will be returned. Optional The response body consists of a JSON object containing four fields: user-total , contains the number of user messages that have not been marked as deleted and match the criteria specified in the query string, system-total contains the number of system messages that are active and have not been dismissed by the user, system-total-new contains the number of system messages that have not been marked as received by the user, and system-total-unseen contains the number of system messages that have not been marked as seen by the user. { \"user-total\" : cou nt , \"system-total\" : cou nt , \"system-total-new\" : cou nt , \"system-total-unseen\" : cou nt } Here are some examples: $ curl -sH \"$AUTH_HEADER\" \"http://by-tor:8888/secured/notifications/count-messages\" | python -mjson.tool { \"user-total\": 409, \"system-total\": 10, \"system-total-new\": 0, \"system-total-unseen\": 1 } In this example, all messages that are available for the user that have not been marked as deleted are counted. $ curl -sH \"$AUTH_HEADER\" \"http://by-tor:8888/secured/notifications/count-messages?filter=data\" | python -mjson.tool { \"user-total\": 91, \"system-total\": 10, \"system-total-new\": 0, \"system-total-unseen\": 1 } In this example only the data notifications that have not been marked as deleted are counted.","title":"Obtaining Notification Counts"},{"location":"services/api/endpoints/notifications/#obtaining-unseen-notifications","text":"Secured Endpoint: GET /secured/notifications/unseen-messages This endpoint is primarily a passthrough endpoint to the notification agent's /unseen-messages endpoint, but it does make calls into the apps service in order to add the app description to job status update notifications. This service is used to obtain notifications that the user hasn't seen yet. This service takes no query-string parameters. Here's an example: $ curl -sH \"$AUTH_HEADER\" \"http://by-tor:8888/secured/notifications/unseen-messages\" | python -mjson.tool { \"messages\": [] }","title":"Obtaining Unseen Notifications"},{"location":"services/api/endpoints/notifications/#obtaining-the-ten-most-recent-notifications","text":"Secured Endpoint: GET /secured/notifications/last-ten-messages This endpoint is primarily a passthrough endpoint to the notification agent's /last-ten-messages endpoint, but it does make calls into the apps service in order to add the app description to job status update notifications. This endpoint returns the ten most recent messages for the authenticated user in ascending order by message timestamp. Obtaining the ten most recent messages in ascending order is difficult using other endpoints. Examples are omitted for this endpoint because the response body is identical to that of the other endpoints used to obtain notifications.","title":"Obtaining the Ten Most Recent Notifications"},{"location":"services/api/endpoints/notifications/#marking-notifications-as-seen","text":"Secured Endpoint: POST /secured/notifications/seen This endpoint is a passthrough to the notification agent's /seen endpoint. Please see the notification agent documentation for more details. Once a user has seen a notification, the notification should be marked as seen to prevent it from being returned by the /notifications/unseen-messages endpoint. This service provides a way to mark notifications as seen. The request body for this service is in the following format: { \"uuids\": [ \"uuid-1\", \"uuid-2\", \"uuid-n\" ] } The response body for this service is a simple JSON object that indicates whether or not the service call succeeded and contains the number of messages that are still marked as unseen. Here's an example: $ curl -sH \"$AUTH_HEADER\" -d ' { \"uuids\": [ \"C15763CF-A5C9-48F5-BE4F-9FB3CB1897EB\" ] } ' \"http://by-tor:8888/secured/notifications/seen\" | python -mjson.tool { \"count\": 0 } Note that the UUIDs provided in the request body must be obtained from the message -> id element of the notification the user wishes to mark as seen.","title":"Marking Notifications as Seen"},{"location":"services/api/endpoints/notifications/#marking-all-notifications-as-seen","text":"Secured Endpoint: POST /secured/notifications/mark-all-seen This endpoint is a passthrough to the notification agent's /mark-all-seen endpoint. Please see the notification agent documentation for more information about the format of the request body. This endpoint will add or overwrite the \"user\" field in the request body forwarded to the NotificationAgent with the username of the authenticated user making the request. The response body for this service is a simple JSON object that indicates whether or not the service call succeeded and contains the number of messages that are still marked as unseen. Here's an example: $ curl -sH \"$AUTH_HEADER\" -d '{}' \"http://by-tor:8888/secured/notifications/mark-all-seen\" | python -mjson.tool { \"count\": 0 }","title":"Marking All Notifications as Seen"},{"location":"services/api/endpoints/notifications/#marking-notifications-as-deleted","text":"Secured Endpoint: POST /secured/notifications/delete This endpoint is a passthrough to the notification agent's /delete endpoint. Please see the notification agent documentation for more details. Users may wish to dismiss notifications that they've already seen. This service marks one or more notifications as deleted so that neither the /notfications/messages endpoint nor the /notifications/unseen-messages endpoint will return them. The request body for this service is in the following format: { \"uuids\": [ \"uuid-1\", \"uuid-2\", \"uuid-n\" ] } The response body for this service is a simple JSON object that indicates whether or not the service call succeeded. Here's an example: $ curl -sH \"$AUTH_HEADER\" -d ' { \"uuids\": [ \"C15763CF-A5C9-48F5-BE4F-9FB3CB1897EB\" ] } ' \"http://by-tor:8888/secured/notifications/delete\" Note that the UUIDs provided in the request body must be obtained from the message -> id element of the notification the user wishes to delete.","title":"Marking Notifications as Deleted"},{"location":"services/api/endpoints/notifications/#marking-all-notifications-as-deleted","text":"Secured Endpoint: DELETE /secured/notifications/delete-all This endpoint is a passthrough to the notification agent's /delete-all endpoint. Please see the notification agent documentation for more details. This endpoint will add or replace the \"user\" parameter in the request forwarded to the NotificationAgent with the username of the authenticated user making the request. The response body for this service is a simple JSON object that indicates whether or not the service call succeeded and contains the number of messages that are still marked as unseen. Here's an example: $ curl -X DELETE -sH \"$AUTH_HEADER\" \"http://by-tor:8888/secured/notifications/delete-all\" | python -mjson.tool { \"count\": 0 }","title":"Marking All Notifications as Deleted"},{"location":"services/api/endpoints/notifications/#sending-an-arbitrary-notification","text":"Unsecured Endpoint: POST /send-notification. This endpoint is a passthrough to the notification agent's /notification endpoint. Please see the notification agent documentation for more details.","title":"Sending an Arbitrary Notification"},{"location":"services/api/endpoints/notifications/#endpoints-for-system-messages-aka-system-notifications","text":"The endpoints for the system messages are straight pass throughs to the corresponding calls in the Notification Agent. The only difference is that the endpoints in Terrain are prefixed with /secured/notifications or /admin/notifications and that endpoints that require the user query string parameter instead take the authentication header. Terrain Endpoint Notification Agent Endpoint GET /secured/notifications/system/messages GET /system/messages GET /secured/notifications/system/new-messages GET /system/new-messages GET /secured/notifications/system/unseen-messages GET /system/unseen-messages POST /secured/notifications/system/received POST /system/received POST /secured/notifications/system/mark-all-received POST /system/mark-all-received POST /secured/notifications/system/seen POST /system/seen POST /secured/notifications/system/mark-all-seen POST /system/mark-all-seen POST /secured/notifications/system/delete POST /system/delete DELETE /secured/notifications/system/delete-all DELETE /system/delete-all PUT /admin/notifications/system PUT /admin/system GET /admin/notifications/system GET /admin/system GET /admin/notifications/system/:uuid GET /admin/system/:uuid POST /admin/notifications/system/:uuid POST /admin/system/:uuid DELETE /admin/notifications/system/:uuid DELETE /admin/system/:uuid GET /admin/notifications/system-types GET /admin/system-types","title":"Endpoints for System Messages (a.k.a. System Notifications)"},{"location":"services/api/endpoints/permanent-id-requests/","text":"Permanent ID Requests \u00b6 These endpoints create and manage user requests for persistent identifiers (DataCite DOIs) for data the user wishes to make public. The persistent identifiers will be created with the DataCite API and managed using DataCite Fabrica . Workflow Overview \u00b6 User reviews DOI Request Quickstart page and determines that a CyVerse DOI is appropriate for their data. User organizes their data in a single folder per Permanent ID, according to very general CyVerse guidelines. Based on what they learned from tutorial, user will complete DataCite metadata template on that folder. User creates a Permanent ID Request for this folder. Request must be for one of the available Permanent ID Request types , however the DataCite API only supports creating new DOIs. Request triggers the validation check within the DE of the metadata and folder name (must not conflict with any folder names in the data commons repo staging or curated folders). If pass: Results of request are emailed to curation team. Folder automatically moved to data commons repo staging folder. Curators automatically given own permission. User automatically given write permission. User may view all Permanent ID Requests they have submitted. User may view details for any of their Permanent ID Requests . If fail, user returns to Step 3. Curator finds request with Permanent ID Request listing . Curator may view the Permanent ID Request details , which includes the request's status history (initially only Submitted ). Curator checks metadata and data structure (see the DOI Creation SOP page for more details). Curator may update the status of the Permanent ID Request in this or any subsequent step. Curator may use any previously created Permanent ID Request Status Codes , or add a new status (which is saved for future reuse). If pass or minor changes that can be made by curator: go to Step 6 If changes needed by user: Curator emails user and asks them to make changes Once corrections are made, go to Step 6. Curator creates Permanent ID for this request. Folder name must not conflict with any folder names in the data commons repo curated folder. Metadata from folder is submitted to the DataCite API in order to create the requested Permanent ID (which currently only supports DOIs). Fails: Permanent ID not generated and error reported to curator. Request status automatically updated to Failed . Curator corrects any errors. Repeat Step 6 until pass. Passes: Permanent ID is generated and notification sent to curator and user. Request status automatically updated to Completion . Folder metadata automatically updated to include new Permanent ID and current date. Folder automatically moved to data commons repo curated folder. Curators automatically given own permission. Public automatically given read permission. Curator checks that data is public and visible on mirrors, that metadata appears correct on the DataCite Fabrica landing page, and that the Permanent ID redirect works.","title":"Permanent ID Requests"},{"location":"services/api/endpoints/permanent-id-requests/#permanent-id-requests","text":"These endpoints create and manage user requests for persistent identifiers (DataCite DOIs) for data the user wishes to make public. The persistent identifiers will be created with the DataCite API and managed using DataCite Fabrica .","title":"Permanent ID Requests"},{"location":"services/api/endpoints/permanent-id-requests/#workflow-overview","text":"User reviews DOI Request Quickstart page and determines that a CyVerse DOI is appropriate for their data. User organizes their data in a single folder per Permanent ID, according to very general CyVerse guidelines. Based on what they learned from tutorial, user will complete DataCite metadata template on that folder. User creates a Permanent ID Request for this folder. Request must be for one of the available Permanent ID Request types , however the DataCite API only supports creating new DOIs. Request triggers the validation check within the DE of the metadata and folder name (must not conflict with any folder names in the data commons repo staging or curated folders). If pass: Results of request are emailed to curation team. Folder automatically moved to data commons repo staging folder. Curators automatically given own permission. User automatically given write permission. User may view all Permanent ID Requests they have submitted. User may view details for any of their Permanent ID Requests . If fail, user returns to Step 3. Curator finds request with Permanent ID Request listing . Curator may view the Permanent ID Request details , which includes the request's status history (initially only Submitted ). Curator checks metadata and data structure (see the DOI Creation SOP page for more details). Curator may update the status of the Permanent ID Request in this or any subsequent step. Curator may use any previously created Permanent ID Request Status Codes , or add a new status (which is saved for future reuse). If pass or minor changes that can be made by curator: go to Step 6 If changes needed by user: Curator emails user and asks them to make changes Once corrections are made, go to Step 6. Curator creates Permanent ID for this request. Folder name must not conflict with any folder names in the data commons repo curated folder. Metadata from folder is submitted to the DataCite API in order to create the requested Permanent ID (which currently only supports DOIs). Fails: Permanent ID not generated and error reported to curator. Request status automatically updated to Failed . Curator corrects any errors. Repeat Step 6 until pass. Passes: Permanent ID is generated and notification sent to curator and user. Request status automatically updated to Completion . Folder metadata automatically updated to include new Permanent ID and current date. Folder automatically moved to data commons repo curated folder. Curators automatically given own permission. Public automatically given read permission. Curator checks that data is public and visible on mirrors, that metadata appears correct on the DataCite Fabrica landing page, and that the Permanent ID redirect works.","title":"Workflow Overview"},{"location":"services/api/endpoints/quick-launches/","text":"Quick Launches \u00b6 Quick Launches provide a way to set default parameter values for an analysis, which can make it much easier to launch a large number of similar jobs without having to select the parameter values that the jobs have in common for every new analysis. Quick Launches are created on a per-user basis, but they can be made public when they\u2019re created and shared with other users. The Swagger docs are available here: https://de.cyverse.org/terrain/docs/index.html#/analyses-quicklaunches Creating a new Quick Launch is a lot like launching an app. The primary difference is that instead of calling the POST /terrain/analyses endpoint, call the POST /terrain/quicklaunches endpoint. The request body contains an analysis submission wrapped in an outer object. The fields at the top level describe the Quick Launch itself. The object in the submission field contains the request body that's normally sent to the POST /terrain/analyses endpoint when launching a job. Note that not all required analysis parameters of the AnalysisSubmissionConfig need to be specified in the Quick Launch. Once a Quick Launch is defined, the GET /terrain/quicklaunches endpoint can be used to list Quick Launches that are available to the user. The description currently says that it lists Quick Launches that were created by the user. This isn\u2019t quite correct. Any Quick Launch that the user has permission to view will be listed by this endpoint. All available Quick Launches for an app can be listed using the GET /terrain/quicklaunches/apps/{app-id} endpoint. This endpoint will only list Quick Launches that the user has permission to view. Quick Launch defaults allow the user to specify which Quick Launch to use for a specific app. Both global defaults and user-specific defaults can be defined. The app launch info saved in a Quick Launch can be retrieved with the GET /terrain/quicklaunches/{ql-id}/app-info endpoint. This is analogous to calling the GET /terrain/apps/{system-id}/{app-id} endpoint.","title":"Quick Launches"},{"location":"services/api/endpoints/quick-launches/#quick-launches","text":"Quick Launches provide a way to set default parameter values for an analysis, which can make it much easier to launch a large number of similar jobs without having to select the parameter values that the jobs have in common for every new analysis. Quick Launches are created on a per-user basis, but they can be made public when they\u2019re created and shared with other users. The Swagger docs are available here: https://de.cyverse.org/terrain/docs/index.html#/analyses-quicklaunches Creating a new Quick Launch is a lot like launching an app. The primary difference is that instead of calling the POST /terrain/analyses endpoint, call the POST /terrain/quicklaunches endpoint. The request body contains an analysis submission wrapped in an outer object. The fields at the top level describe the Quick Launch itself. The object in the submission field contains the request body that's normally sent to the POST /terrain/analyses endpoint when launching a job. Note that not all required analysis parameters of the AnalysisSubmissionConfig need to be specified in the Quick Launch. Once a Quick Launch is defined, the GET /terrain/quicklaunches endpoint can be used to list Quick Launches that are available to the user. The description currently says that it lists Quick Launches that were created by the user. This isn\u2019t quite correct. Any Quick Launch that the user has permission to view will be listed by this endpoint. All available Quick Launches for an app can be listed using the GET /terrain/quicklaunches/apps/{app-id} endpoint. This endpoint will only list Quick Launches that the user has permission to view. Quick Launch defaults allow the user to specify which Quick Launch to use for a specific app. Both global defaults and user-specific defaults can be defined. The app launch info saved in a Quick Launch can be retrieved with the GET /terrain/quicklaunches/{ql-id}/app-info endpoint. This is analogous to calling the GET /terrain/apps/{system-id}/{app-id} endpoint.","title":"Quick Launches"},{"location":"services/api/endpoints/terrain-v-apps/","text":"Overview \u00b6 The \"secured\" endpoints in apps behave a little bit differently from the secured endpoints in Terrain. Specifically, the secured endpoints in Terrain actually require user authentication whereas those in apps merely require information about the user making the request. Calling Secured Terrain Endpoints \u00b6 All secured endpoints in Terrain require an HTTP header containing a JSON Web Token (JWT). The specific header to use depends on the format of the JWT, but in most cases it will be X-Iplant-De-Jwt . Please see the root API documentation page for details. Calling \"Secured\" Apps Endpoints \u00b6 None of these endpoints are actually secured. They retain the \"secured\" label because they're fronted by endpoints in Terrain that are secure. That is, the user interface doesn't hit the apps services directly. Instead, it sends the request to a Terrain endpoint that forwards the request to a corresponding endpoint in apps. The initial purpose of this separation was to provide scalability and separation of concerns. All secured endpoints in apps support four query-string parameters containing user attributes: user - the username email - the user's email address first-name - the user's first name last-name - the user's last name Not all of these parameters are required by every secured service in apps; the only parameter that is required by all secured services is user . The rest of the parameters are only required when the information in them is specifically required. Terrain always passes all of these parameters to apps when it forwards requests. A Simple Example \u00b6 One of the simplest endpoints in both Terrain and apps is the /apps/categories endpoint, which is used to obtain the list of app categories that are visible to the user. The call to the Terrain service would look like this: $ curl -H \"$AUTH_HEADER\" \"http://by-tor:8888/apps/categories\" The equivalent call to the apps service would look like this: $ curl \"http://by-tor:9999/apps/categories?user=nobody&email=nobody@iplantcollaborative.org&first-name=Nobody&last-name=Inparticular\"","title":"Overview"},{"location":"services/api/endpoints/terrain-v-apps/#overview","text":"The \"secured\" endpoints in apps behave a little bit differently from the secured endpoints in Terrain. Specifically, the secured endpoints in Terrain actually require user authentication whereas those in apps merely require information about the user making the request.","title":"Overview"},{"location":"services/api/endpoints/terrain-v-apps/#calling-secured-terrain-endpoints","text":"All secured endpoints in Terrain require an HTTP header containing a JSON Web Token (JWT). The specific header to use depends on the format of the JWT, but in most cases it will be X-Iplant-De-Jwt . Please see the root API documentation page for details.","title":"Calling Secured Terrain Endpoints"},{"location":"services/api/endpoints/terrain-v-apps/#calling-secured-apps-endpoints","text":"None of these endpoints are actually secured. They retain the \"secured\" label because they're fronted by endpoints in Terrain that are secure. That is, the user interface doesn't hit the apps services directly. Instead, it sends the request to a Terrain endpoint that forwards the request to a corresponding endpoint in apps. The initial purpose of this separation was to provide scalability and separation of concerns. All secured endpoints in apps support four query-string parameters containing user attributes: user - the username email - the user's email address first-name - the user's first name last-name - the user's last name Not all of these parameters are required by every secured service in apps; the only parameter that is required by all secured services is user . The rest of the parameters are only required when the information in them is specifically required. Terrain always passes all of these parameters to apps when it forwards requests.","title":"Calling \"Secured\" Apps Endpoints"},{"location":"services/api/endpoints/terrain-v-apps/#a-simple-example","text":"One of the simplest endpoints in both Terrain and apps is the /apps/categories endpoint, which is used to obtain the list of app categories that are visible to the user. The call to the Terrain service would look like this: $ curl -H \"$AUTH_HEADER\" \"http://by-tor:8888/apps/categories\" The equivalent call to the apps service would look like this: $ curl \"http://by-tor:9999/apps/categories?user=nobody&email=nobody@iplantcollaborative.org&first-name=Nobody&last-name=Inparticular\"","title":"A Simple Example"},{"location":"services/api/endpoints/filesystem/coge/","text":"Viewing a Genome File in CoGe \u00b6 A genome file may be submitted to CoGe for viewing within their genome viewer. This endpoint will share the given genome files with the CoGe user, then submit those paths to their \"genome load\" service. This service will return a URL where the authenticated user may view the genome viewer's progress, and the genome visualization once processing is done. URL Path : /coge/genomes/load HTTP Method : POST Error Codes : ERR_NOT_A_USER, ERR_BAD_OR_MISSING_FIELD, ERR_DOES_NOT_EXIST, ERR_NOT_OWNER Request Query Parameters : Request Body : { \"paths\": [ \"/iplant/home/ipctest/simple.fasta\" ] } Response Body : { \"coge_genome_url\": \"http://bit.ly/CoGeSample\" } Curl Command : curl -sH \"$AUTH_HEADER\" -d '{\"paths\":[\"/iplant/home/ipctest/simple.fasta\"]}' http://127.0.0.1:3000/coge/genomes/load Searching for Genomes in CoGe \u00b6 A user may search for genome information in CoGe in order to retrieve a text representation of that genome for use in DE analyses. URL Path : /coge/genomes HTTP Method : GET Error Codes : ERR_REQUEST_FAILED Request Query Parameters : search - The string to search for. Response Body : { \"genomes\": [ { \"chromosome_count\": 35, \"description\": null, \"id\": 9716, \"info\": \"Actinomyces coleocanis strain DSM 15436 (v1, id9716): unmasked\", \"link\": null, \"name\": null, \"organism\": { \"description\": \"Bacteria; Actinobacteria; Actinobacteridae; Actinomycetales; Actinomycineae; Actinomycetaceae; Actinomyces\", \"id\": 23534, \"name\": \"Actinomyces coleocanis strain DSM 15436\" }, \"organism_id\": 23534, \"restricted\": false, \"sequence_type\": { \"description\": \"unmasked sequence data\", \"id\": \"1\", \"name\": \"unmasked\" }, \"version\": \"1\" }, ... } } Note: the response body from CoGe is passed back to the caller without modification. Curl Command : $ curl -sH \"$AUTH_HEADER\" \"http://127.0.0.1:3000/coge/genomes?search=canis\" | python -mjson.tool Exporting CoGe Genome Data to iRODS \u00b6 Once a user has found an interesting genome, he or she may request a text representation of the genome to be stored in the iPlant Data Store for processing in the DE. URL Path : /coge/genomes/{genome-id}/export-fasta HTTP Method : POST Error Codes : ERR_REQUEST_FAILED Request Query Parameters : notify - If present and set to \"true\" the user's email address will be forwarded to CoGe. overwrite - If present and set to \"true\" the output file will be overwritten if it exists. Response Body : { \"id\": 30702, \"success\": true } Note: the response body from CoGe is passed back to the caller without modification.","title":"Coge"},{"location":"services/api/endpoints/filesystem/coge/#viewing-a-genome-file-in-coge","text":"A genome file may be submitted to CoGe for viewing within their genome viewer. This endpoint will share the given genome files with the CoGe user, then submit those paths to their \"genome load\" service. This service will return a URL where the authenticated user may view the genome viewer's progress, and the genome visualization once processing is done. URL Path : /coge/genomes/load HTTP Method : POST Error Codes : ERR_NOT_A_USER, ERR_BAD_OR_MISSING_FIELD, ERR_DOES_NOT_EXIST, ERR_NOT_OWNER Request Query Parameters : Request Body : { \"paths\": [ \"/iplant/home/ipctest/simple.fasta\" ] } Response Body : { \"coge_genome_url\": \"http://bit.ly/CoGeSample\" } Curl Command : curl -sH \"$AUTH_HEADER\" -d '{\"paths\":[\"/iplant/home/ipctest/simple.fasta\"]}' http://127.0.0.1:3000/coge/genomes/load","title":"Viewing a Genome File in CoGe"},{"location":"services/api/endpoints/filesystem/coge/#searching-for-genomes-in-coge","text":"A user may search for genome information in CoGe in order to retrieve a text representation of that genome for use in DE analyses. URL Path : /coge/genomes HTTP Method : GET Error Codes : ERR_REQUEST_FAILED Request Query Parameters : search - The string to search for. Response Body : { \"genomes\": [ { \"chromosome_count\": 35, \"description\": null, \"id\": 9716, \"info\": \"Actinomyces coleocanis strain DSM 15436 (v1, id9716): unmasked\", \"link\": null, \"name\": null, \"organism\": { \"description\": \"Bacteria; Actinobacteria; Actinobacteridae; Actinomycetales; Actinomycineae; Actinomycetaceae; Actinomyces\", \"id\": 23534, \"name\": \"Actinomyces coleocanis strain DSM 15436\" }, \"organism_id\": 23534, \"restricted\": false, \"sequence_type\": { \"description\": \"unmasked sequence data\", \"id\": \"1\", \"name\": \"unmasked\" }, \"version\": \"1\" }, ... } } Note: the response body from CoGe is passed back to the caller without modification. Curl Command : $ curl -sH \"$AUTH_HEADER\" \"http://127.0.0.1:3000/coge/genomes?search=canis\" | python -mjson.tool","title":"Searching for Genomes in CoGe"},{"location":"services/api/endpoints/filesystem/coge/#exporting-coge-genome-data-to-irods","text":"Once a user has found an interesting genome, he or she may request a text representation of the genome to be stored in the iPlant Data Store for processing in the DE. URL Path : /coge/genomes/{genome-id}/export-fasta HTTP Method : POST Error Codes : ERR_REQUEST_FAILED Request Query Parameters : notify - If present and set to \"true\" the user's email address will be forwarded to CoGe. overwrite - If present and set to \"true\" the output file will be overwritten if it exists. Response Body : { \"id\": 30702, \"success\": true } Note: the response body from CoGe is passed back to the caller without modification.","title":"Exporting CoGe Genome Data to iRODS"},{"location":"services/api/endpoints/filesystem/csv-tsv-parsing/","text":"CSV/TSV Parsing \u00b6 URL Path : /secured/filesystem/read-csv-chunk HTTP Method : POST Request Query Parameters : Request Body : { \"path\" : \"/iplant/home/testuser/test-tsv\", \"separator\" : \"%09\", \"page\" : \"4\", \"chunk-size\" : \"400\" } Response Body : { \"chunk-size\": \"238\", \"csv\": [ { \"0\": \"Number of mismatches allowed in each segment alignment for reads mapped independently\", \"1\": \"Integer\", \"2\": \"2\" }, { \"0\": \"Minimum length of read segments\", \"1\": \"Integer\", \"2\": \"20\" }, { \"0\": \"Number of threads\", \"1\": \"Integer\", \"2\": \"4\" }, { \"0\": \"Tophat Version\", \"1\": \"TextSelection\", \"2\": \"1.4.1\" }, { \"0\": \"Bowtie Version\", \"1\": \"TextSelection\", \"2\": \"0.12.7\" } ], \"file-size\": \"1041\", \"page\" : \"4\", \"max-cols\": \"3\", \"number-pages\": \"3\", \"path\": \"/iplant/home/testuser/test-tsv\", \"user\": \"wregglej\" } CURL Command : curl -H \"$AUTH_HEADER\" -d '{\"path\" : \"/iplant/home/testuser/test-tsv\", \"separator\" : \"%09\", \"page\" : \"4\", \"chunk-size\" : \"400\"}' http://127.0.0.1:31325/secured/filesystem/read-csv-chunk JSON Field Descriptions : path is the path to the file in iRODS that should be parsed as a CSV. Note that there isn't any checking in place to make sure that the file is actually a CSV or TSV. This is because we can't depend on the filetype detection to detect all possible types of CSV files (i.e. tab-delimited, pipe-delimited, hash-delimited, etc.). page is the page to start at in the file. Your first request should be at page 0 for any file so you can get back the maximum number of pages in the number-pages field. chunk-size is the size to use when calculating the number of pages that a file contains. This should be in bytes. separator is a single character that the CSV parser uses to split fields. Common values are \",\" and \"\\t\". We don't do any validation on this field so that we can support a wider-range of parsing options. The only constraints on this field is that it needs to be readable as a single char and it must be URL encoded. Notes and Limitations : This should work fine with files that use \\r\\n as the line ending, but it will not work correctly with files that use \\r alone. It that case every page returned will be blank. If the chunk-size is set so that it doesn't cover an entire line, a blank page will be returned. The code that trims and resizes the pages to end on line breaks will detect '\\n' that are embedded in double quoted cells as a line break. This is because we're not tracking opening and closing double quotes across pages. We're looking into ways of doing this, but it isn't in yet. The URL encoded value for \\t characters is '%09', without the quotes. If you aren't sending '%09' for the separator when you're trying to parse a TSV, then you're going to have a bad time.","title":"Csv tsv parsing"},{"location":"services/api/endpoints/filesystem/csv-tsv-parsing/#csvtsv-parsing","text":"URL Path : /secured/filesystem/read-csv-chunk HTTP Method : POST Request Query Parameters : Request Body : { \"path\" : \"/iplant/home/testuser/test-tsv\", \"separator\" : \"%09\", \"page\" : \"4\", \"chunk-size\" : \"400\" } Response Body : { \"chunk-size\": \"238\", \"csv\": [ { \"0\": \"Number of mismatches allowed in each segment alignment for reads mapped independently\", \"1\": \"Integer\", \"2\": \"2\" }, { \"0\": \"Minimum length of read segments\", \"1\": \"Integer\", \"2\": \"20\" }, { \"0\": \"Number of threads\", \"1\": \"Integer\", \"2\": \"4\" }, { \"0\": \"Tophat Version\", \"1\": \"TextSelection\", \"2\": \"1.4.1\" }, { \"0\": \"Bowtie Version\", \"1\": \"TextSelection\", \"2\": \"0.12.7\" } ], \"file-size\": \"1041\", \"page\" : \"4\", \"max-cols\": \"3\", \"number-pages\": \"3\", \"path\": \"/iplant/home/testuser/test-tsv\", \"user\": \"wregglej\" } CURL Command : curl -H \"$AUTH_HEADER\" -d '{\"path\" : \"/iplant/home/testuser/test-tsv\", \"separator\" : \"%09\", \"page\" : \"4\", \"chunk-size\" : \"400\"}' http://127.0.0.1:31325/secured/filesystem/read-csv-chunk JSON Field Descriptions : path is the path to the file in iRODS that should be parsed as a CSV. Note that there isn't any checking in place to make sure that the file is actually a CSV or TSV. This is because we can't depend on the filetype detection to detect all possible types of CSV files (i.e. tab-delimited, pipe-delimited, hash-delimited, etc.). page is the page to start at in the file. Your first request should be at page 0 for any file so you can get back the maximum number of pages in the number-pages field. chunk-size is the size to use when calculating the number of pages that a file contains. This should be in bytes. separator is a single character that the CSV parser uses to split fields. Common values are \",\" and \"\\t\". We don't do any validation on this field so that we can support a wider-range of parsing options. The only constraints on this field is that it needs to be readable as a single char and it must be URL encoded. Notes and Limitations : This should work fine with files that use \\r\\n as the line ending, but it will not work correctly with files that use \\r alone. It that case every page returned will be blank. If the chunk-size is set so that it doesn't cover an entire line, a blank page will be returned. The code that trims and resizes the pages to end on line breaks will detect '\\n' that are embedded in double quoted cells as a line break. This is because we're not tracking opening and closing double quotes across pages. We're looking into ways of doing this, but it isn't in yet. The URL encoded value for \\t characters is '%09', without the quotes. If you aren't sending '%09' for the separator when you're trying to parse a TSV, then you're going to have a bad time.","title":"CSV/TSV Parsing"},{"location":"services/api/endpoints/filesystem/delete/","text":"Deleting Files and/or Directories \u00b6 URL Path : /secured/filesystem/delete HTTP Method : POST Action : \"delete\" Error Codes : ERR_NOT_A_FOLDER, ERR_DOES_NOT_EXIST, ERR_NOT_WRITEABLE, ERR_TOO_MANY_PATHS, ERR_NOT_A_USER The ERR_TOO_MANY_PATHS error code is returned when all of the \"paths\" and the items and sub-directories under them exceed the maximum number of paths that can be processed by this endpoint. Request Parameters : Request Body : { \"paths\" : [\"/tempZone/home/rods/test2\"] } \"paths\" can take a mix of files and directories. Response : { \"paths\":[\"/tempZone/home/rods/test2\"] } Curl Command : curl -H \"$AUTH_HEADER\" -H \"Content-Type:application/json\" -d '{\"paths\" : [\"/tempZone/home/rods/test2\"]}' http://127.0.0.1:3000/secured/filesystem/delete?user=rods Deleting all items in a Directory \u00b6 URL Path : /secured/filesystem/delete-contents HTTP Method : POST Action : \"delete\" Error codes : ERR_NOT_A_FOLDER, ERR_DOES_NOT_EXIST, ERR_NOT_WRITEABLE, ERR_TOO_MANY_PATHS, ERR_NOT_A_USER The ERR_TOO_MANY_PATHS error code is returned when all items in the source directory and its sub-directories exceed the maximum number of paths that can be processed by this endpoint. Request Query Parameters : Request Body : { \"path\": \"/tempZone/home/rods/test\" } Response : { \"paths\":[ \"/tempZone/home/rods/test/test1\", \"/tempZone/home/rods/test/test2\" ] } Example ERR_TOO_MANY_PATHS Error Response : { \"error_code\": \"ERR_TOO_MANY_PATHS\", \"count\": 250, \"limit\": 100 } Curl Command : curl -H \"$AUTH_HEADER\" -H \"Content-Type:application/json\" -d '{\"path\": \"/tempZone/home/rods/test\"}' http://127.0.0.1:3000/secured/filesystem/delete-contents","title":"Delete"},{"location":"services/api/endpoints/filesystem/delete/#deleting-files-andor-directories","text":"URL Path : /secured/filesystem/delete HTTP Method : POST Action : \"delete\" Error Codes : ERR_NOT_A_FOLDER, ERR_DOES_NOT_EXIST, ERR_NOT_WRITEABLE, ERR_TOO_MANY_PATHS, ERR_NOT_A_USER The ERR_TOO_MANY_PATHS error code is returned when all of the \"paths\" and the items and sub-directories under them exceed the maximum number of paths that can be processed by this endpoint. Request Parameters : Request Body : { \"paths\" : [\"/tempZone/home/rods/test2\"] } \"paths\" can take a mix of files and directories. Response : { \"paths\":[\"/tempZone/home/rods/test2\"] } Curl Command : curl -H \"$AUTH_HEADER\" -H \"Content-Type:application/json\" -d '{\"paths\" : [\"/tempZone/home/rods/test2\"]}' http://127.0.0.1:3000/secured/filesystem/delete?user=rods","title":"Deleting Files and/or Directories"},{"location":"services/api/endpoints/filesystem/delete/#deleting-all-items-in-a-directory","text":"URL Path : /secured/filesystem/delete-contents HTTP Method : POST Action : \"delete\" Error codes : ERR_NOT_A_FOLDER, ERR_DOES_NOT_EXIST, ERR_NOT_WRITEABLE, ERR_TOO_MANY_PATHS, ERR_NOT_A_USER The ERR_TOO_MANY_PATHS error code is returned when all items in the source directory and its sub-directories exceed the maximum number of paths that can be processed by this endpoint. Request Query Parameters : Request Body : { \"path\": \"/tempZone/home/rods/test\" } Response : { \"paths\":[ \"/tempZone/home/rods/test/test1\", \"/tempZone/home/rods/test/test2\" ] } Example ERR_TOO_MANY_PATHS Error Response : { \"error_code\": \"ERR_TOO_MANY_PATHS\", \"count\": 250, \"limit\": 100 } Curl Command : curl -H \"$AUTH_HEADER\" -H \"Content-Type:application/json\" -d '{\"path\": \"/tempZone/home/rods/test\"}' http://127.0.0.1:3000/secured/filesystem/delete-contents","title":"Deleting all items in a Directory"},{"location":"services/api/endpoints/filesystem/directory-create/","text":"Batch Directory Creation \u00b6 Secured Endpoint: POST /secured/filesystem/directories Delegates to data-info: POST /data/directories This endpoint is a passthrough to the data-info endpoint above. Please see the data-info documentation for more information. Directory Creation \u00b6 URL Path : /secured/filesystem/directory/create HTTP Method : POST Error Codes : ERR_BAD_OR_MISSING_FIELD, ERR_NOT_WRITEABLE, ERR_EXISTS, ERR_DOES_NOT_EXIST, ERR_NOT_A_USER Request Body : { \"path\" : \"/tempZone/home/rods/test3\" } Response Body : This endpoint uses a similar response as the /secured/filesystem/stat endpoint. For example: { \"id\" : \"/tempZone/home/rods/test3\" , \"path\" : \"/tempZone/home/rods/test3\" , \"label\" : \"test3\" , \"type\" : \"dir\" , \"date-modified\" : 1397063483000 , \"date-created\" : 1397063483000 , \"permission\" : \"own\" , \"share-count\" : 0 , \"dir-count\" : 0 , \"file-count\" : 0 } Curl Command : curl -H \"$AUTH_HEADER\" -H \"Content-Type:application/json\" -d '{\"path\" : \"/tempZone/home/rods/test3\"}' \"http://127.0.0.1:3000/secured/filesystem/directory/create\"","title":"Directory create"},{"location":"services/api/endpoints/filesystem/directory-create/#batch-directory-creation","text":"Secured Endpoint: POST /secured/filesystem/directories Delegates to data-info: POST /data/directories This endpoint is a passthrough to the data-info endpoint above. Please see the data-info documentation for more information.","title":"Batch Directory Creation"},{"location":"services/api/endpoints/filesystem/directory-create/#directory-creation","text":"URL Path : /secured/filesystem/directory/create HTTP Method : POST Error Codes : ERR_BAD_OR_MISSING_FIELD, ERR_NOT_WRITEABLE, ERR_EXISTS, ERR_DOES_NOT_EXIST, ERR_NOT_A_USER Request Body : { \"path\" : \"/tempZone/home/rods/test3\" } Response Body : This endpoint uses a similar response as the /secured/filesystem/stat endpoint. For example: { \"id\" : \"/tempZone/home/rods/test3\" , \"path\" : \"/tempZone/home/rods/test3\" , \"label\" : \"test3\" , \"type\" : \"dir\" , \"date-modified\" : 1397063483000 , \"date-created\" : 1397063483000 , \"permission\" : \"own\" , \"share-count\" : 0 , \"dir-count\" : 0 , \"file-count\" : 0 } Curl Command : curl -H \"$AUTH_HEADER\" -H \"Content-Type:application/json\" -d '{\"path\" : \"/tempZone/home/rods/test3\"}' \"http://127.0.0.1:3000/secured/filesystem/directory/create\"","title":"Directory Creation"},{"location":"services/api/endpoints/filesystem/directory-listing/","text":"Directory List (Non-Recursive) \u00b6 Only lists subdirectories of the directory path passed into it. Delegates to the POST /favorites/filter metadata endpoint in order to set the isFavorite flags in the response. If the metadata service is not available, then these flags will be set to false by default. URL Path : /secured/filesystem/directory HTTP Method : GET Error Codes : ERR_NOT_A_USER, ERR_NOT_READABLE Request Query Params : path - The path to list. Optional. If ommitted, the user's home directory is used. Response Body : { \"date-created\": 1369778522000, \"date-modified\": 1381177547000, \"file-size\": 0, \"folders\": [ { \"date-created\": 1373927956000, \"date-modified\": 1374015533000, \"file-size\": 0, \"badName\": false, \"hasSubDirs\": true, \"path\": \"/iplant/home/wregglej/acsxfdqswfrdafds\", \"label\": \"acsxfdqswfrdafds\", \"isFavorite\" : false, \"id\": \"0c3eb574-df8a-11e3-bfa5-6abdce5a08d5\", \"permission\": \"own\" }, { \"date-created\": 1371157127000, \"date-modified\": 1380909580000, \"file-size\": 0, \"badName\": false, \"hasSubDirs\": true, \"path\": \"/iplant/home/wregglej/analyses\", \"label\": \"analyses\", \"isFavorite\" : false, \"id\": \"1c2c436c-e128-11e3-9087-6abdce5a08d5\", \"permission\": \"own\" }, { \"date-created\": 1380814985000, \"date-modified\": 1380814985000, \"file-size\": 0, \"badName\": false, \"hasSubDirs\": true, \"path\": \"/iplant/home/wregglej/analyses3\", \"label\": \"analyses3\", \"isFavorite\" : false, \"id\": \"1f293516-e128-11e3-9087-6abdce5a08d5\", \"permission\": \"own\" }, ], \"hasSubDirs\": true, \"id\": \"a3794158-df89-11e3-bf7d-6abdce5a08d5\", \"path\": \"/iplant/home/wregglej\", \"label\": \"wregglej\", \"isFavorite\" : false, \"badName\": true, \"permission\": \"own\" } Curl Command : curl -H \"$AUTH_HEADER\" http://127.0.0.1:3000/secured/filesystem/directory Paged Directory Listing \u00b6 Provides a paged directory listing for large directories. Always includes files (unless the directory doesn't contain any). Delegates to the POST /favorites/filter metadata endpoint in order to set the isFavorite flags in the response. If the metadata service is not available, then these flags will be set to false by default. URL Path : /secured/filesystem/paged-directory HTTP Method : GET Error Codes : ERR_NOT_A_USER ERR_NOT_READABLE ERR_NOT_A_FOLDER ERR_NOT_READABLE ERR_DOES_NOT_EXIST ERR_INVALID_SORT_COLUMN ERR_INVALID_SORT_ORDER Request Query Params : path - The path to list. Must be a directory. limit - The total number of results to return in a page. This is the number of folders and files combined. offset - The offset into the directory listing result set to begin the listing at. entity-type - (OPTIONAL) The type of entity to return, FILE, FOLDER, or ANY. The values are case-insensitive. It defaults to ANY. info-type - (OPTIONAL) Filter the files portion of the result set so that only files with this info type are returned. To return multiple info types, and this parameter more than once. sort-col - The column to sort the result set by. Sorting is done in iRODS's ICAT database, not at the application level. Accepted values are NAME, ID, LASTMODIFIED, DATECREATED, SIZE, PATH. The values are case-insensitive. sort-dir - The order to sort the result set in. Accepted values are ASC and DESC. The values are case-insensitive. Response Body : { \"badName\": false, \"date-created\": 1369778522000, \"date-modified\": 1379520049000, \"file-size\": 0, \"files\": [ { \"badName\": false, \"date-created\": 1379519492000, \"date-modified\": 1379520049000, \"file-size\": 196903039, \"id\": \"0d880c78-df8a-11e3-bfa5-6abdce5a08d5\", \"infoType\": null, \"path\": \"/iplant/home/wregglej/centos-5.8-x86-64-minimal.box\", \"label\": \"centos-5.8-x86-64-minimal.box\", \"isFavorite\" : false, \"permission\": \"own\" } ], \"folders\": [ { \"badName\": false, \"date-created\": 1374080225000, \"date-modified\": 1374080225000, \"file-size\": 0, \"hasSubDirs\": true, \"id\": \"6375efce-e061-11e3-bfa5-6abdce5a08d5\", \"infoType\": null, \"path\": \"/iplant/home/wregglej/asdfafa\", \"label\": \"asdfafa\", \"isFavorite\" : false, \"permission\": \"own\" }, { \"badName\": false, \"date-created\": 1377814242000, \"date-modified\": 1377814242000, \"file-size\": 0, \"hasSubDirs\": true, \"id\": \"b4987bf4-e063-11e3-bfa5-6abdce5a08d5\", \"infoType\": null, \"path\": \"/iplant/home/wregglej/asdf bar\", \"label\": \"asdf bar\", \"isFavorite\" : false, \"permission\": \"own\" }, { \"badName\": false, \"date-created\": 1373397344000, \"date-modified\": 1377558112000, \"file-size\": 0, \"hasSubDirs\": true, \"id\" : \"0d622cd8-df8a-11e3-bfa5-6abdce5a08d5\", \"infoType\": null, \"path\": \"/iplant/home/wregglej/Find_Unique_Values_analysis1-2013-07-09-12-15-37.024\", \"label\": \"Find_Unique_Values_analysis1-2013-07-09-12-15-37.024\", \"isFavorite\" : false, \"permission\": \"own\" }, { \"badName\": false, \"date-created\": 1374080529000, \"date-modified\": 1374080529000, \"file-size\": 0, \"hasSubDirs\": true, \"id\": \"0d627292-df8a-11e3-bfa5-6abdce5a08d5\", \"infoType\": null, \"path\": \"/iplant/home/wregglej/zaaaaaaaa\", \"label\": \"zaaaaaaaa\", \"isFavorite\" : false, \"permission\": \"own\" } ], \"hasSubDirs\": true, \"id\": \"16426b48-e128-11e3-9076-6abdce5a08d5\", \"infoType\": null, \"path\": \"/iplant/home/wregglej\", \"label\": \"wregglej\", \"isFavorite\" : false, \"permission\": \"own\", \"total\": 218, \"totalBad\": 0 } Curl Command : curl -H \"$AUTH_HEADER\" \"http://127.0.0.1:31325/secured/filesystem/paged-directory?path=/iplant/home/wregglej&sort-col=SIZE&sort-dir=DESC&limit=5&offset=10\"","title":"Directory listing"},{"location":"services/api/endpoints/filesystem/directory-listing/#directory-list-non-recursive","text":"Only lists subdirectories of the directory path passed into it. Delegates to the POST /favorites/filter metadata endpoint in order to set the isFavorite flags in the response. If the metadata service is not available, then these flags will be set to false by default. URL Path : /secured/filesystem/directory HTTP Method : GET Error Codes : ERR_NOT_A_USER, ERR_NOT_READABLE Request Query Params : path - The path to list. Optional. If ommitted, the user's home directory is used. Response Body : { \"date-created\": 1369778522000, \"date-modified\": 1381177547000, \"file-size\": 0, \"folders\": [ { \"date-created\": 1373927956000, \"date-modified\": 1374015533000, \"file-size\": 0, \"badName\": false, \"hasSubDirs\": true, \"path\": \"/iplant/home/wregglej/acsxfdqswfrdafds\", \"label\": \"acsxfdqswfrdafds\", \"isFavorite\" : false, \"id\": \"0c3eb574-df8a-11e3-bfa5-6abdce5a08d5\", \"permission\": \"own\" }, { \"date-created\": 1371157127000, \"date-modified\": 1380909580000, \"file-size\": 0, \"badName\": false, \"hasSubDirs\": true, \"path\": \"/iplant/home/wregglej/analyses\", \"label\": \"analyses\", \"isFavorite\" : false, \"id\": \"1c2c436c-e128-11e3-9087-6abdce5a08d5\", \"permission\": \"own\" }, { \"date-created\": 1380814985000, \"date-modified\": 1380814985000, \"file-size\": 0, \"badName\": false, \"hasSubDirs\": true, \"path\": \"/iplant/home/wregglej/analyses3\", \"label\": \"analyses3\", \"isFavorite\" : false, \"id\": \"1f293516-e128-11e3-9087-6abdce5a08d5\", \"permission\": \"own\" }, ], \"hasSubDirs\": true, \"id\": \"a3794158-df89-11e3-bf7d-6abdce5a08d5\", \"path\": \"/iplant/home/wregglej\", \"label\": \"wregglej\", \"isFavorite\" : false, \"badName\": true, \"permission\": \"own\" } Curl Command : curl -H \"$AUTH_HEADER\" http://127.0.0.1:3000/secured/filesystem/directory","title":"Directory List (Non-Recursive)"},{"location":"services/api/endpoints/filesystem/directory-listing/#paged-directory-listing","text":"Provides a paged directory listing for large directories. Always includes files (unless the directory doesn't contain any). Delegates to the POST /favorites/filter metadata endpoint in order to set the isFavorite flags in the response. If the metadata service is not available, then these flags will be set to false by default. URL Path : /secured/filesystem/paged-directory HTTP Method : GET Error Codes : ERR_NOT_A_USER ERR_NOT_READABLE ERR_NOT_A_FOLDER ERR_NOT_READABLE ERR_DOES_NOT_EXIST ERR_INVALID_SORT_COLUMN ERR_INVALID_SORT_ORDER Request Query Params : path - The path to list. Must be a directory. limit - The total number of results to return in a page. This is the number of folders and files combined. offset - The offset into the directory listing result set to begin the listing at. entity-type - (OPTIONAL) The type of entity to return, FILE, FOLDER, or ANY. The values are case-insensitive. It defaults to ANY. info-type - (OPTIONAL) Filter the files portion of the result set so that only files with this info type are returned. To return multiple info types, and this parameter more than once. sort-col - The column to sort the result set by. Sorting is done in iRODS's ICAT database, not at the application level. Accepted values are NAME, ID, LASTMODIFIED, DATECREATED, SIZE, PATH. The values are case-insensitive. sort-dir - The order to sort the result set in. Accepted values are ASC and DESC. The values are case-insensitive. Response Body : { \"badName\": false, \"date-created\": 1369778522000, \"date-modified\": 1379520049000, \"file-size\": 0, \"files\": [ { \"badName\": false, \"date-created\": 1379519492000, \"date-modified\": 1379520049000, \"file-size\": 196903039, \"id\": \"0d880c78-df8a-11e3-bfa5-6abdce5a08d5\", \"infoType\": null, \"path\": \"/iplant/home/wregglej/centos-5.8-x86-64-minimal.box\", \"label\": \"centos-5.8-x86-64-minimal.box\", \"isFavorite\" : false, \"permission\": \"own\" } ], \"folders\": [ { \"badName\": false, \"date-created\": 1374080225000, \"date-modified\": 1374080225000, \"file-size\": 0, \"hasSubDirs\": true, \"id\": \"6375efce-e061-11e3-bfa5-6abdce5a08d5\", \"infoType\": null, \"path\": \"/iplant/home/wregglej/asdfafa\", \"label\": \"asdfafa\", \"isFavorite\" : false, \"permission\": \"own\" }, { \"badName\": false, \"date-created\": 1377814242000, \"date-modified\": 1377814242000, \"file-size\": 0, \"hasSubDirs\": true, \"id\": \"b4987bf4-e063-11e3-bfa5-6abdce5a08d5\", \"infoType\": null, \"path\": \"/iplant/home/wregglej/asdf bar\", \"label\": \"asdf bar\", \"isFavorite\" : false, \"permission\": \"own\" }, { \"badName\": false, \"date-created\": 1373397344000, \"date-modified\": 1377558112000, \"file-size\": 0, \"hasSubDirs\": true, \"id\" : \"0d622cd8-df8a-11e3-bfa5-6abdce5a08d5\", \"infoType\": null, \"path\": \"/iplant/home/wregglej/Find_Unique_Values_analysis1-2013-07-09-12-15-37.024\", \"label\": \"Find_Unique_Values_analysis1-2013-07-09-12-15-37.024\", \"isFavorite\" : false, \"permission\": \"own\" }, { \"badName\": false, \"date-created\": 1374080529000, \"date-modified\": 1374080529000, \"file-size\": 0, \"hasSubDirs\": true, \"id\": \"0d627292-df8a-11e3-bfa5-6abdce5a08d5\", \"infoType\": null, \"path\": \"/iplant/home/wregglej/zaaaaaaaa\", \"label\": \"zaaaaaaaa\", \"isFavorite\" : false, \"permission\": \"own\" } ], \"hasSubDirs\": true, \"id\": \"16426b48-e128-11e3-9076-6abdce5a08d5\", \"infoType\": null, \"path\": \"/iplant/home/wregglej\", \"label\": \"wregglej\", \"isFavorite\" : false, \"permission\": \"own\", \"total\": 218, \"totalBad\": 0 } Curl Command : curl -H \"$AUTH_HEADER\" \"http://127.0.0.1:31325/secured/filesystem/paged-directory?path=/iplant/home/wregglej&sort-col=SIZE&sort-dir=DESC&limit=5&offset=10\"","title":"Paged Directory Listing"},{"location":"services/api/endpoints/filesystem/empty-trash/","text":"Emptying a User's Trash Directory \u00b6 URL Path : /secured/filesystem/trash HTTP Method : DELETE Error Codes : ERR_NOT_A_USER Request Query Parameters : Response : { \"trash\" : \"/path/to/user's/trash/dir/\", \"paths\" : [ \"/path/to/deleted/file\", ] } Curl Command : curl -H \"$AUTH_HEADER\" -X DELETE http://127.0.0.1:3000/secured/filesystem/trash","title":"Empty trash"},{"location":"services/api/endpoints/filesystem/empty-trash/#emptying-a-users-trash-directory","text":"URL Path : /secured/filesystem/trash HTTP Method : DELETE Error Codes : ERR_NOT_A_USER Request Query Parameters : Response : { \"trash\" : \"/path/to/user's/trash/dir/\", \"paths\" : [ \"/path/to/deleted/file\", ] } Curl Command : curl -H \"$AUTH_HEADER\" -X DELETE http://127.0.0.1:3000/secured/filesystem/trash","title":"Emptying a User's Trash Directory"},{"location":"services/api/endpoints/filesystem/errors/","text":"Error Codes \u00b6 When it encounters an error, filesystem will generally return a JSON object in the form: { \"error_code\" : \"ERR_CODE\" } Other entries may be included in the map, but you shouldn't depend on them being there for error checking.","title":"Errors"},{"location":"services/api/endpoints/filesystem/errors/#error-codes","text":"When it encounters an error, filesystem will generally return a JSON object in the form: { \"error_code\" : \"ERR_CODE\" } Other entries may be included in the map, but you shouldn't depend on them being there for error checking.","title":"Error Codes"},{"location":"services/api/endpoints/filesystem/existence/","text":"File/Directory existence \u00b6 The /exists endpoint allows the caller to check for the existence of a set of files. The following is an example call to the exists endpoint: URL Path : /secured/filesystem/exists HTTP Method : POST Error Codes : ERR_NOT_A_USER Request Query Parameters : Request Body : { \"paths\" : [ \"/iplant/home/wregglej/pom.xml\", \"/iplant/home/wregglej/pom.xml2\" ] } Response Body : { \"paths\":{ \"/iplant/home/wregglej/pom.xml2\":false, \"/iplant/home/wregglej/pom.xml\":false } } Curl Command : curl -H \"$AUTH_HEADER\" -H \"Content-type:application/json\" -d '{\"paths\" : [\"/iplant/home/wregglej/pom.xml\", \"/iplant/home/wregglej/pom.xml2\"]}' 'http://127.0.0.1:3000/secured/filesystem/exists","title":"Existence"},{"location":"services/api/endpoints/filesystem/existence/#filedirectory-existence","text":"The /exists endpoint allows the caller to check for the existence of a set of files. The following is an example call to the exists endpoint: URL Path : /secured/filesystem/exists HTTP Method : POST Error Codes : ERR_NOT_A_USER Request Query Parameters : Request Body : { \"paths\" : [ \"/iplant/home/wregglej/pom.xml\", \"/iplant/home/wregglej/pom.xml2\" ] } Response Body : { \"paths\":{ \"/iplant/home/wregglej/pom.xml2\":false, \"/iplant/home/wregglej/pom.xml\":false } } Curl Command : curl -H \"$AUTH_HEADER\" -H \"Content-type:application/json\" -d '{\"paths\" : [\"/iplant/home/wregglej/pom.xml\", \"/iplant/home/wregglej/pom.xml2\"]}' 'http://127.0.0.1:3000/secured/filesystem/exists","title":"File/Directory existence"},{"location":"services/api/endpoints/filesystem/manifest/","text":"File manifest \u00b6 URL Path : /secured/filesystem/file/manifest HTTP Method : GET Error Codes : ERR_DOES_NOT_EXIST, ERR_NOT_A_FILE, ERR_NOT_READABLE, ERR_NOT_A_USER Request Query Parameters : path - Path to a file in iRODSs. Response Body : { \"content-type\" : \"text/plain\", \"urls\" : [], \"infoType\" : \"<an info type or empty string>\" } The urls field will contain some or none of the following: CoGe URLs Tree URLS anon-files URLs The URLs are formatted like this: { \"label\" : \"<LABEL>\", \"url\" : \"<URL>\" } For anonymous URLs, the label will be \"anonymous\". For CoGe URLs, the label with start with \"gene_\". For tree URLs, the label will usually start with \"tree_\", but that's not guaranteed (the DE doesn't create the labels). Curl Command : curl -H \"$AUTH_HEADER\" http://127.0.0.1:3000/secured/filesystem/file/manifest?path=/iplant/home/johnw/LICENSE.txt","title":"Manifest"},{"location":"services/api/endpoints/filesystem/manifest/#file-manifest","text":"URL Path : /secured/filesystem/file/manifest HTTP Method : GET Error Codes : ERR_DOES_NOT_EXIST, ERR_NOT_A_FILE, ERR_NOT_READABLE, ERR_NOT_A_USER Request Query Parameters : path - Path to a file in iRODSs. Response Body : { \"content-type\" : \"text/plain\", \"urls\" : [], \"infoType\" : \"<an info type or empty string>\" } The urls field will contain some or none of the following: CoGe URLs Tree URLS anon-files URLs The URLs are formatted like this: { \"label\" : \"<LABEL>\", \"url\" : \"<URL>\" } For anonymous URLs, the label will be \"anonymous\". For CoGe URLs, the label with start with \"gene_\". For tree URLs, the label will usually start with \"tree_\", but that's not guaranteed (the DE doesn't create the labels). Curl Command : curl -H \"$AUTH_HEADER\" http://127.0.0.1:3000/secured/filesystem/file/manifest?path=/iplant/home/johnw/LICENSE.txt","title":"File manifest"},{"location":"services/api/endpoints/filesystem/metadata/","text":"Metadata \u00b6 The following endpoints allow the caller to set and get attributes on files and directories in both iRODS and the CyVerse metadata service. iRODS attributes take the form of Attribute Value Unit triples associated with directories and files. iRODS AVUs are only unique on the full triple, so AVUs with duplicate attributes may exist. DE tools do not, in general, use the unit field. Getting Metadata \u00b6 Secured Endpoint: GET /secured/filesystem/{data-id}/metadata Delegates to data-info: GET /data/{data-id}/metadata This endpoint is a passthrough to the data-info endpoint above. Please see the data-info documentation for more information. Setting Metadata \u00b6 Secured Endpoint: POST /secured/filesystem/{data-id}/metadata Delegates to data-info: PUT /data/{data-id}/metadata This endpoint is a passthrough to the data-info endpoint above. Please see the data-info documentation for more information. Listing Metadata Templates \u00b6 Secured Endpoint: GET /secured/filesystem/metadata/templates Delegates to metadata: GET /templates Secured Endpoint: GET /admin/filesystem/metadata/templates Delegates to metadata: GET /admin/templates These endpoints are passthroughs to the metadata endpoints above. Please see the metadata documentation for more information. Viewing a Metadata Template \u00b6 Secured Endpoint: GET /secured/filesystem/metadata/template/{template-id} Delegates to metadata: GET /templates/{template-id} This endpoint is a passthrough to the metadata endpoint above. Please see the metadata documentation for more information. Downloading a blank template \u00b6 Secured Endpoint: GET /secured/filesystem/metadata/template/{template-id}/blank-csv Delegates to metadata: GET /templates/{template-id}/blank-csv This endpoint is a passthrough to the metadata endpoint above. Please see the metadata documentation for more information. Downloading a template guide \u00b6 Secured Endpoint: GET /secured/filesystem/metadata/template/{template-id}/guide-csv Delegates to metadata: GET /templates/{template-id}/guide-csv This endpoint is a passthrough to the metadata endpoint above. Please see the metadata documentation for more information. Viewing a Metadata Attribute \u00b6 Secured Endpoint: GET /secured/filesystem/metadata/template/attr/{attribute-id} Delegates to metadata: GET /templates/attr/{attribute-id} This endpoint is a passthrough to the metadata endpoint above. Please see the metadata documentation for more information. Adding Metadata Templates \u00b6 Secured Endpoint: POST /admin/filesystem/metadata/templates Delegates to metadata: POST /admin/templates This endpoint is a passthrough to the metadata endpoint above. Please see the metadata documentation for more information. Updating Metadata Templates \u00b6 Secured Endpoint: POST /admin/filesystem/metadata/templates/{template-id} Delegates to metadata: PUT /admin/templates/{template-id} This endpoint is a passthrough to the metadata endpoint above. Please see the metadata documentation for more information. Marking a Metadata Template as Deleted \u00b6 Secured Endpoint: DELETE /admin/filesystem/metadata/templates/{template-id} Delegates to metadata: DELETE /admin/templates/{template-id} This endpoint is a passthrough to the metadata endpoint above. Please see the metadata documentation for more information. Adding Batch Metadata to Multiple Paths from a CSV File \u00b6 Secured Endpoint: POST /secured/filesystem/metadata/csv-parser Delegates to data-info: POST /data/{data-id}/metadata/csv-parser Request Query Parameters \u00b6 Parameter Required Description dest Yes The folder path to look under for files listed in the CSV file. src Yes Path to the CSV source file in IRODS. separator No URL encoded separator character to use for parsing the CSV/TSV file. Comma ( %2C ) by default. This endpoint is a passthrough to the data-info endpoint above. Please see the data-info documentation for more information. Curl Command \u00b6 curl -sH \"$AUTH_HEADER\" -X POST \"http://localhost:3000/secured/filesystem/metadata/csv-parser?dest=/iplant/home/ipcuser/folder_1&src=/iplant/home/ipcuser/metadata.csv\" Copying all Metadata from a File/Folder \u00b6 Secured Endpoint: POST /secured/filesystem/{data-id}/metadata/copy Delegates to data-info: POST /data/{data-id}/metadata/copy This endpoint is a passthrough to the data-info endpoint above. Please see the data-info documentation for more information. Exporting Metadata to a File \u00b6 Secured Endpoint: POST /secured/filesystem/{data-id}/metadata/save Delegates to data-info: POST /data/{data-id}/metadata/save This endpoint is a passthrough to the data-info endpoint above. Please see the data-info documentation for more information.","title":"Metadata"},{"location":"services/api/endpoints/filesystem/metadata/#metadata","text":"The following endpoints allow the caller to set and get attributes on files and directories in both iRODS and the CyVerse metadata service. iRODS attributes take the form of Attribute Value Unit triples associated with directories and files. iRODS AVUs are only unique on the full triple, so AVUs with duplicate attributes may exist. DE tools do not, in general, use the unit field.","title":"Metadata"},{"location":"services/api/endpoints/filesystem/metadata/#getting-metadata","text":"Secured Endpoint: GET /secured/filesystem/{data-id}/metadata Delegates to data-info: GET /data/{data-id}/metadata This endpoint is a passthrough to the data-info endpoint above. Please see the data-info documentation for more information.","title":"Getting Metadata"},{"location":"services/api/endpoints/filesystem/metadata/#setting-metadata","text":"Secured Endpoint: POST /secured/filesystem/{data-id}/metadata Delegates to data-info: PUT /data/{data-id}/metadata This endpoint is a passthrough to the data-info endpoint above. Please see the data-info documentation for more information.","title":"Setting Metadata"},{"location":"services/api/endpoints/filesystem/metadata/#listing-metadata-templates","text":"Secured Endpoint: GET /secured/filesystem/metadata/templates Delegates to metadata: GET /templates Secured Endpoint: GET /admin/filesystem/metadata/templates Delegates to metadata: GET /admin/templates These endpoints are passthroughs to the metadata endpoints above. Please see the metadata documentation for more information.","title":"Listing Metadata Templates"},{"location":"services/api/endpoints/filesystem/metadata/#viewing-a-metadata-template","text":"Secured Endpoint: GET /secured/filesystem/metadata/template/{template-id} Delegates to metadata: GET /templates/{template-id} This endpoint is a passthrough to the metadata endpoint above. Please see the metadata documentation for more information.","title":"Viewing a Metadata Template"},{"location":"services/api/endpoints/filesystem/metadata/#downloading-a-blank-template","text":"Secured Endpoint: GET /secured/filesystem/metadata/template/{template-id}/blank-csv Delegates to metadata: GET /templates/{template-id}/blank-csv This endpoint is a passthrough to the metadata endpoint above. Please see the metadata documentation for more information.","title":"Downloading a blank template"},{"location":"services/api/endpoints/filesystem/metadata/#downloading-a-template-guide","text":"Secured Endpoint: GET /secured/filesystem/metadata/template/{template-id}/guide-csv Delegates to metadata: GET /templates/{template-id}/guide-csv This endpoint is a passthrough to the metadata endpoint above. Please see the metadata documentation for more information.","title":"Downloading a template guide"},{"location":"services/api/endpoints/filesystem/metadata/#viewing-a-metadata-attribute","text":"Secured Endpoint: GET /secured/filesystem/metadata/template/attr/{attribute-id} Delegates to metadata: GET /templates/attr/{attribute-id} This endpoint is a passthrough to the metadata endpoint above. Please see the metadata documentation for more information.","title":"Viewing a Metadata Attribute"},{"location":"services/api/endpoints/filesystem/metadata/#adding-metadata-templates","text":"Secured Endpoint: POST /admin/filesystem/metadata/templates Delegates to metadata: POST /admin/templates This endpoint is a passthrough to the metadata endpoint above. Please see the metadata documentation for more information.","title":"Adding Metadata Templates"},{"location":"services/api/endpoints/filesystem/metadata/#updating-metadata-templates","text":"Secured Endpoint: POST /admin/filesystem/metadata/templates/{template-id} Delegates to metadata: PUT /admin/templates/{template-id} This endpoint is a passthrough to the metadata endpoint above. Please see the metadata documentation for more information.","title":"Updating Metadata Templates"},{"location":"services/api/endpoints/filesystem/metadata/#marking-a-metadata-template-as-deleted","text":"Secured Endpoint: DELETE /admin/filesystem/metadata/templates/{template-id} Delegates to metadata: DELETE /admin/templates/{template-id} This endpoint is a passthrough to the metadata endpoint above. Please see the metadata documentation for more information.","title":"Marking a Metadata Template as Deleted"},{"location":"services/api/endpoints/filesystem/metadata/#adding-batch-metadata-to-multiple-paths-from-a-csv-file","text":"Secured Endpoint: POST /secured/filesystem/metadata/csv-parser Delegates to data-info: POST /data/{data-id}/metadata/csv-parser","title":"Adding Batch Metadata to Multiple Paths from a CSV File"},{"location":"services/api/endpoints/filesystem/metadata/#request-query-parameters","text":"Parameter Required Description dest Yes The folder path to look under for files listed in the CSV file. src Yes Path to the CSV source file in IRODS. separator No URL encoded separator character to use for parsing the CSV/TSV file. Comma ( %2C ) by default. This endpoint is a passthrough to the data-info endpoint above. Please see the data-info documentation for more information.","title":"Request Query Parameters"},{"location":"services/api/endpoints/filesystem/metadata/#curl-command","text":"curl -sH \"$AUTH_HEADER\" -X POST \"http://localhost:3000/secured/filesystem/metadata/csv-parser?dest=/iplant/home/ipcuser/folder_1&src=/iplant/home/ipcuser/metadata.csv\"","title":"Curl Command"},{"location":"services/api/endpoints/filesystem/metadata/#copying-all-metadata-from-a-filefolder","text":"Secured Endpoint: POST /secured/filesystem/{data-id}/metadata/copy Delegates to data-info: POST /data/{data-id}/metadata/copy This endpoint is a passthrough to the data-info endpoint above. Please see the data-info documentation for more information.","title":"Copying all Metadata from a File/Folder"},{"location":"services/api/endpoints/filesystem/metadata/#exporting-metadata-to-a-file","text":"Secured Endpoint: POST /secured/filesystem/{data-id}/metadata/save Delegates to data-info: POST /data/{data-id}/metadata/save This endpoint is a passthrough to the data-info endpoint above. Please see the data-info documentation for more information.","title":"Exporting Metadata to a File"},{"location":"services/api/endpoints/filesystem/move/","text":"Moving Files and/or Directories \u00b6 URL Path : /secured/filesystem/move This endpoint delegates to data-info's /mover endpoint. HTTP Method : POST Error codes : ERR_NOT_A_FOLDER, ERR_DOES_NOT_EXIST, ERR_NOT_WRITEABLE, ERR_EXISTS, ERR_TOO_MANY_PATHS, ERR_NOT_A_USER The ERR_DOES_NOT_EXIST error code pops up when the destination directory does not exist and when one of the \"sources\" directories does not exist. The ERR_EXISTS code pops up when one of the new destination directories already exists. The ERR_TOO_MANY_PATHS error code is returned when all of the \"sources\" and the items and sub-directories under them exceed the maximum number of paths that can be processed by this endpoint. Request Query Parameters : Request Body : { \"sources\" : [ \"/tempZone/home/rods/test1\" ], \"dest\" : \"/tempZone/home/rods/test\" } \"sources\" can contain a mix of files and directories. Response : { \"dest\":\"/tempZone/home/rods/test\", \"sources\":[ \"/tempZone/home/rods/test1\" ] } Curl Command : curl -H \"$AUTH_HEADER\" -H \"Content-Type:application/json\" -d '{\"sources\" : [\"/tempZone/home/rods/test1\"],\"dest\" :\"/tempZone/home/rods/test\"}' http://127.0.0.1:3000/secured/filesystem/move Moving all items in a Directory \u00b6 URL Path : /secured/filesystem/move-contents This endpoint delegates to data-info's /data/:data-id/children/dir endpoint, after looking up the source's UUID. HTTP Method : POST Error codes : ERR_NOT_A_FOLDER, ERR_DOES_NOT_EXIST, ERR_NOT_WRITEABLE, ERR_EXISTS, ERR_TOO_MANY_PATHS, ERR_NOT_A_USER The ERR_DOES_NOT_EXIST error code is returned when the \"destination\" directory does not exist and when the \"source\" directory does not exist. The ERR_EXISTS error code is returned when one of the new destination directories or files already exists. The ERR_TOO_MANY_PATHS error code is returned when all items in the \"source\" directory and its sub-directories exceed the maximum number of paths that can be processed by this endpoint. Request Query Parameters : Request Body : { \"source\": \"/tempZone/home/rods/test1\", \"dest\": \"/tempZone/home/rods/test\" } Response : { \"dest\":\"/tempZone/home/rods/test\", \"sources\":[ \"/tempZone/home/rods/test1/test2\", \"/tempZone/home/rods/test1/test3\" ] } Example ERR_TOO_MANY_PATHS Error Response : { \"error_code\": \"ERR_TOO_MANY_PATHS\", \"count\": 250, \"limit\": 100 } Curl Command : curl -H \"$AUTH_HEADER\" -H \"Content-Type:application/json\" -d '{\"source\": \"/tempZone/home/rods/test1\",\"dest\": \"/tempZone/home/rods/test\"}' http://127.0.0.1:3000/secured/filesystem/move-contents","title":"Move"},{"location":"services/api/endpoints/filesystem/move/#moving-files-andor-directories","text":"URL Path : /secured/filesystem/move This endpoint delegates to data-info's /mover endpoint. HTTP Method : POST Error codes : ERR_NOT_A_FOLDER, ERR_DOES_NOT_EXIST, ERR_NOT_WRITEABLE, ERR_EXISTS, ERR_TOO_MANY_PATHS, ERR_NOT_A_USER The ERR_DOES_NOT_EXIST error code pops up when the destination directory does not exist and when one of the \"sources\" directories does not exist. The ERR_EXISTS code pops up when one of the new destination directories already exists. The ERR_TOO_MANY_PATHS error code is returned when all of the \"sources\" and the items and sub-directories under them exceed the maximum number of paths that can be processed by this endpoint. Request Query Parameters : Request Body : { \"sources\" : [ \"/tempZone/home/rods/test1\" ], \"dest\" : \"/tempZone/home/rods/test\" } \"sources\" can contain a mix of files and directories. Response : { \"dest\":\"/tempZone/home/rods/test\", \"sources\":[ \"/tempZone/home/rods/test1\" ] } Curl Command : curl -H \"$AUTH_HEADER\" -H \"Content-Type:application/json\" -d '{\"sources\" : [\"/tempZone/home/rods/test1\"],\"dest\" :\"/tempZone/home/rods/test\"}' http://127.0.0.1:3000/secured/filesystem/move","title":"Moving Files and/or Directories"},{"location":"services/api/endpoints/filesystem/move/#moving-all-items-in-a-directory","text":"URL Path : /secured/filesystem/move-contents This endpoint delegates to data-info's /data/:data-id/children/dir endpoint, after looking up the source's UUID. HTTP Method : POST Error codes : ERR_NOT_A_FOLDER, ERR_DOES_NOT_EXIST, ERR_NOT_WRITEABLE, ERR_EXISTS, ERR_TOO_MANY_PATHS, ERR_NOT_A_USER The ERR_DOES_NOT_EXIST error code is returned when the \"destination\" directory does not exist and when the \"source\" directory does not exist. The ERR_EXISTS error code is returned when one of the new destination directories or files already exists. The ERR_TOO_MANY_PATHS error code is returned when all items in the \"source\" directory and its sub-directories exceed the maximum number of paths that can be processed by this endpoint. Request Query Parameters : Request Body : { \"source\": \"/tempZone/home/rods/test1\", \"dest\": \"/tempZone/home/rods/test\" } Response : { \"dest\":\"/tempZone/home/rods/test\", \"sources\":[ \"/tempZone/home/rods/test1/test2\", \"/tempZone/home/rods/test1/test3\" ] } Example ERR_TOO_MANY_PATHS Error Response : { \"error_code\": \"ERR_TOO_MANY_PATHS\", \"count\": 250, \"limit\": 100 } Curl Command : curl -H \"$AUTH_HEADER\" -H \"Content-Type:application/json\" -d '{\"source\": \"/tempZone/home/rods/test1\",\"dest\": \"/tempZone/home/rods/test\"}' http://127.0.0.1:3000/secured/filesystem/move-contents","title":"Moving all items in a Directory"},{"location":"services/api/endpoints/filesystem/ore/","text":"Generating OAI-ORE files for a data set. \u00b6 Secured Endpoint: POST /secured/filesystem/{data-id}/ore/save Delegates to data-info: POST /data/{data-id}/ore/save This endpoint is a passthrough to the data-info endpoint above. Please see the data-info documentation for more information.","title":"Generating OAI-ORE files for a data set."},{"location":"services/api/endpoints/filesystem/ore/#generating-oai-ore-files-for-a-data-set","text":"Secured Endpoint: POST /secured/filesystem/{data-id}/ore/save Delegates to data-info: POST /data/{data-id}/ore/save This endpoint is a passthrough to the data-info endpoint above. Please see the data-info documentation for more information.","title":"Generating OAI-ORE files for a data set."},{"location":"services/api/endpoints/filesystem/path-lists/","text":"HT Path List Creator \u00b6 Delegates to data-info: POST /path-list-creator This endpoint is a passthrough to the data-info endpoint above. Please see the data-info documentation for more information. URL Path : /secured/filesystem/path-list-creator HTTP Method : POST","title":"Path lists"},{"location":"services/api/endpoints/filesystem/path-lists/#ht-path-list-creator","text":"Delegates to data-info: POST /path-list-creator This endpoint is a passthrough to the data-info endpoint above. Please see the data-info documentation for more information. URL Path : /secured/filesystem/path-list-creator HTTP Method : POST","title":"HT Path List Creator"},{"location":"services/api/endpoints/filesystem/permissions/","text":"Listing User Permissions \u00b6 Lists the users that have access to a file and the their permissions on the file. The user making the request and the configured rodsadmin user are filtered out of the returned list. The user making the request must own the file. URL Path : /secured/filesystem/user-permissions HTTP Method : POST Error Codes : ERR_NOT_A_USER, ERR_DOES_NOT_EXIST, ERR_NOT_OWNER Request Query Parameters : Request Body : { \"paths\" : [ \"/iplant/home/testuser/testfile\" , \"/iplant/home/testuser/testfile2\" ] } Response Body : { \"paths\" : [ { \"path\" : \"/iplant/home/testuser/testfile\" , \"user-permissions\" : [ { \"user\" : \"user1\" , \"permission\" : \"read\" } ] }, { \"path\" : \"/iplant/home/testuser/testfile2\" , \"user-permissions\" : [ { \"user\" : \"user2\" , \"permission\" : \"read\" } ] } ] } Curl Command : curl -H \"$AUTH_HEADER\" -d '{\"paths\" : [\"/iplant/home/testuser/testfile\", \"/iplant/home/testuser/testfile2\"]}' 'http://nibblonian.example.org/secured/filesystem/user-permissions","title":"Permissions"},{"location":"services/api/endpoints/filesystem/permissions/#listing-user-permissions","text":"Lists the users that have access to a file and the their permissions on the file. The user making the request and the configured rodsadmin user are filtered out of the returned list. The user making the request must own the file. URL Path : /secured/filesystem/user-permissions HTTP Method : POST Error Codes : ERR_NOT_A_USER, ERR_DOES_NOT_EXIST, ERR_NOT_OWNER Request Query Parameters : Request Body : { \"paths\" : [ \"/iplant/home/testuser/testfile\" , \"/iplant/home/testuser/testfile2\" ] } Response Body : { \"paths\" : [ { \"path\" : \"/iplant/home/testuser/testfile\" , \"user-permissions\" : [ { \"user\" : \"user1\" , \"permission\" : \"read\" } ] }, { \"path\" : \"/iplant/home/testuser/testfile2\" , \"user-permissions\" : [ { \"user\" : \"user2\" , \"permission\" : \"read\" } ] } ] } Curl Command : curl -H \"$AUTH_HEADER\" -d '{\"paths\" : [\"/iplant/home/testuser/testfile\", \"/iplant/home/testuser/testfile2\"]}' 'http://nibblonian.example.org/secured/filesystem/user-permissions","title":"Listing User Permissions"},{"location":"services/api/endpoints/filesystem/read-chunk/","text":"Reading a chunk of a file \u00b6 URL Path : /secured/filesystem/read-chunk This endpoint delegates to data-info's /data/:data-id/chunks/:position/:size, after looking up the UUID corresponding to the path it was passed. HTTP Method : POST Error Codes_ : ERR_DOES_NOT_EXIST, ERR_NOT_READABLE, ERR_NOT_A_USER Request Query Parameters_ : Request Body : { \"path\" : \"/iplant/home/wregglej/testfile\", \"position\" : \"20\", \"chunk-size\" : \"7\" } Response : { \"path\" : \"/iplant/home/wregglej/testfile\", \"user\" : \"wregglej\", \"start\" : \"20\", \"chunk-size\" : \"7\", \"file-size\" : \"33\", \"chunk\" : \"0\\n12345\" } Curl Command : curl -H \"$AUTH_HEADER\" -H \"Content-Type:application/json\" -d '{\"path\" : \"/iplant/home/wregglej/testfile\", \"position\" : \"20\", \"chunk-size\" : \"7\"}' http://127.0.0.1:31360/secured/filesystem/read-chunk Notes: * 'position' and 'chunk-size' are both in bytes. * 'position' and 'chunk-size' must be parseable as longs. * 'start', 'chunk-size', and 'file-size' in the response are all in bytes. * 'start', 'chunk-size', amd 'file-size' should all be parseable as longs. * The byte at 'position' is not included in the response. The chunk begins at position + 1.","title":"Read chunk"},{"location":"services/api/endpoints/filesystem/read-chunk/#reading-a-chunk-of-a-file","text":"URL Path : /secured/filesystem/read-chunk This endpoint delegates to data-info's /data/:data-id/chunks/:position/:size, after looking up the UUID corresponding to the path it was passed. HTTP Method : POST Error Codes_ : ERR_DOES_NOT_EXIST, ERR_NOT_READABLE, ERR_NOT_A_USER Request Query Parameters_ : Request Body : { \"path\" : \"/iplant/home/wregglej/testfile\", \"position\" : \"20\", \"chunk-size\" : \"7\" } Response : { \"path\" : \"/iplant/home/wregglej/testfile\", \"user\" : \"wregglej\", \"start\" : \"20\", \"chunk-size\" : \"7\", \"file-size\" : \"33\", \"chunk\" : \"0\\n12345\" } Curl Command : curl -H \"$AUTH_HEADER\" -H \"Content-Type:application/json\" -d '{\"path\" : \"/iplant/home/wregglej/testfile\", \"position\" : \"20\", \"chunk-size\" : \"7\"}' http://127.0.0.1:31360/secured/filesystem/read-chunk Notes: * 'position' and 'chunk-size' are both in bytes. * 'position' and 'chunk-size' must be parseable as longs. * 'start', 'chunk-size', and 'file-size' in the response are all in bytes. * 'start', 'chunk-size', amd 'file-size' should all be parseable as longs. * The byte at 'position' is not included in the response. The chunk begins at position + 1.","title":"Reading a chunk of a file"},{"location":"services/api/endpoints/filesystem/rename/","text":"Renaming a File or Directory \u00b6 URL Path : /secured/filesystem/rename HTTP Method : POST Error codes : ERR_NOT_A_FOLDER, ERR_DOES_NOT_EXIST, ERR_NOT_WRITEABLE, ERR_EXISTS, ERR_INCOMPLETE_RENAME, ERR_NOT_A_USER, ERR_TOO_MANY_PATHS The ERR_TOO_MANY_PATHS error code is returned when the items and sub-directories under the \"source\" folder exceed the maximum number of paths that can be processed by this endpoint. Request Query Parameters : Request Body : { \"source\" : \"/tempZone/home/wregglej/test3\", \"dest\" : \"/tempZone/home/wregglej/test2\" } Response : { \"source\":\"/tempZone/home/wregglej/test3\", \"dest\":\"/tempZone/home/wregglej/test2\" } Curl Command : curl -H \"$AUTH_HEADER\" -H \"Content-Type:application/json\" -d '{\"source\" : \"/tempZone/home/wregglej/test3\", \"dest\" : \"/tempZone/home/wregglej/test2\"}' http://127.0.0.1:3000/secured/filesystem/rename","title":"Rename"},{"location":"services/api/endpoints/filesystem/rename/#renaming-a-file-or-directory","text":"URL Path : /secured/filesystem/rename HTTP Method : POST Error codes : ERR_NOT_A_FOLDER, ERR_DOES_NOT_EXIST, ERR_NOT_WRITEABLE, ERR_EXISTS, ERR_INCOMPLETE_RENAME, ERR_NOT_A_USER, ERR_TOO_MANY_PATHS The ERR_TOO_MANY_PATHS error code is returned when the items and sub-directories under the \"source\" folder exceed the maximum number of paths that can be processed by this endpoint. Request Query Parameters : Request Body : { \"source\" : \"/tempZone/home/wregglej/test3\", \"dest\" : \"/tempZone/home/wregglej/test2\" } Response : { \"source\":\"/tempZone/home/wregglej/test3\", \"dest\":\"/tempZone/home/wregglej/test2\" } Curl Command : curl -H \"$AUTH_HEADER\" -H \"Content-Type:application/json\" -d '{\"source\" : \"/tempZone/home/wregglej/test3\", \"dest\" : \"/tempZone/home/wregglej/test2\"}' http://127.0.0.1:3000/secured/filesystem/rename","title":"Renaming a File or Directory"},{"location":"services/api/endpoints/filesystem/restore/","text":"Restoring a file or directory from a user's trash \u00b6 URL Path : /secured/filesystem/restore HTTP Method : POST Error Codes : ERR_EXISTS, ERR_DOES_NOT_EXIST, ERR_NOT_A_USER, ERR_NOT_WRITEABLE, ERR_TOO_MANY_PATHS The ERR_TOO_MANY_PATHS error code is returned when all of the \"paths\" and the items and sub-directories under them exceed the maximum number of paths that can be processed by this endpoint. Request Query Parameters : Request Body : { \"paths\" : [\"/iplant/trash/home/proxy-user/johnworth/foo.fq\", \"/iplant/trash/home/proxy-user/johnworth/foo1.fq\"] } Response Body : { \"restored\" : { \"/iplant/trash/home/proxy-user/johnworth/foo.fq\" : { \"restored-path\" : /iplant/home/johnworth/foo.fq\", \"partial-restore\" : true }, \"/iplant/trash/home/proxy-user/johnworth/foo1.fq\" : { \"restored-path\" : \"/iplant/home/johnworth/foo1.fq\" \"partial-restore\" : true } } } The \"restored\" field is a map that whose keys are the paths in the trash that were restored. Associated with those paths is a map that contains two entries, \"restored-path\" that contains the path that the file was restored to, and \"partial-restore\" which is a boolean that is true if the restoration was to the home directory because there was no alternative and false if the restoration was a full restore. If a file is deleted and then the directory that the file originally lived in is deleted, the directory will be recreated if the file is restored. If the deleted directory is then subsequently restored, it will moved back to its original location with a numerical suffix. The suffix is generated according to how many directories with the same name exist in the same parent directory. File restorations follow similar logic. If a file with the same name is restored to the same directory multiple times, the subsequent restored versions will have numerical suffixes as well. Curl Command : curl -H \"$AUTH_HEADER\" -d '{\"paths\" : [\"/iplant/trash/home/proxy-user/johnworth/foo.fq\", \"/iplant/trash/home/proxy-user/johnworth/foo1.fq\"]}' http://sample.nibblonian.org/secured/filesystem/restore Restoring all items in a user's trash \u00b6 URL Path : /secured/filesystem/restore-all HTTP Method : POST Error Codes : ERR_EXISTS, ERR_NOT_A_USER, ERR_NOT_WRITEABLE, ERR_TOO_MANY_PATHS The ERR_TOO_MANY_PATHS error code is returned when all items in the user's trash and its sub-directories exceed the maximum number of paths that can be processed by this endpoint. Request Query Parameters : Request Body : No body is required for this request. Response : { \"restored\" : { \"/iplant/trash/home/proxy-user/johnworth/foo.fq\" : { \"restored-path\" : /iplant/home/johnworth/foo.fq\", \"partial-restore\" : true }, \"/iplant/trash/home/proxy-user/johnworth/foo1.fq\" : { \"restored-path\" : \"/iplant/home/johnworth/foo1.fq\" \"partial-restore\" : true } } } Example ERR_TOO_MANY_PATHS Error Response : { \"error_code\": \"ERR_TOO_MANY_PATHS\", \"count\": 250, \"limit\": 100 } Curl Command : curl -H \"$AUTH_HEADER\" -H \"Content-Type:application/json\" -X POST http://127.0.0.1:3000/secured/filesystem/restore-all","title":"Restore"},{"location":"services/api/endpoints/filesystem/restore/#restoring-a-file-or-directory-from-a-users-trash","text":"URL Path : /secured/filesystem/restore HTTP Method : POST Error Codes : ERR_EXISTS, ERR_DOES_NOT_EXIST, ERR_NOT_A_USER, ERR_NOT_WRITEABLE, ERR_TOO_MANY_PATHS The ERR_TOO_MANY_PATHS error code is returned when all of the \"paths\" and the items and sub-directories under them exceed the maximum number of paths that can be processed by this endpoint. Request Query Parameters : Request Body : { \"paths\" : [\"/iplant/trash/home/proxy-user/johnworth/foo.fq\", \"/iplant/trash/home/proxy-user/johnworth/foo1.fq\"] } Response Body : { \"restored\" : { \"/iplant/trash/home/proxy-user/johnworth/foo.fq\" : { \"restored-path\" : /iplant/home/johnworth/foo.fq\", \"partial-restore\" : true }, \"/iplant/trash/home/proxy-user/johnworth/foo1.fq\" : { \"restored-path\" : \"/iplant/home/johnworth/foo1.fq\" \"partial-restore\" : true } } } The \"restored\" field is a map that whose keys are the paths in the trash that were restored. Associated with those paths is a map that contains two entries, \"restored-path\" that contains the path that the file was restored to, and \"partial-restore\" which is a boolean that is true if the restoration was to the home directory because there was no alternative and false if the restoration was a full restore. If a file is deleted and then the directory that the file originally lived in is deleted, the directory will be recreated if the file is restored. If the deleted directory is then subsequently restored, it will moved back to its original location with a numerical suffix. The suffix is generated according to how many directories with the same name exist in the same parent directory. File restorations follow similar logic. If a file with the same name is restored to the same directory multiple times, the subsequent restored versions will have numerical suffixes as well. Curl Command : curl -H \"$AUTH_HEADER\" -d '{\"paths\" : [\"/iplant/trash/home/proxy-user/johnworth/foo.fq\", \"/iplant/trash/home/proxy-user/johnworth/foo1.fq\"]}' http://sample.nibblonian.org/secured/filesystem/restore","title":"Restoring a file or directory from a user's trash"},{"location":"services/api/endpoints/filesystem/restore/#restoring-all-items-in-a-users-trash","text":"URL Path : /secured/filesystem/restore-all HTTP Method : POST Error Codes : ERR_EXISTS, ERR_NOT_A_USER, ERR_NOT_WRITEABLE, ERR_TOO_MANY_PATHS The ERR_TOO_MANY_PATHS error code is returned when all items in the user's trash and its sub-directories exceed the maximum number of paths that can be processed by this endpoint. Request Query Parameters : Request Body : No body is required for this request. Response : { \"restored\" : { \"/iplant/trash/home/proxy-user/johnworth/foo.fq\" : { \"restored-path\" : /iplant/home/johnworth/foo.fq\", \"partial-restore\" : true }, \"/iplant/trash/home/proxy-user/johnworth/foo1.fq\" : { \"restored-path\" : \"/iplant/home/johnworth/foo1.fq\" \"partial-restore\" : true } } } Example ERR_TOO_MANY_PATHS Error Response : { \"error_code\": \"ERR_TOO_MANY_PATHS\", \"count\": 250, \"limit\": 100 } Curl Command : curl -H \"$AUTH_HEADER\" -H \"Content-Type:application/json\" -X POST http://127.0.0.1:3000/secured/filesystem/restore-all","title":"Restoring all items in a user's trash"},{"location":"services/api/endpoints/filesystem/root-listing/","text":"Top Level Root Listing \u00b6 Delegates to data-info: GET /navigation/root This endpoint is a passthrough to the data-info endpoint above, though it will add the label and hasSubDirs fields to data-info responses. Please see the data-info documentation for more information. URL Path : /secured/filesystem/root HTTP Method : GET Request Query Parameters : Response Body : { \"roots\" : [ { \"date-modified\" : 1340918988000 , \"hasSubDirs\" : true , \"permission\" : \"own\" , \"date-created\" : 1335217160000 , \"label\" : \"wregglej\" , \"id\" : \"0b331f99-896f-4465-b0bf-15185c53414c\" , \"path\" : \"/iplant/home/wregglej\" }, { \"date-modified\" : 1335476028000 , \"hasSubDirs\" : true , \"permission\" : \"write\" , \"date-created\" : 1335217387000 , \"label\" : \"Community Data\" , \"id\" : \"54ab8910-f9b3-11e4-9d60-1a5a300ff36f\" , \"path\" : \"/iplant/home/shared\" } ] } Curl Command : curl -H \"$AUTH_HEADER\" 'http://127.0.0.1::3000/secured/filesystem/root'","title":"Root listing"},{"location":"services/api/endpoints/filesystem/root-listing/#top-level-root-listing","text":"Delegates to data-info: GET /navigation/root This endpoint is a passthrough to the data-info endpoint above, though it will add the label and hasSubDirs fields to data-info responses. Please see the data-info documentation for more information. URL Path : /secured/filesystem/root HTTP Method : GET Request Query Parameters : Response Body : { \"roots\" : [ { \"date-modified\" : 1340918988000 , \"hasSubDirs\" : true , \"permission\" : \"own\" , \"date-created\" : 1335217160000 , \"label\" : \"wregglej\" , \"id\" : \"0b331f99-896f-4465-b0bf-15185c53414c\" , \"path\" : \"/iplant/home/wregglej\" }, { \"date-modified\" : 1335476028000 , \"hasSubDirs\" : true , \"permission\" : \"write\" , \"date-created\" : 1335217387000 , \"label\" : \"Community Data\" , \"id\" : \"54ab8910-f9b3-11e4-9d60-1a5a300ff36f\" , \"path\" : \"/iplant/home/shared\" } ] } Curl Command : curl -H \"$AUTH_HEADER\" 'http://127.0.0.1::3000/secured/filesystem/root'","title":"Top Level Root Listing"},{"location":"services/api/endpoints/filesystem/search/","text":"This document describes the endpoints used to performing searches of user data. Table of Contents \u00b6 Indexed Information Basic Usage Search Requests Indexed Information \u00b6 For each file and folder stored in the CyVerse Data Store, its ACL, system metadata, and user metadata are indexed as a JSON document. For files, file records are indexed, and for folders, folder records are indexed. In addition, CyVerse metadata (non-iRODS metadata) is indexed in a child document, and tags are indexed separately in their own way as well. Basic Usage \u00b6 For the client without administrative privileges, Terrain provides endpoints for performing searches and for checking the status of the indexer. Search Requests \u00b6 Terrain provides search endpoints that allow callers to search the data by name and various pieces of system and user metadata. It supports the all the queries in the [ElasticSearch query DSL] ( http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-queries.html ) for searching. Each field in an indexed document may be explicitly used in a search query. If the field is an object, i.e. an aggregate of fields, the object's fields may be explicitly referenced as well using dot notation, e.g. acl.access . Endpoints \u00b6 Secured Endpoint: GET /secured/filesystem/index This endpoint searches the data index and retrieves a set of files and/or folders matching the terms provided in the query string. Request Parameters \u00b6 The following additional URI parameters are recognized. Parameter Required? Default Description q yes* This parameter holds a JSON encoded search query. See query syntax for a description of the syntax. type no any This parameter restricts the search to either files or folders. It can take the values any , meaning files and folders, file , only files, and folders , only folders. tags yes* This is a comma-separated list of tag UUIDs. This parameter restricts the search to only entities that have at least of the provided tags. includeTrash no false This parameter allows the result set to contain matching files and folders from the user's DE trash. A value true allows trash, and a value of false excludes it. offset no 0 This parameter indicates the number of matches to skip before including any in the result set. When combined with limit , it allows for paging results. limit no 200 This parameter limits the number of matches in the result set to be a most a certain amount. When combined with offset , it allows for paging results. sort no score:desc See sorting * At least q or tags is required. Sorting \u00b6 The result set is sorted. By default the sort is performed on the score field in descending order. The sort request parameter can be used to change the sort order. Its value has the form field:direction where field is one of the fields of a match record and direction is either asc for ascending or desc for descending. Use dot notation to sort by one of the nested fields of the match records, e.g., entity.label will sort by the label field of the matched entities. If no :direction is provided, the search direction will default to desc . Response \u00b6 When the search succeeds the response document has these additional fields. Field Type Description total number This is the total number of matches found, not the number of elements in the matches array. offset number This is the value of the offset parameter in the query string. matches array This is the set or partial set of matches found, each entry being a match record . It contains at most limit entries and is sorted by descending score. execution-time number This is the number of milliseconds that it took to perform the query and get a response from elasticsearch. Match Record Field Type Description score number an indication of how well this entity matches the query compared to other matches type string the entity is this type of filesystem entry, either \"file\" or \"folder\" entity object the file record or folder record matched Example \u00b6 $ curl -H \"$AUTH_HEADER\" \\ > \"http://localhost:8888/secured/filesystem/index?q=\\\\{\\\"wildcard\\\":\\\\{\\\"label\\\":\\\"?e*\\\"\\\\}\\\\}&type=file&tags=64581dbe-3aa1-11e4-a54e-3c4a92e4a804,6504ca14-3aa1-11e4-8d83-3c4a92e4a804&offset=1&limit=2&sort:desc\" \\ > | python -mjson.tool { \"matches\": [ { \"entity\": { \"creator\": \"rods#iplant\", \"dateCreated\": 1381325224090, \"dateModified\": 1384998366001, \"fileSize\": 13225, \"id\": \"6278f8e0-121f-4ce6-a15f-1083cfad6de5\", \"label\": \"read1_10k.fq\", \"fileType\": null, \"metadata\": [], \"path\": \"/iplant/home/rods/analyses/fc_01300857-2013-10-09-13-27-04.090/read1_10k.fq\", \"permission\" : \"own\", \"userPermissions\": [ { \"permission\": \"read\", \"user\": \"rods#iplant\" }, { \"permission\": \"own\", \"user\": \"rodsadmin#iplant\" } ] }, \"score\": 1.0, \"type\": \"file\" }, { \"entity\": { \"creator\": \"rods#iplant\", \"dateCreated\": 1381325285602, \"dateModified\": 1384998366001, \"fileSize\": 14016, \"fileType\": null, \"id\": \"acaeb63d-8e84-412d-89a2-716a6a4dda7e\", \"label\": \"read1_10k.fq\", \"metadata\": [ { \"attribute\": \"color\", \"unit\": null, \"value\": \"red\" } ], \"path\": \"/iplant/home/rods/analyses/ft_01251621-2013-10-09-13-28-05.602/read1_10k.fq\", \"permission\": \"write\", \"userPermissions\": [ { \"permission\": \"read\", \"user\": \"rods#iplant\" }, { \"permission\": \"write\", \"user\": \"rodsadmin#iplant\" } ] }, \"score\": 1.0, \"type\": \"file\" } ], \"execution-time\" : 300, \"offset\": 1, \"total\": 7 }","title":"Data Search"},{"location":"services/api/endpoints/filesystem/search/#table-of-contents","text":"Indexed Information Basic Usage Search Requests","title":"Table of Contents"},{"location":"services/api/endpoints/filesystem/search/#indexed-information","text":"For each file and folder stored in the CyVerse Data Store, its ACL, system metadata, and user metadata are indexed as a JSON document. For files, file records are indexed, and for folders, folder records are indexed. In addition, CyVerse metadata (non-iRODS metadata) is indexed in a child document, and tags are indexed separately in their own way as well.","title":"Indexed Information"},{"location":"services/api/endpoints/filesystem/search/#basic-usage","text":"For the client without administrative privileges, Terrain provides endpoints for performing searches and for checking the status of the indexer.","title":"Basic Usage"},{"location":"services/api/endpoints/filesystem/search/#search-requests","text":"Terrain provides search endpoints that allow callers to search the data by name and various pieces of system and user metadata. It supports the all the queries in the [ElasticSearch query DSL] ( http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-queries.html ) for searching. Each field in an indexed document may be explicitly used in a search query. If the field is an object, i.e. an aggregate of fields, the object's fields may be explicitly referenced as well using dot notation, e.g. acl.access .","title":"Search Requests"},{"location":"services/api/endpoints/filesystem/search/#endpoints","text":"Secured Endpoint: GET /secured/filesystem/index This endpoint searches the data index and retrieves a set of files and/or folders matching the terms provided in the query string.","title":"Endpoints"},{"location":"services/api/endpoints/filesystem/search/#request-parameters","text":"The following additional URI parameters are recognized. Parameter Required? Default Description q yes* This parameter holds a JSON encoded search query. See query syntax for a description of the syntax. type no any This parameter restricts the search to either files or folders. It can take the values any , meaning files and folders, file , only files, and folders , only folders. tags yes* This is a comma-separated list of tag UUIDs. This parameter restricts the search to only entities that have at least of the provided tags. includeTrash no false This parameter allows the result set to contain matching files and folders from the user's DE trash. A value true allows trash, and a value of false excludes it. offset no 0 This parameter indicates the number of matches to skip before including any in the result set. When combined with limit , it allows for paging results. limit no 200 This parameter limits the number of matches in the result set to be a most a certain amount. When combined with offset , it allows for paging results. sort no score:desc See sorting * At least q or tags is required.","title":"Request Parameters"},{"location":"services/api/endpoints/filesystem/search/#sorting","text":"The result set is sorted. By default the sort is performed on the score field in descending order. The sort request parameter can be used to change the sort order. Its value has the form field:direction where field is one of the fields of a match record and direction is either asc for ascending or desc for descending. Use dot notation to sort by one of the nested fields of the match records, e.g., entity.label will sort by the label field of the matched entities. If no :direction is provided, the search direction will default to desc .","title":"Sorting"},{"location":"services/api/endpoints/filesystem/search/#response","text":"When the search succeeds the response document has these additional fields. Field Type Description total number This is the total number of matches found, not the number of elements in the matches array. offset number This is the value of the offset parameter in the query string. matches array This is the set or partial set of matches found, each entry being a match record . It contains at most limit entries and is sorted by descending score. execution-time number This is the number of milliseconds that it took to perform the query and get a response from elasticsearch. Match Record Field Type Description score number an indication of how well this entity matches the query compared to other matches type string the entity is this type of filesystem entry, either \"file\" or \"folder\" entity object the file record or folder record matched","title":"Response"},{"location":"services/api/endpoints/filesystem/search/#example","text":"$ curl -H \"$AUTH_HEADER\" \\ > \"http://localhost:8888/secured/filesystem/index?q=\\\\{\\\"wildcard\\\":\\\\{\\\"label\\\":\\\"?e*\\\"\\\\}\\\\}&type=file&tags=64581dbe-3aa1-11e4-a54e-3c4a92e4a804,6504ca14-3aa1-11e4-8d83-3c4a92e4a804&offset=1&limit=2&sort:desc\" \\ > | python -mjson.tool { \"matches\": [ { \"entity\": { \"creator\": \"rods#iplant\", \"dateCreated\": 1381325224090, \"dateModified\": 1384998366001, \"fileSize\": 13225, \"id\": \"6278f8e0-121f-4ce6-a15f-1083cfad6de5\", \"label\": \"read1_10k.fq\", \"fileType\": null, \"metadata\": [], \"path\": \"/iplant/home/rods/analyses/fc_01300857-2013-10-09-13-27-04.090/read1_10k.fq\", \"permission\" : \"own\", \"userPermissions\": [ { \"permission\": \"read\", \"user\": \"rods#iplant\" }, { \"permission\": \"own\", \"user\": \"rodsadmin#iplant\" } ] }, \"score\": 1.0, \"type\": \"file\" }, { \"entity\": { \"creator\": \"rods#iplant\", \"dateCreated\": 1381325285602, \"dateModified\": 1384998366001, \"fileSize\": 14016, \"fileType\": null, \"id\": \"acaeb63d-8e84-412d-89a2-716a6a4dda7e\", \"label\": \"read1_10k.fq\", \"metadata\": [ { \"attribute\": \"color\", \"unit\": null, \"value\": \"red\" } ], \"path\": \"/iplant/home/rods/analyses/ft_01251621-2013-10-09-13-28-05.602/read1_10k.fq\", \"permission\": \"write\", \"userPermissions\": [ { \"permission\": \"read\", \"user\": \"rods#iplant\" }, { \"permission\": \"write\", \"user\": \"rodsadmin#iplant\" } ] }, \"score\": 1.0, \"type\": \"file\" } ], \"execution-time\" : 300, \"offset\": 1, \"total\": 7 }","title":"Example"},{"location":"services/api/endpoints/filesystem/sharing/","text":"Sharing & Unsharing \u00b6 Please see the /secured/share and /secured/unshare endpoints. Sharing files with the anonymous user \u00b6 Shares files with the anonymous user. It gives the anonymous user read access. All paths must be files. URL Path : /secured/filesystem/anon-files HTTP Method : POST ERROR CODE : ERR_NOT_A_USER, ERR_BAD_OR_MISSING_FIELD, ERR_DOES_NOT_EXIST, ERR_NOT_OWNER, ERR_NOT_A_FILE Request Body : { \"paths\" :[ \"/path/to/a/file\" ] } Curl Command : curl -H \"$AUTH_HEADER\" -d '{\"paths\" : [\"/path/to/a/file\"]}' http://example.org/secured/filesystem/anon-files Response Body : { \"paths\" : { \"/path/to/a/file\" : \"http://URL-to-file/\" }, \"user\" : \"username\" }","title":"Sharing"},{"location":"services/api/endpoints/filesystem/sharing/#sharing-unsharing","text":"Please see the /secured/share and /secured/unshare endpoints.","title":"Sharing &amp; Unsharing"},{"location":"services/api/endpoints/filesystem/sharing/#sharing-files-with-the-anonymous-user","text":"Shares files with the anonymous user. It gives the anonymous user read access. All paths must be files. URL Path : /secured/filesystem/anon-files HTTP Method : POST ERROR CODE : ERR_NOT_A_USER, ERR_BAD_OR_MISSING_FIELD, ERR_DOES_NOT_EXIST, ERR_NOT_OWNER, ERR_NOT_A_FILE Request Body : { \"paths\" :[ \"/path/to/a/file\" ] } Curl Command : curl -H \"$AUTH_HEADER\" -d '{\"paths\" : [\"/path/to/a/file\"]}' http://example.org/secured/filesystem/anon-files Response Body : { \"paths\" : { \"/path/to/a/file\" : \"http://URL-to-file/\" }, \"user\" : \"username\" }","title":"Sharing files with the anonymous user"},{"location":"services/api/endpoints/filesystem/stat/","text":"File and Directory Status Information \u00b6 The /stat endpoint allows the caller to get serveral pieces of information about a file or directory at once. For directories, the response includes the created and last-modified timestamps along with a file type of dir . For regular files, the response contains the created and last-modified timestamps, the file size in bytes and a file type of file . The following is an example call to the stat endpoint: URL Path : /secured/filesystem/stat HTTP Method : POST Request Query Parameters : Request Body : { \"paths\" : [ \"/iplant/home/wregglej/BTSync.dmg\" ] } Response Body : { \"paths\" : { \"/iplant/home/wregglej/BTSync.dmg\" : { \"date-created\" : 1.398183371e+12 , \"date-modified\" : 1.398183371e+12 , \"file-size\" : 1.0822742e+07 , \"id\" : \"0dfcac40-df8a-11e3-bfa5-6abdce5a08d5\" , \"infoType\" : \"\" , \"label\" : \"BTSync.dmg\" , \"content-type\" : \"application/octet-stream\" , \"path\" : \"/iplant/home/wregglej/BTSync.dmg\" , \"permission\" : \"own\" , \"share-count\" : 1 , \"type\" : \"file\" } } } Note that entries in the \"paths\" map that are directories will include \"file-count\" and \"dir-count\" fields, while file entries will not. The \"share-count\" field is provided for both files and directories and lists the number of users that a file is shared with. Curl Command : curl -H \"$AUTH_HEADER\" -H \"Content-Type:application/json\" -sd '{\"paths\":[\"/iplant/home/wregglej/BTSync.dmg\"]}' http://services-2:31360/secured/filesystem/stat","title":"Stat"},{"location":"services/api/endpoints/filesystem/stat/#file-and-directory-status-information","text":"The /stat endpoint allows the caller to get serveral pieces of information about a file or directory at once. For directories, the response includes the created and last-modified timestamps along with a file type of dir . For regular files, the response contains the created and last-modified timestamps, the file size in bytes and a file type of file . The following is an example call to the stat endpoint: URL Path : /secured/filesystem/stat HTTP Method : POST Request Query Parameters : Request Body : { \"paths\" : [ \"/iplant/home/wregglej/BTSync.dmg\" ] } Response Body : { \"paths\" : { \"/iplant/home/wregglej/BTSync.dmg\" : { \"date-created\" : 1.398183371e+12 , \"date-modified\" : 1.398183371e+12 , \"file-size\" : 1.0822742e+07 , \"id\" : \"0dfcac40-df8a-11e3-bfa5-6abdce5a08d5\" , \"infoType\" : \"\" , \"label\" : \"BTSync.dmg\" , \"content-type\" : \"application/octet-stream\" , \"path\" : \"/iplant/home/wregglej/BTSync.dmg\" , \"permission\" : \"own\" , \"share-count\" : 1 , \"type\" : \"file\" } } } Note that entries in the \"paths\" map that are directories will include \"file-count\" and \"dir-count\" fields, while file entries will not. The \"share-count\" field is provided for both files and directories and lists the number of users that a file is shared with. Curl Command : curl -H \"$AUTH_HEADER\" -H \"Content-Type:application/json\" -sd '{\"paths\":[\"/iplant/home/wregglej/BTSync.dmg\"]}' http://services-2:31360/secured/filesystem/stat","title":"File and Directory Status Information"},{"location":"services/api/endpoints/filesystem/tickets/","text":"Creating Tickets \u00b6 URL Path : /secured/filesystem/tickets HTTP Method : POST Error Codes : ERR_NOT_A_USER, ERR_DOES_NOT_EXIST, ERR_NOT_WRITEABLE Request Parameters : public - Tells terrain whether to make the ticket accessible to the public group. Setting it to 1 means that the ticket will be made publicly accessible. Any other value means that the ticket will not be accessible publicly. This parameter is optional and defaults to not making tickets public. Request Body : { \"paths\" : [ \"/path/to/file/or/directory\", \"/path/to/another/file/or/directory\" ] } Response Body : { \"user\" : \"<username>\", \"tickets\" : [ { \"path\" : \"/path/to/file/or/directory\", \"ticket-id\" : \"<ticket-id>\", \"download-url\" : \"http://127.0.0.1:8080/d/<ticket-id>\", \"download-page-url\" : \"http://127.0.0.1:8080/<ticket-id>\" } ] } Curl Command : curl -H \"$AUTH_HEADER\" -d '{\"paths\":\"/path/to/file/or/directory\",\"/path/to/another/file/or/directory\"]}' 'http://127.0.0.1:3000/secured/filesystem/tickets?public=1' Listing Tickets \u00b6 URL Path : /secured/filesystem/list-tickets HTTP Method : POST Error Codes : ERR_NOT_A_USER, ERR_DOES_NOT_EXIST, ERR_NOT_WRITEABLE Request Parameters : Request Body : { \"paths\" : [\"list of paths to look up tickets for\"] } Response Body : { \"tickets\" : { \"/path/to/file\" : [ { \"path\" : \"/path/to/file\", \"ticket-id\" : \"<ticket-id>\", \"download-url\" : \"http://127.0.0.1:8080/d/<ticket-id>\", \"download-page-url\" : \"http://127.0.0.1:8080/<ticket-id>\" } ], \"/path/to/dir\" : [ { \"path\" : \"/path/to/dir\", \"ticket-id\" : \"<ticket-id>\", \"download-url\" : \"http://127.0.0.1:8080/d/<ticket-id>\", \"download-page-url\" : \"http://127.0.0.1:8080/<ticket-id>\" } ] } } Curl Command : curl -H \"$AUTH_HEADER\" -d '{\"paths\":[\"/path/to/file\",\"/path/to/dir\"]}' http://127.0.0.1:3000/secured/filesystem/list-tickets Deleting Tickets \u00b6 URL Path : /secured/filesystem/delete-tickets HTTP Method : POST Error Codes : ERR_NOT_A_USER, ERR_DOES_NOT_EXIST, ERR_NOT_WRITEABLE Request Parameters : Request Body : { \"tickets\" : [\"ticket-id1\", \"ticket-id2\"] } Response Body : { \"tickets\" : [\"ticket-id1\", \"ticket-id2\"] } Curl Command : curl -H \"$AUTH_HEADER\" -d '{\"tickets\":[\"ticket-id1\",\"ticket-id2\"]}' http://127.0.0.1:4000/secured/filesystem/delete-tickets","title":"Tickets"},{"location":"services/api/endpoints/filesystem/tickets/#creating-tickets","text":"URL Path : /secured/filesystem/tickets HTTP Method : POST Error Codes : ERR_NOT_A_USER, ERR_DOES_NOT_EXIST, ERR_NOT_WRITEABLE Request Parameters : public - Tells terrain whether to make the ticket accessible to the public group. Setting it to 1 means that the ticket will be made publicly accessible. Any other value means that the ticket will not be accessible publicly. This parameter is optional and defaults to not making tickets public. Request Body : { \"paths\" : [ \"/path/to/file/or/directory\", \"/path/to/another/file/or/directory\" ] } Response Body : { \"user\" : \"<username>\", \"tickets\" : [ { \"path\" : \"/path/to/file/or/directory\", \"ticket-id\" : \"<ticket-id>\", \"download-url\" : \"http://127.0.0.1:8080/d/<ticket-id>\", \"download-page-url\" : \"http://127.0.0.1:8080/<ticket-id>\" } ] } Curl Command : curl -H \"$AUTH_HEADER\" -d '{\"paths\":\"/path/to/file/or/directory\",\"/path/to/another/file/or/directory\"]}' 'http://127.0.0.1:3000/secured/filesystem/tickets?public=1'","title":"Creating Tickets"},{"location":"services/api/endpoints/filesystem/tickets/#listing-tickets","text":"URL Path : /secured/filesystem/list-tickets HTTP Method : POST Error Codes : ERR_NOT_A_USER, ERR_DOES_NOT_EXIST, ERR_NOT_WRITEABLE Request Parameters : Request Body : { \"paths\" : [\"list of paths to look up tickets for\"] } Response Body : { \"tickets\" : { \"/path/to/file\" : [ { \"path\" : \"/path/to/file\", \"ticket-id\" : \"<ticket-id>\", \"download-url\" : \"http://127.0.0.1:8080/d/<ticket-id>\", \"download-page-url\" : \"http://127.0.0.1:8080/<ticket-id>\" } ], \"/path/to/dir\" : [ { \"path\" : \"/path/to/dir\", \"ticket-id\" : \"<ticket-id>\", \"download-url\" : \"http://127.0.0.1:8080/d/<ticket-id>\", \"download-page-url\" : \"http://127.0.0.1:8080/<ticket-id>\" } ] } } Curl Command : curl -H \"$AUTH_HEADER\" -d '{\"paths\":[\"/path/to/file\",\"/path/to/dir\"]}' http://127.0.0.1:3000/secured/filesystem/list-tickets","title":"Listing Tickets"},{"location":"services/api/endpoints/filesystem/tickets/#deleting-tickets","text":"URL Path : /secured/filesystem/delete-tickets HTTP Method : POST Error Codes : ERR_NOT_A_USER, ERR_DOES_NOT_EXIST, ERR_NOT_WRITEABLE Request Parameters : Request Body : { \"tickets\" : [\"ticket-id1\", \"ticket-id2\"] } Response Body : { \"tickets\" : [\"ticket-id1\", \"ticket-id2\"] } Curl Command : curl -H \"$AUTH_HEADER\" -d '{\"tickets\":[\"ticket-id1\",\"ticket-id2\"]}' http://127.0.0.1:4000/secured/filesystem/delete-tickets","title":"Deleting Tickets"}]}